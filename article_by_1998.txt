
        
          
Introduction

La « Théorie des Représentations Discursives » (DRT introduite par Hans Kamp (1981;
1993) a été élaborée dans le cadre de la sémantique formelle, et constitue à ce titre un domaine
d'investigation théorique et philosophique des propriétés sémantiques et discursives des langues
naturelles (LN). Les propriétés de la DRT font aussi de cette théorie un outil de représentation
des connaissances portées par des énoncés en LN. À ce titre, elle a fait l'objet de divers travaux
qui élaborent des méthodes compositionnelles de construction de DRS 1. Le travail présenté
dans cet article s'inscrit dans cette lignée.
Notre proposition est basée sur la méthode proposée pour l'anglais par Asher (1993). Notre
version de cette méthode utilise des représentations syntaxiques pour le français, inspirée de
l'approche générative. Le mécanisme de typage esquissé par Asher a été revu et étendu pour lui
donner plus de cohérence et pour traiter des problèmes laissés en suspens dans (Asher, 1993).
C'est sur ce mécanisme que nous nous focalisons dans cet article 2. Nous présentons d'abord la
DRT dans ses grandes lignes (§ 2 puis la méthode ascendante que nous utilisons (§ 3 avant
de détailler les sytèmes de types eux-mêmes à la section 4.
. Voir en particulier (van Eijck et Kamp, 1996; Amsili et Bras, 1998) pour une discussion récente sur les
problèmes de compositionnalité et de représentationnalité de la DRT.
. On trouvera dans (Amsili et Hathout, 1996) une présentation dans le même cadre général, mais consacrée à
l'interaction temps/négation.

          
La théorie des représentations discursives

La DRT s'intéresse spécifiquement aux discours, c'est-à-dire à des ensembles de phrases
formant un tout cohérent. On considère selon cette vue la contribution sémantique de chaque
phrase P , comme une fonction du contexte : étant donné un contexte C , , la prise en compte de
P , consiste en une «mise à jour» de C , , qui intègre les éléments nouveaux apportés par P , , et qui
sert à son tour de contexte C. pour les phrases suivantes. Les DRS (Discourse Representation
Structures — structures de représentation discursives), généralement représentées sous forme
de «boîtes», jouent précisément ce rôle de «contexte».
(1) Pierre possède une voiture. Il la bichonne.
Supposons que les deux phrases de (1) constituent les phrases initiales d'un discours. Le
contexte initial de (1) est donc vide ; il correspond à la DRS K / (figure 1). La contribution de la
première phrase de (1) correspond à la DRS K  ou plus exactement au passage de K / à K  .

            
Représentation du discours

La boîte K se compose de deux ensembles. Le premier, { : , ; }, appelé univers de discours,
contient des référents de discours, analogues à des variables en logique du premier ordre (LPO),
chacun correspondant à un «individu» mentionné dans le discours. Ainsi, la première phrase de
(1) introduit deux individus, Pierre et une voiture, représentés respectivement par les référents
et ; . Ces individus deviennent ainsi présents dans le contexte ; il est alors possible d'y faire
référence par des pronoms anaphoriques comme dans la seconde phrase de (1). Le second ensemble de K  , {Pierre( : ), voiture( ; ), : posséder ; }, regroupe des DR-conditions qui portent sur
les référents de discours. Ces conditions sont ici des conditions atomiques, dont l'interprétation
est la même que celle des prédicats en LPO. Les DR-conditions peuvent également être des
expressions complexes faisant intervenir des «sous-DRS» (négation, quantification, etc.).
La seconde phrase de (1) fait passer de K à K # . De nouveaux référents de discours sont
introduits, et de nouvelles conditions sont ajoutées. Cette seconde phrase fait intervenir des pronoms anaphoriques (il et la), qui, pour être interprétés, doivent être mis en relation avec d'autres
référents de discours déjà présents. Cette relation est exprimée par les équations anaphoriques
figurant dans K.
Faute de place, nous nous contentons de cette présentation succincte des aspects les plus élémentaires du langage des DRS 3 , et nous nous focalisons maintenant sur la manière de construire
une DRS à partir de la donnée d'une phrase en LN. On trouve dans (Kamp et Reyle, 1993) une
méthode de construction, appelée descendante, qui repose sur le repérage de configurations particulières dans l'arbre syntaxique, chaque configuration déclenchant l'introduction de référents
de discours et de DR-conditions dans la DRS courante. Cette méthode présente l'inconvénient
de n'être pas compositionnelle au sens strict, et pose des problèmes de contrôle pour sa mise
en oeuvre effective. C'est la raison pour laquelle on préfère en général utiliser une méthode plus
opératoire, basée sur les principes mis en oeuvre dans la grammaire de Montague et le + -  calcul 4 .
. Comme nous l'avons dit, on trouve dans ce langage des conditions de forme plus complexe, qui permettent de
rendre compte de la sémantique de phénomènes comme la négation, la quantification, etc. Par ailleurs, l'ontologie
sous-jacente doit évidemment être enrichie dès lors que l'on veut prendre en compte par exemple la dimension
temporelle des énoncés, etc. Nous n'évoquerons pas ici tous ces aspects, qui sont présentés en détail dans (Kamp et
Reyle, 1993). Nous laissons de côté aussi tous les aspects qui portent sur le problème du traitement des anaphores.

          
La méthode ascendante

La DRT ascendante, proposée par (Asher, 1993), procède par + abstraction sur les constituants des DRS afin de définir des + - expressions ( + - DRS ) que l'on associe aux feuilles de
l'arbre syntaxique. Ces + - DRS, composées par > réduction, conduisent à une DRS 5 . La construction de la DRS K , d'une phrase se fait en deux étapes : dans un premier temps, on
construit une DRS « incomplète ». Cette construction est ascendante : on part des feuilles de
l'arbre, et on «remonte » le long des branches en composant les + expressions. Dans un second
temps, la représentation incomplète, est «plongée » dans la DRS contexte K, pour permettre
la résolution des aspects incomplets (typiquement les équations anaphoriques) et aboutir à K.
Nous ne nous intéressons dans cet article qu'à la première étape : la construction, pour une
phrase donnée, de la DRS qui lui correspond. Le coeur de la méthode est constitué par la liste
des + - DRS que l'on associe aux différents éléments lexicaux. On va retrouver là des choix
analogues à ceux de la tradition montagovienne. On distingue parmi ces + - DRS d'une part les
DRS prédicatives associées en particulier aux verbes et aux noms (communs), et d'autre part
les DRS partielles associées aux déterminants et aux syntagmes nominaux. La composition de
-DRS de ces deux types peut conduire à des + - DRS à la fois prédicatives et partielles.
On fait l'hypothèse que l'on dispose pour chaque phrase ?A, d'une représentation syntaxique,
qui décrit son organisation en syntagmes. La théorie syntaxique choisie n'est pas déterminante,
dès lors qu'elle rend compte de manière satisfaisante des propriétés linguistiques en jeu. Le travail présenté ici s'appuie sur une analyse des phrases du français qui s'inscrit dans une approche
«générativiste» de la syntaxe (Chomsky, 1981).

            
Représentation syntaxique

La représentation syntaxique occupe dans la méthode ascendante une position centrale car
elle définit la structure syntagmatique qui guide la construction de la DRS et où elle fournit
l'ensemble des informations linguistiques nécessaires au calcul. Cette représentation doit donc
décrire la décomposition de la phrase en constituants, mais aussi les traits morphologiques de
ces derniers, leurs fonctions syntaxiques, leurs attributs et contributions sémantiques, etc. Les
représentations syntaxiques que nous utilisons sont des arbres tels que les définit la Théorie X
(Chomsky, 1981; Chomsky, 1986). Chaque syntagme est composé d'une tête X, des projections
intermédiaires X B et maximales XP de X, et éventuellement de compléments, de spécificateurs et
de modificateurs adjoints qui sont eux-mêmes des syntagmes. À titre d'exemple, la représentation de la phrase Pierre possède une voiture neuve est donnée en figure 2. Nous ne représentons
pas dans cette figure les niveaux de projection intermédiaire lorsqu'ils sont inutiles. Par ailleurs,
nous adoptons l'«analyse DP» qui fait du déterminant D la tête du groupe nominal DP. La proposition est considérée comme un syntagme flexionnel IP dont la tête I porte les traits de temps
et d'accord, la tête verbale étant générée sous V sans flexion. Pour acquérir cette dernière, V
s'adjoint à I en syntaxe (par mouvement de tête), ce que nous avons noté (V L )I 6 .
. C'est le cas des tous premiers travaux d'implémentation de la DRT (Frey et Reyle, 1983; Reyle, 1985; Wada
et Asher, 1986; Johnson et Klein, 1986) et aussi de nombreuses approches récentes (Asher, 1993; Bos et al., 1994;
Blackburn et Bos, 1997).
. Pour une présentation du C -calcul, voir (Dowty et al., 1981; Hindley et Seldin, 1986; Krivine, 1990).

              
Représentation syntaxique de Pierre possède une voiture neuve

              
Représentation résultant de l'analyse syntaxique

L'arbre de la figure 2 ne donne qu'une petite partie des informations dont on dispose à l'issue
de l'analyse syntaxique. Certaines proviennent directement du lexique, comme les informations
sémantiques générales (le fait que Pierre est un humain, etc.). D'autres sont calculées en cours
d'analyse, comme les propriétés morphologiques (catégorie, genre, nombre, personne, temps
verbal, cas morphologique des pronoms, etc.), les rôles thématiques assignés aux arguments
. Faute de place, nous ne pouvons expliciter plus en détail les choix résumés dans ce paragraphe. Le lecteur
intéressé pourra se reporter à (Haegeman, 1991)

            
DRS partielles et DRS prédicatives

On considère que la contribution des verbes et des noms communs, représentés en DRT par
des prédicats, est précisément un prédicat, dont l'arité est déterminée par le lexique, et dont
les arguments font l'objet d'une + - abstraction. On associe donc à ces éléments lexicaux des
DRS prédicatives (PDRS). Par exemple, aux éléments voiture et posséder sont associées les + - 
expressions (2). Les lettres en italique gras représentent des variables portant sur des référents
de discours (DR-variables ). En général, une DRS prédicative est une + - DRS dans laquelle des
arguments d'au moins un prédicat font l'objet d'une + - abstraction. De manière duale, les + - 
expressions de l'autre catégorie, les DRS partielles, font une + - abstraction sur les prédicats. Par
exemple, on associe à un déterminant comme un la DRS partielle (3a) (est un référent
de discours nouveau), tout à fait dans la tradition montagovienne — cf., par exemple, (Dowty
et al., 1981, pp. 7-17).
La combinaison d'une DRS partielle et d'une DRS prédicative donne lieu à deux > -réductions. L'exemple (3) détaille ces deux opérations. La première substitue à la variable P de (3a) la
DRS prédicative (3b) : (3c). Celle-ci se trouve donc appliquée à l'argument de P, ce qui donne
lieu à la seconde > -réduction, qui produit en toute rigueur (3d), lequel est mis finalement sous
la forme (3e). Il faut faire à ce propos différentes remarques. Au point de vue technique, il faut
noter que le passage de (3d) à (3e) repose sur deux transformations : d'une part, on admet que
les variables sous + figurant dans ces expressions ont toujours une portée large ; d'autre part, on
suppose que l'on est capable de « fusionner » les boîtes comme c'est le cas dans cet exemple.
Ces deux procédés sont délicats à formuler rigoureusement (Asher, 1993, pp. 98ss), mais ils ne
posent pas de problème particulier. Nous laissons de côté ce point dans le reste de cet article.
Deux autres aspects vont au contraire nous occuper particulièrement. D'abord, il faut noter que
le calcul peut conduire à l'apparition de + - DRS à la fois partielles et prédicatives (ex. (3e)) ;
d'autre part, et c'est une conséquence de ce qui précède, l'ordre d'application de deux + - DRS
données n'est pas toujours déterminé seulement par la nature «partielle» ou «prédicative» (dans
(3), on «applique» la DRS partielle (3a) à la DRS prédicative (3b)) : il est nécessaire de raffiner
le typage des + - expressions pour garantir que l'on applique la bonne + - DRS à l'autre (cf. § 4.1).
Ensuite, on trouve des cas de combinaison dans lesquels le choix de la variable à saturer (cf.
choix de x dans le passage de (3c) à (3d)) est problématique car il ne peut être déterminé par
la forme seule des expressions. Il est alors nécessaire de disposer d'informations sur les rôles
thématiques (rôles) des variables (cf. § 4.2).

              
Treillis de types pour les + -  DRS

          
calcul typé pour la composition des DRS

On peut schématiser la situation de composition de la manière suivante : un noeud  étant
donné, ayant deux fils et auquels sont attachés respectivement les + -DRS + - et + - ,
la + -DRS associée à  est obtenue par composition.

            
Typage par les fonctions syntaxiques

Comme nous l'avons évoqué, la nature de + - et + - # ne permet pas de déduire l'ordre de
composition. C'est la configuration syntaxique qui permet de déterminer cet ordre composition,
la configuration syntaxique étant elle-même déterminée par la fonction syntaxiques des noeuds
en jeu. Nous ne pouvons pas exemplifier ici ces différents cas, qu'il est possible de résumer
dans le tableau (4) 7 :

L'adjonction constitue un cas particulier, la forme des + -expressions rend nécessaire une
opération de promotion de l'adjoint qui transforme une DRS prédicative comme (5a) en une
DRS partielle et prédicative (5b). La suite de la composition est une simple application de la
DRS partielle à la PDRS du constituant modifié (5c).
Les règles (4) s'appuient sur le treillis de types donné en figure 4. Ce sont les homologues
pour la construction des DRS des principes de la Théorie X B . Nous avons regroupé les compléments, les spécificateurs et les adjoints dans un sur-type XP afin de rendre compte de la
«dualité » des fonctions syntaxiques qui se définissent d'une part par rapport aux pères et aux
frères, et d'autre part par rapport à leurs fils. Bien que le typage des + -DRS soit relatif aux
fonctions remplies par les noeuds vis-à-vis de leur père et de leurs frères, nous sommes amenés
à les typer dans la partie droite des règles à l'aide de ce sur-type, sans référence à la fonction
qu'ils remplissent effectivement dans l'arbre. Signalons également que les règles (4) ne sont
pas « orientées » en ce sens qu'elles s'appliquent à une paire d'opérandes sans tenir compte
de l'ordre linéaire dans lequel ils apparaissent dans l'arbre syntaxique. Par ailleurs, il est né-
cessaire de disposer de règles de «translation de type » pour traiter les + -DRS qui se trouvent
sur un noeud qui n'a pas de frère et qui se projette simplement en un noeud de niveau de barre
supérieur (X en X B ou X B en XP).
. Dans ce tableau, nous utilisons les notations habituelles de la syntaxe X G , qui distingue, outre les têtes (X),
les projections intermédiaires (X G ) et maximales (XP), la notion de complément (Compl), de spécificateur (Spec)
et d'adjoint (Adjoint) selon la configuration syntaxique, qui peut prendre l'une des trois formes :
Dans notre cas, on distingue en plus les têtes fonctionnelles (X  ) des têtes prédicatives, dites majeures (X).

Ce premier typage est illustré en (6) par la construction de la + -DRS associée au syntagme
nominal une voiture via la règle de composition X  -Compl et une translation de type. Cet
exemple montre clairement que ce sont les + -DRS qui sont typées et non pas les PDRS-variables
ni les boîtes du calcul. En d'autres termes, la composition exemplifiée en (3) demeure dans le
cadre du + -calcul non typé proposé par (Asher, 1993). Les boîtes et les PDRS-variables ne
peuvent en effet pas être typées par les fonctions syntaxiques car ces informations sont déterminées par le contexte de chaque syntagme et non par son contenu : la structure d'un syntagme ne
permet pas de lui assigner une fonction syntaxique, ni de définir compositionnellement un type
pour la boîte qui lui est associée.

La mise en oeuvre du typage par les fonctions syntaxiques impose ainsi à la DRT ascendante de franchir un pas de plus dans son intégration avec la représentation syntaxique. Asher
(1993) utilise uniquement la structure arborescente pour établir un ordre de précédence sur
de caractériser totalement les opérations de composition, nous proposons d'utiliser les informations associées aux noeuds pour spécifier plus complètement ces opérations (cf. (4. Notons
que la nature compositionnelle du calcul ascendant des DRS ne se trouve pas affaiblie par le
recours aux systèmes de types que nous proposons : ces derniers sont totalement déterminés par
la représentation syntaxique qui sert de base à ce calcul.

            
Typage par les rôles

La composition des + - DRS peut être dans certains cas problématique car ni leurs DRla composition de cette+ - DRS avec la DRS partielle correspondant au syntagme nominal une
falaise (8a deux> -réductions ont lieu (cf. § 3.2). La première est complètement spécifiée par
turée. Ainsi,< peut saturer indifféremment y ou x, ce qui donne l'une des deux formes données
variables ni leurs PDRS-variables ne sont ordonnées. Ainsi, par exemple, pour une phrase telle
que (7 le verbe gravir est représenté par une DRS prédicative à deux variables (8a). Lors de
la règle de composition. En revanche, pour la seconde (application de la DRS prédicative au
référent de discours de la DRS partielle) rien ne permet de choisir quelle variable doit être sasous (8b dont seule une est acceptable.

Le choix de la variable que doit saturer un référent de discours donné dépend à la fois de
la sémantique lexicale du prédicat qui les contient, et des relations thématiques établies entre
ce prédicat et ses arguments dans la représentation syntaxique. Par exemple, en (8a le verbe
gravir est représenté par une DRS partielle à deux variables x et y, l'une correspondant au
differents typages, on ajoute un troisième champ aux DRS dans lequel on rassemble les typages
Notation. La DRT ascendante met donc en oeuvre au moins deux systèmes de types concuret aux référents de discours sous la forme d'un typage ; la compatibilité entre deux types est
calculée relativement à un treillis de types. Reprenons l'exemple (7). Les constituants Pierre et
Pierre et une falaise qui remplissent ces rôles sont identifiés par l'analyseur (cf. figure 3) et
deux relations thématiques sont établies avec le prédicat verbal. Ainsi, en associant à chaque et l'autre. Dans la représentation syntaxique de (7 les syntagmes nominaux
référent de discours le - rôle du constituant auquel est accrochée la + - DRS qui l'introduit, et
à chaque variable le - rôle qui lui correspond dans la grille thématique du prédicat, on peut
imposer sur les> -réductions du type (3c) une contrainte de compatibilité entre les - rôles de
la variable et du référent de discours. Plus précisément, les - rôles sont associés aux variables
une falaise ont respectivement pour+ - DRS (9a) et (9c) ; la + - DRS associée au verbe gravir est
(9b). On peut alors construire la DRS de (7) en appliquant les règles du + - calcul typé (Krivine,
1990).

Notation. La DRT ascendante met donc en oeuvre au moins deux systèmes de types concurrents, l'un sur les + -DRT, l'autre sur les DR-variables et les référents de discours. Pour noter ces
differents typages, on ajoute un troisième champ aux DRS dans lequel on rassemble les typages
des entités manipulés. Cette nouvelle rubrique correspond donc au « contexte ¶ » du + -calcul
typé. En convenant de noter par un point la ( + -) DRS courante, cela donne pour les + -DRS (9) :

Le typage des DR-variables et des référents de discours par les - rôles ne pose pas de problème particulier pour sa mise en oeuvre. Pour les prédicats qui assignent les - rôles, cette
information est directement codée dans le lexique, sur la+ - DRS qui leur y est associée. Aux
fecte initialement le type le universelÁ
qu'aucun rôle thématique n'est assigné aux noeuds auxquels sont accrochées les+ - DRS qui les
contiennent (i.e. pour les noeuds N, NB , NP, D et DB dans le cas d'un syntagme nominal comme
une falaise). Lorsque la composition des+ - DRS produit une boîte accrochée à un noeud qui reçoit un - rôle, une nouvelle contrainte de type est imposée sur le référent de discours introduit
sous-type commun du type universelÁ et du typeÂ correspondant au - rôle, c'est-à-direÂ
DR-variables et aux référents de discours qui participent à la construction des arguments, on af. Ces types sont composés de la manière habituelle tant
au niveau de la tête fonctionnelle de ce syntagme. Le type du référent est alors le plus grand
sous-type commun du type universel Á et du type Â correspondant au - rôle, c'est-à-dire Â .

            
Traits sémantiques

Outre les deux systèmes de types que nous venons de décrire, la DRT «classique» en utilise
un troisième qui code les propriétés morphologiques et sémantiques nécessaires à la résolution
des anaphores. Les propriétés morphologiques concernent le genre, le nombre, la personne, le
temps verbal, etc. ; ces informations sont calculées lors de l'analyse syntaxique. Les propriétés
sémantiques décrivent les traits pertinents pour distinguer les différents référents de discours
de la DRS ; elles sont décrites dans le lexique et sont recopiées telles quelles dans l'arbre syntaxique. Bien que l'on représente communément ces informations comme des DR-conditions,
elles ne relèvent pas du même niveau puisqu'elles n'interviennent pas directement dans l'interprétation. Il est préférable de les représenter séparément sous la forme d'un typage complexe à
l'aide de structures de traits typées semblables à celles du langage de programmation logique
Life (Aït-Kaci & Podelski, 1993) : on associe à chaque référent de discours un type morphologique et sémantique, et on contraint le processus de résolution anaphorique à s'assurer que les
deux membres d'une équation anaphorique sont compatibles (unifiables). À titre d'exemple, si
l'on utilise les informations décrites dans la figure 3, la+ - DRS (10a) devient (11) :

          
 Conclusion

Les structures de traits typées qui servent à contraindre les résolutions anaphoriques sont
simplement recopiées au niveau des noeuds qui introduisent les DR-variables ou les référents de
obtenues en unifiant celles des DR-variables et des référents de discours impliqués.
La DRT ascendante est une méthode de calcul compositionnel des DRS, basée sur la rediscours. Lors de la composition des+ - DRS, les structures de traits associées aux résultats sont
présentation syntaxique des phrases. Cette méthode est formalisée dans le cadre du+ - calcul :
les éléments du langage sont des boîtes contenant des variables sous+
(+ - DRS) que l'on compose par> -réduction. La composition des+ - DRS est cependant sous-spécifiée, dans sa version
à typer les+ - DRS par les fonctions syntaxiques des noeuds auxquels elles sont accrochées, à
formuler les règles de composition dans le cadre du+ - calcul typé et à typer les DR-variables
vers le contexte (au sens du+ - calcul typé) associé à chaque boîte. La DRT ascendante typée
est ainsi mieux définie du point de vue opératoire. Il nous reste à montrer formellement que
les aménagements que nous proposons préservent les propriétés théoriques établies par Asher
(1993). Nous travaillons également sur l'utilisation de versions plus sophistiquées de la grammaire sous-jacente dans lesquelles les configurations structurelles seraient suffisamment discriclassique, pour plusieurs points essentiels, en particulier la sélection de la règle de composition et des variables à saturer. Nous avons proposé dans cet article une solution qui consiste
et les référents de discours à l'aide des rôles thématiques.




        
          
Introduction

TAG est un formalisme initialement développé dans le cadre de la Théorie des langages formels, dont la pertinence pour la représentation de phénomènes linguistiques a été argumentée après coup (Kroch& Joshi 85) . L'aspect procédural a été étudié en profondeur et des implémentations à large échelle ont été réalisées pour l' analyse ou la génération automatique (XTAG 95; Danlos& Meunier 96). Si la lexicalisation forte du formalisme a apporté des avantages tant linguistiques que computationnels (Abeillé 91 elle entraîne une redondance dans la représentation de la grammaire, sans qu'une organisation modulaire vienne y remédier . A l'inverse, la TST est une théorie qui d'une part est modulaire et d'autre part a été développée par des linguistes en élaguant volontairement toutes les questions procédurales. Diverses implémentations basées sur la TST ont certes été réalisées FOG, AlethGen, ... mais l'aspect procédural de la correspondance Sens-Texte, comme le formalisme des règles de correspondance n'ont fait l'objet que d'un début d'étude (Polguère 90; Kahane& Melcuk 97). Cependant ces deux modèles ont des points de convergence notables (outre la lexicalisation forte et le traitement différencié des arguments et des modifieurs, chaque entrée lexicale en TAG et TST correspond à une unité sémantique, dont sont déclarés les actants (Abeillé 91. Ces similitudes rendent possibles une comparaison et un enrichissement de chacun des deux modèles. La compar aison permet , pour TAG, de mieux spécifier les notions linguistiques utilisées, de réinterpréter le résultat de l'analyse TAG comme une analyse sémantique et de calquer l'organisation modulaire de la TST pour construire la grammaire. Pour la TST, cela permet de s'inspirer de la procédure de dérivation TAG pour obtenir une procédure de correspondance Sens-Texte, et de mettre en évidence les points qui doivent être développés pour que la TST devienne un modèle de référence en linguistique computationnelle, comme l'est déjà TAG.
 Nous remercions Anne Abeillé, Laurence Danlos, Lidija Iordanskaja, Igor Melcuk, Alain Polguère, Owen
Rambow et nos trois referees pour leurs nombreuses remarques et suggestions faites sur la première version de cet
article.


Dans les Sections 1 et 2, nous présenterons la TST, puis TAG. Nous avons intégré, dans ces
présentations, les améliorations dont ont bénéficié chacun des deux modèles au cours de cette
étude. Dans la Section 3, nous montrerons que les deux modèles utilisent la même notion de
dépendance sémantique, bien que de manière implicite en TAG. Ceci nous permet de comparer
les représentations linguistiques des deux modèles (et de montrer à quel point elles sont proches).
Ce parallèle permet, d'une part, d'utiliser la procédure de dérivation TAG pour assurer la
correspondance entre le niveau sémantique et la phrase en TST (Section 4) et, d'autre part, de
s'inspirer de la représentation modulaire des informations linguistiques en TST pour spécifier la
création d'arbres élémentaires TAG (Section 5).

          
Introduction à la TST

            
Les différents niveaux de représentation en TST

Dans cette étude, nous considèrerons 4 des 7 niveaux de représentations de la TST :
sémantique, syntaxique profond, syntaxique de surface et morphologique profond. A chaque
niveau, la représentation d'une phrase est constituée d'une structure centrale et de structures
périphériques qui lui sont superposées. A part pour le niveau sémantique, seule la structure
centrale sera considérée.
La représentation sémantique [ RSém] d'une phrase est une représentation du sens
langagier de la phrase. La structure centrale de la RSém, la structure sémantique [ SSém est
un graphe dont les noeuds sont étiquetés par des sémantèmes de la langue considérée (un
sémantème étant le signifié d'une lexie) et dont les arcs sont numérotés de façon à distinguer les
différents arguments d'un même sémantème prédicatif . Dorénavant , le terme dépendance
sémantique désignera la dépendance entre sémantèmes au sens de la TST (Zolkovski j& Melcuk
67; Melcuk 88). Sur la SSém, viennent se superposer un certain nombre de marques de noeuds ou
de sous-graphes constituant la structure communicative sémantique [ SComm-Sém ] (thème-rhème, focalisation, arrière-plan, …). La Fig. 1 donne la RSém pour la nouvel le bibliothèque
possède le livre que Pierre pense que Jean cherche (notre phrase de référence). (Notation: Le mot
cherche est une forme de la lexie dont le signifié est ‘chercher'.

              
la RSém pour La nouvelle bibliothèque possède le livre que Pierre pense que Jean cherche.

La structure syntaxique profon de [ SSyntP] est un arbre de dépendance dont les noeuds
sont étiquetés par des lexies pleines (accompagnées éventuellement de grammèmes) et dont les
branches sont étiquetées par des relations syntaxiques universelles: six relations actancielles (I , II,
... ,VI la relation attributive ATT R, la relation de coordination COORD et la relation appenditive
.
La structure syntaxique de surface [ SSyntS] est un arbre de dépendance dont les noeuds
sont étiquetés par des lexies (pleines ou vides) et dont les branches sont étiquetées par des
relations syntaxiques ( fonctions ou rôles syntaxiques) propres à chaque langue.

              
SSyntP et SSyntS

            
Le dictionnaire et les règles de correspondance de la TST

Une entrée lexicale en TST pour une lexie L indique le sémantème, la catégorie, la diathèse (
la correspondance entre relations sémantiques et relations syntaxiques profondes) de base et
com porte un cer tain nombre de traits comme <non_passi vable>, <ver be_pont> pour un verbe,
<ant éposable> pour un adjectif, et c... Par exemple pour PENSER on a les informations : ‘ penser ',
V, 1/I , 2/II [queV; Vinf <non_passi vable>, <ver be_pont>.
Le passage d'une RSém à une RMorphP est assuré par des règles linguistiques de
correspondance (entre deux niveaux adjacents) , avec consultation du dictionnaire quand
nécessaire, et par des règles procédurales qui les mettent en jeu. Il nous faut considérer entre le
niveau sémantique et le niveau morphologique profonds trois modules de correspondance ( chacun
matérialisé par une ⇔ RSém ⇔ RSyntP ⇔ RSyntS ⇔ RMorphR. Nous présent erons les règles2
de correspondance dans le sens de la synthèse, bien qu'il s'agisse de règles statiques, non
orientées.
Lors de la correspondance RSém ⇔ RSyntP sont assurées la lexicalisation et l'arborisation.
L'application des règles dépend fortement de la diathèse de la lexie (notée X) correspondant au
gouverneur sémantique (noté ‘X').
 Le format des règles présentées suit le format adopté dans (Kah ane & Mel'cuk 9 8 sont indiqués en gras les
éléments réellement manipulés par la règle, les éléments en maigre constituant le contexte; les éléments figurant dans
l'article de dictionnaire de la lexie, comme la partie du discours, sont entre parenthèses. Le signe ⇔ signifie que
l'élément de gauche PEUT être exprimé par l'élément de droite ( et vice v ersa) à condition que soient réalisées les
conditions imposées par le contexte et par la partie condition placée à la droite de la règle proprement dite.

La grosse flèche dans la partie gauche de la règle indique le sens de parcours de l'arc qui est contraint par la
SComm-Sém.

              
Exemples de règles de correspondance RSém et RSyntP

Lors de la correspondance SSyntP et SSyntS sont assurées la sélection des relations
syntaxiques de surface (Fig. 4), l'introduction des éventuelles prépositions régime en fonction des
indications de l'entrée de dictionnaire de X et de la catégorie de Y, et la pronominalisation (grosso
modo lorsqu'il y a plusieurs occurrences coréférentes en SyntP, toutes sauf une sont
pronominalisées en SyntS) 4 .
Lors de la correspondance SSyntS ⇔ SMorphP sont assurés la linéarisation — à une
branche SyntS correspond grossièrement un ordre entre deux noeuds MorphP (F ig. 5) — et les
accords morphologiques.

              
Exemples de règles de correspondance SSyntP et SSyntS

              
Exemples de règles de correspondance SSyntS et SMorphP

          
Introduction à TAG

            
Les arbres élémentaires

Dans une grammaire TAG, chaque entrée lexicale L est associée à une famille d'arbres
syntagmatiques, appelés arbres élémentaires, qui décrivent les différentes configurations
possibles pour L. La lexie L ancre chaque arbre élémentaire de sa famille, c'est-à-dire figure
comme feuille de l'arbre 5. On distingue deux types d'arbres élémentaires: les arbres initiaux et
les arbres auxilliaires. Ces deux types d'arbres se combinent avec d'autres arbres suivant des
opérations différentes: les arbres initiaux par substitut ion (l 'arbre vient se substituer par sa racine
à une feuille d'un autre ar bre) et les arbres auxilliaires par adjonction (l 'arbre vient s'insérer à la
place d'un noeud non-feuille d'un autre arbre).
 De plus, l'un des noeuds d'une relative coréférent à l' antécédent de la relative doit être pronominalisé par un
pronom relatif. Nous ne présentons pas les règles, d'autant que la pronominalisation n' a pas encore fait l'objet d'une
réelle étude dans le cadre de la TST (et n'est pas directement abordée en TAG).
 Les prépositions et conjonctions imposées par le régime de L figureront également comme feuilles des arbres
élémentaires ancrés par L. D'autre part, les formes composant une expression idiomatique sont les ancres multiples
d'un même arbre élémentaire.

Les feuilles d'un arbre initial autres que celle(s) occupée(s) par une lexie sont des noeuds à
substituer (notés X↓ c'est-à-dire des noeuds qui devront obligatoirement être étendus par
substitution. Un arbre auxiliaire possède en plus parmi ses feuilles un noeud dit pied (noté X* de
même catégorie que sa racine; c'est à ce noeud que doit être rattaché lors de l'adjonction le sous-arbre dominé par le noeud de l'arbre site d'ajonction (sur la Fig. 6).

              
la substitution et l'adjonction

              
Exemples d'arbres élementaires

On peut imposer aux arbres élémentaires de respecter certains principes linguistiques ( Kroch et amp;
Joshi 85; Abeillé 91; Franck 92 lexicalisation et cohérence sémantique (l 'ensemble des
éléments lexicaux à la frontière d'un arbre élémentaire correspond exactement à une unité
sémantique) et co-occurrence prédicat-arguments (les feuilles non lexicales d'un arbre
élémentaire correspondent bijectivement aux arguments sémantiques de l'ancre) . Dans les
exemples de la Fig. 7 (d'après (Abeillé 91 les différentes projections des arguments de l'ancre
sont numérotés comme les actants sémantiques en TS T.
On notera qu'il y a deux types d'arbres auxilliaires: les arbres auxiliaires modifieurs dont la
racine est la projection maximale du noeud pied (voi r βnouvelle et βcherche) et les arbres
auxiliaires prédicatifs dont la racine est la projection maximale de l'ancre (βpense). En termes de
dépendance, cela signifie que l'ancre d'un arbre modifieur dépend syntaxiquement de l'ancre de
l'arbre auquel il s'adjoint , tandis que l'ancre d'un arbre prédicatif en est le gouverneur syntaxique.
Les arbres prédicatifs, qui servent essentiellement à représenter les verbes ponts ( les verbes à
complétive permettant l'extraction hors de celle-ci partagent avec les arbres initiaux la propriété
que l'ensemble des arguments sémantiques de l'ancre soient des arguments syntaxiques, tandis
que les lexies représentées par un arbre modifieur ont un de leur argument sémantique comme
gouverneur syntaxique. Comme nous le verrons dans la Section 4, le choix d'associer à une
configuration un arbre prédicatif plutôt qu'un arbre initial est davantage liée à des contraintes
procédurales (la procédure étant elle-même contrainte par la nature des opérations permettent de
combiner les arbres entre eux) qu des propriétés linguistiques de la lexie concernée.

            
L'arbre de dérivation

Le témoin de cette analyse est appelé l'arbre de dérivation: chaque noeud de
l'arbre de dérivation est le nom d'un arbre élémentaire et les branches de l'arbre se répartissent en
deux types selon que l'arbre associé au noeud dépendant a été substitué ou adjoint à l'arbre associé
au noeud gouverneur. Les branches de substitution sont représentées graphiquement par des traits
pointillés et celles d'adjonction par des traits pleins.

              
l'arbre de dérivation et la SSém correspondante (disposée parallèlement)

          
Comparaison entre la SSém de la TST et l'arbre de dérivation

Dans cette section, nous établissons un parallèle entre SSém et arbre de dérivation. En TAG,
l'arbre de dérivation encode totalement la façon dont est dérivée une phrase. On peut cependant le
concevoir comme une représentation linguistique, indépendamment de la procédure qui donne
l'arbre dérivé. Bien qu'il ait des propriétés hybrides entre syntaxe et sém antique, nous allons
montrer qu'il existe un par allèle entre l'arbre de dérivation et la SSém en TST 7. Prenons pour cela
comme exemple l'arbre de dérivation et la SSém pour notre phrase de référence (Fi g. 8). On peut
faire les remarques suivantes :
 Nous prenons la définition de la dérivation TAG de (Sch abes & Shieber 9 4). Pour être non ambigu la dérivation
doit spécifier l'ordre dans lequel se font les adjonctions de différents arbres auxiliaires à un même noeud (le fils le plus
à gauche est adjoint en premier et ainsi de suite). Un seul arbre prédicatif peut être adjoint à un noeud donné, puisque
l'ancre de l'arbre prédicatif devient après l'adjonction le gouverneur syntaxique de l' ancre de l'arbre site. Comme le
soulignaient déjà ces auteurs, cette définition de la dérivation est plus appropriée pour la représentation des
dépendances sémantiques que celle où les branches ne sont pas ordonnées.
 On pourrait être tenté de rapprocher l'arbre de dérivation de l'arbre SyntP plutôt que de la SSém (comme le font
(Ram bow & Jos hi 94) ) en raison de la structure d' arbre et de l'étiquetage par des lexies. Nous insistons sur le fait que
les arcs de l'arbre de dérivation représentent bien des dépendances sémantiques. Comme il n'y a pas isomorphie entre
dépendance sémantique et dépendance syntaxique (profonde ou de surface il ne peut pas y avoir un parallèle
systématique entre arbre de dérivation et SSyntP. Si nous reprenons la SSyntP de notre phrase de référence (Fig. 2 on
peut voir plusieurs points de divergence avec la topologie de la SSém/arbre de dérivation. D'une manière générale,
entre deux éléments en dépendance syntaxique profonde, il peut y avoir entre les sémantèmes correspondants
(Melcuk, 88: 119sv - dépendance sémantique de même sens (par exemple entre ‘posséder ' et ‘ livre' - dépendance sémantique de sens opposé (par exemple entre ‘ bibliothèque' et ‘ nouveau'
- pas de dépendance sémantique directe (par exemple entre ‘ livre' et ‘penser' ).
Enfin il peut exister une dépendance sémantique directe entre deux sémantèmes dont les lexies ne sont pas liées
directement par une dépendance syntaxique (comme, par exemple, entre ‘boire' et ‘vin ', dans boire un verre de vin ).

Les noeuds de la SSém et de l'arbre de dérivation sont en bijection (cela est vrai sauf dans le
cas de coréférences, où en TAG en général, on aura plusieurs noeuds dans l'arbre de dérivation,
au lieu d'un dans la Rsém). Chaque noeud de l'arbre de dérivation renvoie à une unité
sémantique i.e. un sémantème de la SSém, mais contient en outre des informations sur la
lexicalisation de l'unité sémantique, la voix, la construction syntaxique, la pronominalisation,
l'ordre l inéaire, ...
Chaque couple de sémantèmes reliés directement dans la SSém l'est aussi dans l'arbre de
dérivation (c'est-à-dire que les deux représentations induisent un même graphe non orienté
entre leurs noeuds) 8.
En ce qui concerne l' orientation des relations entre noeuds, chaque branche de substitution correspond à une dépendance sémantique de même sens, et chaque branche d'adjonction - modificative ou prédicative - correspond à une dépendance sémantique inversée. Avec cette interprétation des arcs de substitution et d'adjonction, l'arbre de dérivation induit le même graphe orienté que la SSém.

          
Génération à partir d'une RSém avec TAG

Comme nous venons de montrer, il faut voir l' arbre de dérivation comme une SSém, à laquelle
s'ajoutent les informations de niveau syntaxique et morphologique permettent la correspondance
avec une phrase donnée. Une partie de ces informations est encodées par la SComm-Sém , mais
une partie seulement, puisque la RSém ( SSém + SComm-Sém ) représentent un invariant de
paraphrase et peut généralement être exprimée par différentes phrases.
Pour générer une phrase à partir d'une RSém avec une grammaire TAG, le principe est simple:
il faut passer de la RSém à un arbre de dérivation TAG (éventuellement sous-spécifié puis
effectuer la (ou les) dérivation( s) encodée(s) par cet arbre de dérivation.
La première étape consiste à choisir un noeud d'entrée pour la SSém, c'est à dire le noeud qui
sera la racine de l'arbre de dérivation. Le choix de ce noeud dépend de la SComm-Sém et
notamment des lexicalisations possibles des noeuds dominants des thème et rhème; de telles règles
ont été proposées par (Ior danskaja et Polguère 88; Polguère 90)9.
 Il existe des cas où cela n'est pas vrai. D' une part dans les cas de contrôle, la dépendance sémantique entre
l'infinitif et son sujet n' est pas reflétée en TAG moins qu' un sujet vid e soit substitué). D'autre part, les principes
linguistiques qui président à la formation des arbres élémentaires (cf. §2.1) assurent que chaque noeud de l'arbre de
dérivation contrôle autant de positions qu'il a d'arguments sémantiques. Néanmoins ces principes ne suffisent pas
assurer que ces positions soient effectivement remplies par les arguments sémantiques, notamment à cause de
l'adjonction prédicative. Ainsi pour dériver Que Pierre veuille partir dérange Marie en TAG, le sujet de dérange doit
être substitué (pour bloquer l'extraction hors du sujet p hrastique mais comme veuille reste adjoint sur partir, l'arbre
de dérivation possède un lien entre dérange et partir alors que c'est vouloir l'argument sémantique de ‘déranger' (que
Pierre veuille partir exprime une volonté, pas un départ). ( Ram bow et al.. 95) propose d 'augmenter le pouvoir de
l'opération de substitution afin de traiter les verbes ponts par des arbres initiaux et d'éviter ce type de problème (voir
(Can dito & Kah ane, soumis) pour une étude détaillée).
Le graphe sémantique est ensuite parcouru continûment à partir du noeud d'entrée 10;
lorsqu'un arc est par couru positivement — c'est-à-dire dans le sens gouverneur-dépendant —, on
obtient un arc de substitution; lorsqu' un arc est parcouru négativement — c'est-à-dire dans le
sens dépendant -gouverneur —, on obtient un arc d'adjonction (Cf. Fig 8).11 Lors du parcours de
la SSém, chaque sémantèm e reçoit en fonction notamment de la SComm-Sém des informations
sur la lexicalisation (notamment la partie du discours la diathèse, la construction syntaxique, etc,
ainsi que des traits morphologiques traduisant les sémant èmes flexionnels pointant sur lui. Ceci
donne un arbre de dérivation sous-spécifié dont les noeuds portent des traits (<passif>,
<suj et_inversé>, …) caractérisant un ensemble d'arbres élémentaires. On peut ensuite effectuer
les différentes dérivations qu'encode cet arbre, comme cela est fait en G-T AG (Danlos & Meunier
96) , et générer ainsi une ou plusieurs phrases.
Pour terminer nous allons voir que cette procédure peut en fait être appliquée à partir d'une
grammaire Sens-Texte, puisque celle-ci peut être compilée pour donner une grammaire TAG.

          
Compilation d'une grammaire Sens-Texte en une grammaire TAG

Rappelons qu'à chaque entrée lexicale d'une grammaire TAG est associée une famille d'arbres
élémentaires, chacun rassemblant des informations qui sont codées à différents niveaux en TST .
Nous allons voir comment on peut construire une famille d'arbres élémentaires pour chaque
entrée lexicale du dictionnaire de la TST, en utilisant les règles de correspondance Sens-Texte.
Cela correspond formellement à une compilation de la grammaire Sens-Texte.
Pour construire à partir de l'entrée lexicale L et des règles de correspondance TST, la famille
d'arbres élémentaires associée à L, l'idée est la suivante. On part de la configuration sémantique
associée à L (c'est-à-dire le bout de graphe consti tué de ‘L' et de ses actants sémantiques ‘X1', ...,
‘Xn'). En appliquant des règles de correspondance, on peut faire correspondre à cette
configuration un bout d'arbre SyntP, puis un bout d'arbre SyntS et enfin une chaîne MorphP. Une
séquence formée de la SSém associée à une lexie L, plus une SSyntP, une SSyntS et une
SMorphS synthétisées est appelée une chaîne de synthèse pour L. En appliquant différentes
règles de correspondance, on obtient différentes chaînes de synthèse pour L, dont chacune peut
être traduite en un arbre syntagmatique élémentaire d'ancre L. On obtient ainsi la famille d' arbres
élémentaires associée à L.
Partant d'une chaîne de synthèse, voyons comment obtenir un arbre élémentaire TAG.
Un arbre auxiliaire ou un arbre initial?
Les différentes chaînes de synthèse associées à L peuvent en particulier varier selon le parcours
de la SSém associée à L : le parcours peut démarrer par ‘L', mais aussi par chacun de ses
dépendants.
• Si ‘L' est le noeud par lequel commence la synthèse, tous les arcs sémantiques sont parcourus
positivement. On obtient donc un arbre initial où les ‘X i ' donnent les noeuds à substituer.
 En raison des discontinuités entre la SSém et la SSy ntP (cf. Note 7 il n'est pas possible, lors du passage de la
SSém à la SSy ntP d'assurer simultanément un parcours continu de la SSém et un traçage continu de la SSy ntP. Dans
(Kah ane & Mel'cuk 9 7) est proposée une procédure où on privilégie un traçage continu de l'arbre SyntP du sommet
vers les feuilles, quitte à effectuer des sauts dans le parcours de la SSém. Si on privilégie, comme ici un parcours
continu de la SSém, on doit lors du traçage de l'arbre Syn tP revenir en arrière insérer des bouts d'arbre, ce qui revient
formellement à des adjonctions.
 Le parcours du graphe pose néanmoins un problème non négligeable (et non résolu) en raison de la présence de
cycles: chaque cylcle doit être coupé en un noeud . Le noeud où le cycle est coupé donne deux noeuds coréférents dont
l'un des deux doit être pronominalisé.

Sinon, la synthèse commence par un des actants sémantiques ‘X k '. L'arc pointant sur lui est
par couru négativement et tous les autres arcs sémantiques sont parcourus positivement. On
obtient un arbre auxiliaire dont ‘Xk' donne le noeud pied et les autres ‘Xi' les noeuds à
substituer.
Un arbre modifieur ou arbre prédicatif?
Si la synthèse commence par ‘L', L est nécessairement le sommet de l' arbre SyntS, Mais dans
le cas contraire, i.e. dans le cas où on construit un arbre auxiliaire, L peut ou pas être la tête de
l'arbre SyntP.
Si X k est le gouverneur SyntP de L, on obtient un arbre modifieur.
Si L est le gouverneur SyntP de X k , on obtient un arbre prédicatif.

Pour le reste le passage d'une chaîne de synthèse à un arbre élémentaire est relativement
simple. L donne l'ancre lexicale de l'arbre élémentaire. Au dessus de L sont créés une ou
plusieurs projections12 auxquelles sont accrochés les noeuds à substituer, ainsi que le noeud pied
dans le cas d'un arbre auxiliaire prédicatif. Dans le cas d'un arbre auxiliaire modifieur, la
projection maximale de L et le noeud pied sont accrochés à la racine, qui est une projection du
noeud pied.

            
Traduction d'une chaîne de synthèse TST en un arbre élémentaire TAG

Du fait que la synthèse commence en ‘Y' (i ndiqué par la grosse flèche la chaîne de synthèse
donne un arbre auxiliaire dont Y donne le noeud pied. Comme L est le gouverneur SyntS de Y, il
s'agit d' un arbre auxiliaire prédicatif . L ét ant un verbe, un noeud V et un noeud P (projection
maximale de V) sont placés au dessus de L. L'act ant Sém 1 ‘X' donne le noeud à substituer N1
placé à gauche de V; l'actant Sém 2 ‘Y' donne le noeud pied de catégor ie P2 pl acé à droite de V.
On obtient ainsi un arbre du type βpense (Fi g. 7).

          
Conclusion

Notre étude n'est pas sans conséquences pour un modèle comme pour l'autre. Pour la TST, la
« procédure de dérivation T AG » ( c'est- dire le parcours continu de la SSém et la précompilation
des règles de correspondance) fournit une procédure de référence pour toutes les études à venir
sur les utilisations possibles de la TS T en TAL. Même s'il est probable qu' une précompilation
dans un format du type grammaires de dépendance serait plus économique, une opération du type de l'adjonction TAG devra être maintenue. 
 Le nombre de projections d e L est conséquence du nombre d 'actants de L et de la nature des linéarisations
possibles des actants de L et des adjoints en L. Nous ne traitons pas ce problème ici.
 La chaîne des ynthèse est obtenue comme suit. La synthèse commence en ‘Y'. Bien que parcouru négativement,
l'arc Sém 2 pointant sur ‘Y' p eut être traduit p ar un arc Syn tP II. Ceci est autorisé car L est un verbe pont et peut donc
être inséré lors d'une extraction . En Syn tS, une conjonction QUE est introduite. L' arc Sém 1, qui est parcouru
positivement, donne une branche Syn tP I, qui donne une branche Syn tS subjectale dont le dépendant X est un nom ; X
est placé à gauche de L (et L s'accorde avec X). 

De plus, le passage à une grammaire TAG reste à
l'heure actuelle le meilleur moyen d'utiliser la TST pour l'analyse.
Pour TAG, il était clair avant cette étude (Vij ay-Shanker & Schabes 92; Candito 96) qu' une
grammaire TAG quelque peu exhaustive ne peut être écrite et maintenue sans une surcouche
modulaire (comme l'est la grammaire TST ). Même si la TST n'est peut être pas le formalisme le
mieux adapté pour synthétiser une grammaire TAG, elle peut être un guide précieux pour la
réalisation d'une telle surcouche.
D'autre part, l'étude montre de manière précise qu' une dérivation TAG fournit davantage
qu' une analyse syntaxique. Il s'agit d' une représentation sémantique (au sens de la TST à la
représentation de la coréférence près. Ce résultat a des applications évidentes pour le TAL
utilisant les TAG, notamment la traduction automatique.




        
          
Introduction

L'analyse syntaxique guidée par les données (Data-Oriented Parsing ou DOP) (Bod, 1995)
constitue aujourd'hui une des voix de recherche prometteuses dans le cadre des approches à
base de grammaires faiblement sensibles au contexte 1 . Elle correspond à une mise en oeuvre
spécifique des grammaires probabilistes à substitution d'arbres et présente, à ce titre, plusieurs
différences importantes avec le modèle usuel des grammaires stochastiques (SCFG 2 ). En particulier, et à la différence des SCFG, il n'y a pas dans le cadre du modèle DOP une correspondance
bi-univoque entre un arbre d'analyse et la façon de produire cet arbre par la grammaire (par exemple une liste de sous-arbres) 3 .
La conséquence fondamentale de cet état de fait est qu'à la différence des SCFG, trouver
l'arbre d'analyse le plus probable devient un problème NP-difficile (Sima'an, 1996). Seule la
dérivation la plus probable peut être trouvée en un temps polynômial. Une solution possible
consiste alors à chercher l'arbre d'analyse le plus probable, non plus par une méthode exacte
mais par échantillonnage statistique au sein de la forêt des dérivations.
Cependant, pour qu'une telle technique soit effectivement opérationnelle, il est nécessaire
que les probabilités servant à échantillonner la forêt des dérivations soient (1) elles-mêmes
calculables en un temps polynômial et (2) compatibles avec les probabilités des arbres d'analyse
au sens du modèle DOP, de façon à ce que la probabilité d'obtenir un arbre d'analyse par
échantillonnage soit égale à sa DOP -probabilité.
Le but de cet article est, d'une part, de fournir une démonstration formelle de la faisabilité
des deux points ci-dessus (sections 2 et 3 ci-après) et, d'autre part, de proposer une méthode
d'échantillonnage contrôlée permettant de garantir (avec un seuil de confiance donné a priori)
la sélection effective de l'arbre d'analyse le plus probable (section 4). La section 5 complète
l'analyse en détaillant certains aspects d'implémentation des méthodes décrites.
. « mildly context-sensitive grammars »
2. pour « Stochastic Context-Free Grammars »
3. et ceci malgré la convention consistant à réécrire systématiquement en premier la feuille non-terminale la
plus à gauche.

          
Problématique

            
Définitions et notations

Une grammaire à substitution d'arbres est une grammaire formelle dont les règles sont des
arbres (appelés ci-après arbres de référence) dont les racines et les noeuds intérieurs sont des
symboles non-terminaux de la grammaire et les feuilles des symboles quelconques (terminaux
ou non-terminaux). Pour un arbre a , on notera r a sa racine, F a la séquence ordonnée 4 de
ses feuilles et f i a la i -ième feuille dans F a .
Sur l'ensemble des arbres d'analyse 5 , on définit une loi interne qui à deux arbres a et b
associe l'arbre c
a b résultant de la substitution par b de la feuille non-terminale la plus
à gauche de a , si cette feuille est égale à la racine de b , et l'arbre vide sinon.  n'étant pas
associative,  a m sera interprété par convention comme ::: a 1  a 2  :::  a m . Une
décomposition d'un arbre d'analyse a quelconque est la donnée de m sous-arbres de référence
a 1 , ..., a m , tels que a a 1 a m.
Dans le cadre du modèle DOP, la probabilisation d'une grammaire à substitution d'arbres
s'effectue alors de la façon suivante : à chaque arbre de référence a est associé un coefficient
stochastique p a (équivalent au coefficient stochastique associé aux règles dans une SCFG)
vérifiant la contrainte stochastique b j r ( b )= r ( a ) p b
. Ce coefficient stochastique sera appelé
probabilité élémentaire de a.
La probabilité P d'une décomposition est alors définie comme le produit des
probabilités élémentaires de chacun des a i , et la probabilité d'un arbre d'analyse quelconque
a , appelée DOP -probabilité et notée P DOP a 6 , est définie comme la somme des probabilités
de toutes ses décompositions. Notons la différence, pour un arbre élémentaire, entre sa DOP -
probabilité et sa probabilité élémentaire: la première est toujours supérieure ou égale à la seconde et ne lui est égale que dans le cas particulier où l'arbre de référence considéré ne possède
pas d'autre décomposition que lui-même sur l'ensemble des arbres de référence.

            
Extraction de l'arbre le plus probable

Le fait de pouvoir produire toutes les analyses de la séquence à analyser en un temps polynômial
ne garantit en aucun cas que l'on puisse déterminer l'analyse la plus probable en un temps
polynômial. En effet, non seulement une séquence donnée peut avoir un nombre exponentiel
d'arbres d'analyse, mais un arbre d'analyse peut de plus lui-même avoir un nombre exponentiel
de décompositions. Il n'est donc pas possible, dans le cas général, d'extraire de façon efficace
l'arbre d'analyse le plus probable simplement en parcourant toutes les décompositions possibles. En fait, il a été démontré que ce problème d'extraction est NP-difficile (Sima'an, 1996).
Cependant, le fait qu'il n'existe pas d'algorithme polynômial exact permettant de trouver
l'arbre d'analyse le plus probable ne signifie pas qu'il n'existe pas d'algorithme polynômial
permettant d'estimer cet arbre d'analyse avec une probabilité d'erreur aussi petite que possible. C'est précisément cette stratégie qui est appliquée dans (Bod, 1995). Le problème est
alors de trouver un algorithme d'échantillonnage probabiliste qui permette l'extraction d'un arbre d'analyse en un temps polynômial 7 et soit compatible avec le modèle probabiliste utilisé,
c.-à-d. tel que la probabilité d'obtenir par échantillonnage un arbre d'analyse a soit être égale à
P DOP a j w , sa DOP -probabilité sachant la séquence w à analyser.

4. graphiquement de gauche à droite
5. ou « arbres syntaxiques »
6. Ceci est une notation un peu abusive car en toute rigueur il s'agit de la probabilité P DOP ( a; w ) puisqu'en effet
la somme P des DOP probabilités d'arbre couvrant la même phrase est égale à la probabilité de la phrase, c.-à-d.

          
Échantillonnage

Pour une séquence de mots à analyser w fixée, la sélection d'une décomposition parmi toutes
les décompositions possibles de tous les arbres d'analyse possibles de w s'effectue par une suite
de choix successifs (et indépendants) d'éléments de décomposition.
Plus précisément, si un élément de décomposition e est un couple u e ; e , où u e est
un arbre de référence et e un tuple de taille p
d'indices strictement croissants, p étant le
nombre de feuilles de u e , alors une décomposition a  formule  formule  formule  formule  formule  formule  formule  formule  formule  formule  formule  formule  formule  formule  formule  formule  formule  formule  formule  formule . Ce qui signifie en particulier que les probabilités d'échantillonnage utilisées doivent également pouvoir être
calculées en un temps polynômial.
. Pour 
          
Calcul de l'analyse la plus probable par échantillonnage

Nous avons montré dans la section précédente que, si l'on choisit convenablement la loi
de probabilité utilisée pour l'échantillonnage au sein des arbres syntaxiques associés à une
séquence de mots donnée, la probabilité de sélectionner l'un quelconque des ces arbres est
égale à la probabilité conditionnelle de cet arbre au sens du modèle DOP.
L'idée intuitive qui est alors à la base de l'approche par échantillonnage est de chercher à
identifier l'arbre le plus probable sur la base de sa fréquence relative d'occurrence au sein d'un
échantillon produit de façon aléatoire à l'aide du mécanisme d'échantillonnage détaillé à la
section 3. En effet, puisque cette fréquence relative va tendre vers la probabilité de l'arbre, pour
une taille d'échantillon suffisamment grande, l'arbre le plus fréquent sera aussi l'arbre le plus
probable.
Toute la question est alors de contrôler le mécanisme d'échantillonnage de façon à garantir
(avec une probabilité d'erreur fixée a priori) la justesse de la procédure de sélection fréquentielle. Ceci correspond à un problème classique d'ordonnancement statistique car, si k est le
nombre d'arbres associés à la séquence de mots analysée ( k peut être calculé sans sur-coût
algorithmique lors de l'analyse syntaxique), rechercher l'arbre le plus probable revient à déterminer la modalité la plus probable d'une variable aléatoire discrète à k modalités (chacune des
modalités correspondant à l'un des arbres d'analyse possibles) suivant une loi multinomiale.
La méthode de sélection proposée par R. Bod (1995) constitue un premier exemple d'un tel
mécanisme. Elle souffre cependant de l'imprécision introduite par l'estimation de la probabilité
de sélection erronée utilisée 13 dont il est extrêmement difficile d'évaluer l'impact sur la qualité
des résultats obtenus. De plus, la nature purement séquentielle de la méthode (qui fournit en fait
un critère d'arrêt pour l'échantillonnage) a pour conséquence qu'elle ne permet pas un calcul a
priori de la taille d'échantillon nécessaire.
Pour ces différentes raisons, il nous a paru important d'utiliser des méthodes plus sophistiquées telles que celles proposées et testées dans la littérature spécialisée (sélection et ordonnancement de populations statistiques). Dans cette contribution, nous nous limiterons à
la présentation de la méthode de sélection séquentielle Bechhofer-Kiefel-Sobel avec troncature (Bechhofer et Goldsman, 1985) (appelée ci-après BKST ), qui est une des meilleures connues pour le problème de la sélection de la modalité la plus probable d'une variable aléatoire
discrète suivant une loi multinomiale.
La procédure de sélection BKST est en fait la combinaison d'une méthode de sélection
séquentielle (appelée ci-après BKS ) 14 (Bechhofer et al., 1968) et d'une méthode de sélection
non-séquentielle (appelée ci-après BEM ) (Bechhofer et al., 1959).

            
La procédure de sélection non-séquentielle BEM

Pour toute variable aléatoire discrète à k modalités k ) suivant une loi de probabilité
multinomiale, nous noterons ci-après par  formule  formule 
            
La procédure de sélection séquentielle BKS et sa troncature

La procédure de sélection séquentielle BKS repose sur la propriété fondamentale suivante (Bechhofer et al., 1968; Levin, 1984) : pour toute variable aléatoire discrète à k modalités
suivant une loi multinomiale vérifiant p [1] p [2] (avec > fixé), si l'on note f  formule 
          
Implémentation

Le but de cette section est d'expliciter quelques aspects d'implémentation importants de
notre algorithme d'échantillonage. Nous nous intéresserons en particulier au calcul, durant la
phase de construction de la forêt d'analyse, des probabilités P 0 nécessaires pour l'échantillonnage (ce calcul doit pouvoir être fait dans un temps polynômial) ; puis à la nature et au coût de
l'extraction aléatoire d'une décomposition.

            
Calcul ascendant (bottom-up) des probabilités

L'algorithme d'analyse syntaxique utilisé dans notre approche est une version généralisée de
l'algorithme CYK et de l'algorithme Earley bottom-up, voisine des travaux de Graham et al.
(1980). Les détails de cet algorithme sont donnés dans (Chappelier et Rajman, 1998). Parmi
ses caractéristiques importantes on peut citer (1) qu'il permet de traiter des grammaires SCFG
quelconques (i.e. non limitées à la forme normale de Chomsky) en effectuant, à l'aide d'une
généralisation des « Earley items », une binarisation dynamique de la grammaire 19 ;(2) que le
calcul des probabilités 20 de ces « items généralisés » peut être réalisé de façon ascendante sans
augmentation de la complexité algorithmique de l'analyse syntaxique 21 . Ce calcul s'effectue en
effet de proche en proche au cours de la construction « bottom-up » des interprétations, en multipliant le coefficient de la règle appliquée par les deux probabilités partielles (déjà calculées)
des deux constituants utilisés.

            
Extraction descendante (top-down) des décompositions

Contrairement à l'approche proposée par R. Bod (1995), nous avons choisi une méthode
d'extraction descendante, semblable à celle utilisée pour l'extraction des arbres d'analyse dans
le cadre des SCFG classiques. La différence importante est que les choix (des éléments constitutifs des décompositions) sont ici aléatoires au lieu de correspondre à la recherche d'une
probabilité optimale. À chaque étape un nouveau constituant de décomposition est choisi de
façon aléatoire suivant la probabilité P E dont les composants P 0 ont été calculés durant la phase
d'analyse. Ce processus se déroule itérativement de façon descendante à partir du symbole de
plus haut niveau jusqu'à obtention d'une décomposition complète et la démonstration effectuée
à la section 3 assure que la probabilité d'obtenir un arbre donné (sachant la séquence à analyser)
est alors effectivement égale à sa DOP -probabilité.
Notons par ailleurs qu'à la différence de la méthode proposée par R. Bod (1995), qui, pour
chaque extraction, nécessite un tirage aléatoire pour tous les terminal dans toutes les cases de la
table, il nous suffit de réaliser autant de tirages que d'éléments présents dans la décomposition.

          
 Conclusion

Dans la présente contribution, nous avons présenté deux résultats importants pour l'extraction d'arbres d'analyse dans le modèle DOP :
– Tout d'abord nous avons démontré de façon formelle le bien-fondé de la mise en oeuvre
de méthodes de type Monte-Carlo. A notre connaissance, une telle démonstration n'a été,
au mieux, que brièvement esquissée dans les publications disponibles sur le sujet.
– Ensuite nous avons proposé une méthode permettant de contrôler la qualité de l'échantillonnage et de garantir (avec un seuil de confiance fixé a priori) que l'arbre d'analyse
sélectionné est effectivement l'arbre d'analyse le plus probable.
Ces deux résultats permettent de fonder sur une base théorique plus solide les expérimentations nécessaires pour une meilleure évaluation du modèle syntaxique DOP qui apparaît aujourd'hui comme l'un des candidats prometteurs de la classe des grammaires faiblement sensibles
au contexte.




        
          
Introduction

Le "tagging", ou "étiquetage", ou "marquage", consiste à affecter une "étiquette" ("tag", ou catégorie) à chaque "mot" d'un texte. Nous proposons dans cet article de prendre du recul par rapport à l'aspect opératoire de cet outil devenu d'usage courant, et de poser des questions plus fondamentales sur ses rapports avec l'analyse syntaxique automatique traditionnelle (section 1 sur le champ d'action théorique des déductions contextuelles (section 2 sur le rôle des ressources lexicales (section 3 sur la définition du jeu d'étiquettes (section 4) et sur le renouveau qu'il pourrait apporter en analyse syntaxique (section 5).

          
Tagging et analyse syntaxique traditionnelle

            
Deux opérations complémentaires dans une chaîne de traitement

Par "analyse syntaxique traditionnelle", nous entendons l'analyse syntaxique dans son état
canonique telle que la décrit Gérard Sabah dans le chapitre 2 de (Sabah 1989 pages 37 à 71 :
"Les principes de l'analyse syntaxique des phrases".

              
Chaîne de traitement de l'analyse syntaxique traditionnelle

La chaîne de traitement de l'analyse syntaxique traditionnelle (voir figure 1 ci-dessus)
comprend au minimum deux modules : l'analyseur morpho-lexical, qui, à partir de ressources
lexicales considérées comme exhaustives, produit une liste de "tokens" ("mots" arbitraires)
munis chacun d'une ou plusieurs étiquettes, suivi de l'analyseur syntaxique proprement dit,
qui, pour chaque phrase, produit zéro, un ou plusieurs arbres syntagmatiques.

Le problème central de l'analyse syntaxique traditionnelle est son aspect combinatoire :
l'analyseur doit choisir une étiquette pour chaque "token", et l'ensemble des étiquettes choisies
(par les substitutions lexicales) doit permettre d'attribuer à la phrase une structure syntaxique
attendue, reconnue dans l'inventaire exhaustif des structures attendues, codé sous la forme
d'une grammaire syntagmatique. La cause profonde de cet aspect combinatoire est que l'analyse
syntaxique traditionnelle est fondée sur les principes de la compilation, analyse d'un langage
formel dont le lexique est clos (les mots ayant une seule catégorie) et dont la syntaxe est
exhaustivement définie par une grammaire formelle, alors qu'une langue a un lexique ouvert
(les mots ayant plusieurs catégories) et une syntaxe partiellement définie.
Dans la nouvelle chaîne de traitement, le tagger vient se substituer à l'analyseur morpho-lexical, pour fournir à l'analyseur syntaxique une liste de "tokens" munis chacun d'une seule
étiquette (ou le moins possible et ainsi annuler, ou réduire le plus possible, la combinatoire
des catégories possibles, par l'apport de nouvelles ressources : les déductions contextuelles
(voir figure 2 ci-dessous).

              
Nouvelle chaîne de traitement de l'analyse syntaxique traditionnelle

            
Deux démarches opposées vis-à-vis des structures et des processus

Dans l'analyse syntaxique traditionnelle, l'ensemble des structures syntaxiques attendues est
exhaustivement explicité sous la forme d'une grammaire syntagmatique.
À l'opposé, le tagging privilégie le processus par rapport aux structures. Le processus du
tagging est une propagation de déductions contextuelles sur les tokens (voir figure 3 ci-dessous). Le processus est conduit par les fréquences des contiguïtés des catégories dans un
tagger statistique, ou explicité par des règles contextuelles symboliques dans un tagger
symbolique, mais aucune structure n'est attendue explicitement. L'algorithme d'un tagger
consiste à tester sur chaque token l'applicabilité d'un nombre constant de règles, et ceci lui
confère une complexité linéaire en temps par rapport au nombre de tokens de la phrase. Dans un
renversement complet par rapport à l'analyse syntaxique traditionnelle, le tagging explicite le
processus mais n'explicite pas les structures.

              
Processus de propagation de déductions contextuelles dans le tagging

Du point de vue de son processus, un analyseur traditionnel intègre toutes les caractéristiques
du modèle de la compilation, à la différence non négligeable qu'un compilateur analyse un
langage de programmation, exhaustivement connu et modélisé, et qu'un analyseur syntaxique
de langue analyse une langue, connue partiellement, et donc partiellement modélisée. En
conséquence, alors que le processus d'un compilateur est déterministe, celui d'un analyseur
syntaxique traditionnel de langue est non déterministe, car l'absence de critères locaux pour
prendre des décisions locales implique des retours en arrière aux tokens précédents (voir les
"points d'embarras" de Sabah 1989, page 58). Les algorithmes sont de complexité polynomiale
avec des "formalismes simples", ou bien le problème devient NP-complet "pour les
formalismes plus évolués, comme les grammaires d'unification" (voir Abeillé et Blache 1997,
pages 79 et 80).

          
Les déductions contextuelles dans le tagging : leur champ d'action théorique

Tentons de circonscrire a priori le champ d'action des déductions contextuelles.
Définissons un segment intermédiaire entre les mots et les phrases : le syntagme non récursif
(SNR dans la suite de cet article ou syntagme simple, ou syntagme noyau, ou syntagme sans
ses syntagmes compléments, ou "core phrase", ou "chunk" dans la littérature en anglais (voir
Abney 1996). Ce segment est stable entre des langues différentes, et trouve approximativement
son équivalent à l'oral sous la forme du groupe accentuel.
On a donc une hiérarchie de segments à trois niveaux : mots, SNR, phrases, hiérarchie où le
segment d'un niveau est constitué de segments du niveau inférieur : un tout est d'un type
différent du type de ses parties, contrairement au syntagme récursif, qui est constitué de mots et
de syntagmes récursifs (certaines parties sont de même type que le tout).
Les mots dans un SNR, et les SNR dans une phrase ont des comportements très différents :
les mots d'un SNR forment un agrégat très contraint autour d'un nom ou d'un verbe, une
structure stable et explicitable exhaustivement, mais les SNR dans une phrase sont soumis à des
contraintes plus relâchées, et forment des structures instables, dont l'explicitation exhaustive
nous semble actuellement un objectif hors de portée.

On ne peut alors considérer une phrase comme une suite continue, indifférenciée de mots,
dans laquelle toutes les contiguïtés seraient équivalentes. On doit considérer de manière
différente une contiguïté de deux tokens à l'intérieur d'un SNR, et une contiguïté de deux tokens
à la frontière de deux SNR contigus. Une déduction contextuelle sûre devra donc s'appuyer sur
la structure interne stable du SNR et être interne au SNR, et non entre le dernier token d'un SNR
et le premier du SNR suivant.
Comme un SNR est constitué d'un élément central lexical (nom ou verbe entouré d'éléments
périphériques grammaticaux (prépositions, déterminants, clitiques, adverbes, ... la déduction
contextuelle canonique consiste en une déduction de la forme :
Plus précisément, un mot grammatical marque le plus souvent (en français) le début d'un
SNR ainsi que le type de ce SNR , ce qui signifie que la déduction est indirecte car elle passe par
le type du SNR :

On peut alors définir un cas où il est impossible de faire une déduction locale et interne au
SNR : c'est le cas d'un SNR constitué d'un seul mot lexical. Cette déduction locale impossible ne
porte pas à conséquence si ce mot n'a qu'une seule catégorie possible.
Dans l'exemple suivant, remporte constitue à lui seul un SNR, et ne peut être que verbe :
Comme prévu, M. Museveni remporte la quasi-totalité des votes dans l'Ouest, ...
Mais, si ce mot a plusieurs catégories possibles, alors la décision ne peut pas être prise par
déduction contextuelle locale : la décision ne pourra être prise que par la mise en relation du SNR
de type multiple avec un SNR de type connu. Dans le corpus qui a servi à l'évaluation des essais
de l'action GRACE (action internationale d'évaluation comparative des "taggers" du français 1 )
en septembre 1996 (environ 10 000 mots du journal Le Monde, fichier "lemon06" environ 1%
des mots sont dans ce dernier cas (1/6 sont des noms, 1/6 sont des verbes, et 2/3 sont des
adjectifs ou des participes passés en position adjectivale).
Dans l'exemple suivant, montre constitue à lui seul un SNR, et peut être verbe ou nom ; la
décision ne peut pas être prise localement, mais par la mise en relation avec son sujet présence:
La présence de Florence Arthaud au milieu d'un plateau de spécialistes montre que cette
Transat a été la course la plus disputée de ces dix dernières années.
Cette déduction est du type :

En conclusion de cette section, l'étiquetage des mots qui forment à eux seuls un SNR de type
multiple est impossible par déduction contextuelle (il devra donc être confié à une étape
ultérieure de l'analyse). Notons en outre que tout étiquetage fondé sur une relation entre deux
SNR est aussi impossible par déduction contextuelle ; citons par exemple : la résolution de de d'
du des préposition ou partitif-article, la résolution de que qu' conjonction ou pronom relatif, les
genre et nombre des pronoms relatifs selon leur antécédent, les genre, nombre, cas, type des
clitiques réfléchis et des nous et vous, toute propagation de genre, nombre, personne par accord
entre SNR (sujet - verbe, antécédent - pronom relatif, ...).

          
Les ressources lexicales dans le tagging

Dans les sections précédentes, nous avons focalisé notre attention sur les déductions
contextuelles. Mais ces déductions s'appuient sur des ressources lexicales, et doivent s'articuler
avec elles. Étudions de quelle manière.

            
Trois articulations possibles entre ressources lexicales et déductions contextuelles

Voyons comment le problème du tagging est habituellement posé : pour chaque token,
toutes ses catégories sont exhaustivement énumérées à partir de sources d'informations lexicales
(lexique de lemmes ou de formes, reconnaisseur de formes verbales, règles sur les finales - ou
"guesser" - pour les mots absents du lexique), et le tagger doit choisir parmi les catégories
possibles (comme l'homographie polycatégorielle est couramment appelée "ambiguïté", ce
processus de choix est souvent appelé "désambiguïsation", vue comme une annulation ou une
diminution de l'"ambiguïté").
Une première solution pour effectuer un choix est d'attribuer au contexte le rôle de supprimer
des catégories possibles de cette liste (c'est le cas des grammaires de contraintes - voir
Voutilainen 1994). Nous appellerons une telle déduction : "déduction négative", qui correspond
à une règle de la forme :
dans tel contexte, tel token ne peut pas avoir telles catégories
Le principal défaut d'une déduction négative est que la catégorie attendue du token doit
appartenir à la liste des catégories possibles, ce qui revient à faire l'hypothèse que tout token
appartient au lexique et est muni de la liste exhaustive de ses catégories possibles, hypothèse
manifestement fausse pour les mots lexicaux, même avec un "guesser". Une autre solution pour choisir est d'utiliser des "déductions affirmatives", de la forme :
dans tel contexte, tel token a telle catégorie.

Cette solution a l'avantage de pallier cette double incomplétude des ressources lexicales :
certains tokens n'y sont pas, et certains tokens présents n'ont pas toutes leurs catégories
possibles.
Par exemple, dans je positive, positive est pré-étiqueté adjectif dans le lexique, et la
déduction est la suivante :

Mais il y a encore une autre manière de poser le problème : on remarque que les différentes
catégories possibles d'un token sont loin d'être équiprobables : en général une catégorie est de
beaucoup la plus fréquente. Prenons l'exemple caricatural de le l' la les dans ce même corpus de
10 687 mots 2 du journal Le Monde : 1054 occurrences qui se répartissent en 1029 déterminants
(97,6%), et 25 pronoms clitiques objet (2,4%).
Au lieu de poser au départ des déductions que ces graphies peuvent être déterminants ou
pronoms, posons qu'elles sont déterminants par défaut (ces informations par défaut sont
codées dans le lexique et qu'elles seront pronoms dans un contexte particulier (ces informations liées au contexte sont codées dans les règles de déduction contextuelle du tagging) :
On trouvera une démarche analogue dans (Chanod et Tapanainen 1995, page 151, 4.2.2
mais restreinte à certains mots grammaticaux qui ont une homographie très rare avec un mot
lexical (est, cela, avions homographie détectée à l'aide du contexte.
Donner une catégorie par défaut dans le lexique, et la modifier éventuellement par le contexte
constitue en quelque sorte une implémentation du concept de translation de Lucien Tesnière
(voir Tesnière 1959, à partir de la page 361).

            
Types d'homographies polycatégorielles, et étude statistique

Étudions les principaux types d'homographies, et faisons-en une étude statistique sur ce
même corpus du journal Le Monde (de 10 687 mots), corpus pour lequel nous disposons d'un
étiquetage manuel réalisé par des linguistes du comité de coordination de l'action GRACE

Ce comptage a été effectué automatiquement dans un éditeur (BBEdit sur Mac) : l'apostrophe
et le tiret sont des séparateurs, la ponctuation et les guillemets ne sont pas comptés comme mots.

selon des spécifications proposées par le comité de coordination, puis discutées durant l'adjudication des essais. En consultant automatiquement des ressources lexicales incluant les homographies, nous obtenons les résultats suivants, en mettant en évidence l'alternative la plus rare :
- homographies sur les catégories de mots grammaticaux :
sur 1054 le l' la les homographes déterminant / pronom
2,4% de pronoms
8,5% de partitifs-articles sur 1088 de d' du des homographes préposition / partitif-article
16,2% de pronoms relatifs sur 130 que qu' homographes conjonction / pronom relatif
sur 171 est être avoir homographes verbe auxiliaire / nom
4,7% de noms
17,1% de noms
sur 111 bien mal moins plus ... homographes adverbe / nom
- homographies sur les catégories de mots lexicaux :
16,4% de verbes
33,8% de noms
sur 487 homographes nom / verbe non auxiliaire
sur 714 homographes adjectif / nom
(ces derniers concernent les absents du lexique, normalement étiquetés par notre
analyseur adjectif ou nom, par déduction locale dans le SNR nominal ou verbal)
- homographies sur les attributs de noms, adjectifs, pronoms, verbes :
Cette étude confirme qu'une des alternatives est toujours beaucoup plus fréquente.

            
Une expérience d'évaluation de l'étiquetage par défaut

Pour évaluer l'intérêt et l'importance des étiquettes par défaut, codées dans les ressources
lexicales, par rapport aux déductions contextuelles, faisons l'expérience d'étiqueter uniquement
avec les étiquettes par défaut des ressources lexicales, sans aucune déduction contextuelle
locale. Comme valeurs par défaut, prenons systématiquement l'alternative la plus fréquente :
L'expérience porte sur ce même corpus de 10 687 mots du journal Le Monde des essais de
l'action GRACE, étiqueté à la main par des linguistes du comité GRACE. Nous pouvons alors
comparer un étiquetage automatique avec l'étiquetage manuel 3.
Le jeu d'étiquettes GRACE est dérivé du jeu MULTEXT 4 ; il comprend 11 catégories de
base, avec 0 à 6 attributs ayant de 2 à 8 valeurs, ce qui donne 311 étiquettes différentes. La
tokenisation GRACE est très fine : tout mot contenant une apostrophe ou un tiret est subdivisé,
et apostrophes et tirets sont des tokens (respectivement 4,8% et 1,0% des tokens comme le
reste de la ponctuation (10,5% des tokens ce qui donne 12741 "tokens" pour 10 687 "mots".
 Pour les mettre en conformité avec les spécifications de l'étiquetage GRACE résultant de
l'adjudication des essais, nous avons modifié environ 2% des tokens du corpus de référence
étiqueté à la main.
4 
Dans ces conditions expérimentales, et en reproduisant aussi fidèlement que possible le
protocole d'évaluation GRACE, on obtient alors les résultats suivants :
aucune étiquette multiple
92,1% des tokens ont la même catégorie de base que dans le corpus de référence
(décision 1,0000)
(précision sur les catégories de base 0,9208)
82,5% des tokens ont exactement la même étiquette que dans le corpus de référence
(précision sur les étiquettes complètes 0,8254)
En conclusion de notre expérience, nous constatons que les ressources lexicales avec des
valeurs par défaut, sans aucune déduction contextuelle, permettent d'obtenir 9 catégories de
base sur 10 égales à celles du corpus de référence, sur le jeu de 11 catégories de base, et 8
étiquettes complètes sur 10 égales à celles du corpus de référence, sur le jeu de 311 étiquettes
complètes (notons qu'il est important de préciser avec quelle tokenisation et avec quel jeu
d'étiquettes on obtient telle précision). Avec les seules ressources lexicales, l'étiquetage
automatique a ainsi parcouru la majeure partie du chemin vers l'étiquetage manuel.
Nous faisons l'hypothèse que la valeur par défaut (venant du lexique jusqu preuve du
contraire (apportée par le contexte est une propriété générale des langues, permettant une
économie dans les processus de communication langagiers.

            
Contributions des déductions contextuelles et des mises en relation

En introduisant successivement les déductions contextuelles, puis les mises en relations des
voici un tableau résumant les valeurs obtenues en précision (la décision reste égale à
1,0000) :
En comparant les contributions des trois types de ressources, on observe la répartition
suivante (en précision sur les étiquettes complètes) :
- le lexique avec des valeurs par défaut résout 82,5% des tokens,
- la déduction contextuelle améliore la précision pour 12,3% des tokens, ce qui fait plafonner
la précision à 94,8
- et enfin la mise en relation des SNR permet de résoudre 2,3% des tokens, qui sont environ
pour moitié les mots qui forment à eux seuls un SNR de type multiple (ceux évoqués ci-dessus en section 2 et pour moitié certains mots grammaticaux étiquetés par la mise en
relation : résolution de de d' du des préposition ou partitif-article, résolution de que qu'
conjonction ou pronom relatif, genre et nombre des pronoms relatifs selon leur
antécédent, genre, nombre, cas, type des clitiques réfléchis et des nous et vous.
L'écart de 3% en précision sur les étiquettes complètes qui sépare les 97,1% des 100% est
dû pour moitié à des erreurs de notre analyseur, et pour moitié à un écart incompressible dû aux
faits que l'étiquetage manuel ne peut être d'une régularité parfaite et que les spécifications aussi
précises soient-elles ne peuvent pas prévoir tous les cas réels.
Remarque sur les valeurs des précisions données ci-dessus : ces valeurs ne peuvent pas être
comparées avec les valeurs obtenues par d'autres taggers dans d'autres conditions
expérimentales : tokenisation, jeu d'étiquettes (nombre d'étiquettes, jeu de type traditionnel ou
distributionnel corpus, spécifications d'étiquetage du corpus de référence, protocole et
métrique de calcul des écarts entre résultats calculés et corpus de référence.

L'intérêt de ces
valeurs est surtout de permettre ici la comparaison relative entre les apports des trois types de
ressources dans la précision du tagging. La comparaison des performances de taggers différents
ne peut se faire avec rigueur que s'ils sont comparés dans le cadre de conditions expérimentales
strictement identiques, comme dans l'action GRACE par exemple. Remarquons l'importance du
nombre d'étiquettes dans la comparaison des précisions de différents taggers : un petit nombre
d'étiquettes rend la décision plus facile (la probabilité est plus grande de tomber par hasard sur
la bonne étiquette mais jusqu un certain point, car un trop petit nombre d'étiquettes rendrait
les déductions contextuelles locales plus difficiles par manque de finesse et de régularité dans la

          
Définition du "token", définition du "tagset"

Le mot, qui exprime une segmentation conventionnelle et instable entre des langues
différentes, et évolutive à l'intérieur d'une langue, n'est pas le bon candidat pour être
systématiquement le "token", segment de phrase, unité traitée dans un traitement de langue : un
mot peut être divisé en plusieurs tokens (cas des amalgames, des mots composés ou plusieurs
mots peuvent être regroupés en un seul token : locutions diverses, noms propres composés,
numéraux composés, mots composés. Le token est défini en rapport avec l'objectif du
traitement.
Le "tagset", ou jeu d'étiquettes des tokens, doit-il, par fidélité aux traditions, reprendre les
parties du discours? Rappelons-nous que les parties du discours constituent une taxinomie
traditionnelle et empirique des "mots", consacrée surtout à l'enseignement, une théorie parmi de
nombreuses autres possibles (voir un exemple de théorie originale dans Tesnière 1959, pages
51 à 94 et que nous avons toute liberté d'en créer une autre, adéquate au tagging. Posonsnous alors la question : quelles seraient les caractéristiques d'un jeu d'étiquettes adéquat aux
déductions contextuelles, déductions sur les régularités des contiguïtés des catégories, sur les
régularités distributionnelles, captées à travers le filtre du jeu d'étiquettes.
Par exemple, ces trois étiquettes traditionnelles caractérisent un SNR nominal :
"une" : article "cette" : adjectif démonstratif
Elles sont maintenant plus souvent regroupées sous une étiquette unique : le déterminant,
étiquette fondée sur la régularité de la classe distributionnelle des déterminants, différenciée de
celle des adjectifs.
"sa" : adjectif possessif
En revanche, "il", "elle", "nous", "y", "ceux", "qui", "dont" sont tous des pronoms, mais
ils ont des distributions bien différentes !
Donc, pour capter (puis utiliser) des régularités distributionnelles, chaque étiquette du jeu
d'étiquettes doit définir une classe distributionnelle de tokens. Par exemple, parmi les adjectifs,
on devra différencier les épithètes antéposées, les épithètes postposées, et les attributs, car leurs
distributions sont différentes.
Un jeu d'étiquettes distributionnel, associé au concept de SNR nominal ou verbal, implique
que ce jeu est partitionné en deux sous-ensembles (leur intersection est vide) : le jeu des
étiquettes dans le SNR nominal et le jeu des étiquettes dans le SNR verbal. Par exemple,
l'étiquette de l'adjectif épithète, dans le SNR nominal, est différente de celle de l'adjectif attribut,
dans le SNR verbal.

          
Les concepts du tagging : un renouveau pour l'analyse syntaxique

Comme nous l'avons vu en section 1.2., l'apport conceptuel principal et original du tagging
est l'explicitation des processus, associé à l'abandon de l'explicitation des structures. Les
processus explicités sont des déductions contextuelles.
Nous y ajoutons le concept central de syntagme non récursif (défini en section 2 des
ressources lexicales avec des valeurs par défaut (étudiées en section 3) généralisées pour
l'ensemble des mots grammaticaux, et un jeu d'étiquettes distributionnel (présenté en section 4).
Le tagger, devenu moteur de déduction contextuelle, produit en sortie, non seulement une liste
de tokens étiquetés, mais aussi une liste de syntagmes non récursifs délimités et typés. Environ
1% des tokens ont à la fois les deux propriétés suivantes : ils ont plusieurs catégories possibles parmi verbe, nom, adjectif et ils constituent seuls un SNR de type multiple (voir ci-dessus en section 2).

Les autres tokens et les autres SNR ont respectivement une catégorie unique et un
type unique. Après les déductions contextuelles, les catégories par défaut de de d' du des et que
qu' sont encore respectivement préposition et conjonction.
Que manque-t-il alors pour terminer l'analyse syntaxique? Il faut relier les SNR (relations de
dépendances, d'apposition, de coordination, d'antécédence, ...).
Dans l'esprit du tagging (l'explicitation des processus nous avons conçu un processus de
mise en relation des SNR, qui ne fait aucune hypothèse explicite sur les structures syntaxiques
situées entre les SNR reliés, ni sur la distance qui les sépare. Ce processus est implémenté
comme dans le tagger, sous la forme d'un moteur qui interprète des règles 5 qui explicitent le
processus. Ses principes et son implémentation sont décrits dans (Giguet et Vergne 1997).
L'ensemble, les ressources lexicales, les deux moteurs et les deux bases de règles,
constituent un analyseur de complexité linéaire, dans lequel les processus sont explicités et qui
produit en sortie les structures syntaxiques sous la forme de syntagmes non récursifs reliés
(voir figure 4 ci-dessous).

            
Tagger intégré dans notre analyseur

Les résultats sur des corpus variés en français, articles de journaux (Le Monde littérature,
textes scientifiques sont visibles sur internet à l'adresse : httpwww.info.unicaen.fr/~giguet.
C'est cet analyseur que nous avons utilisé pour réaliser l'expérience rapportée ci-dessus (en
section 3.3.). C'est aussi cet analyseur qui nous a permis de participer à l'action GRACE, en lui
ajoutant en module final une fonction de transfert qui transforme notre tokenisation et notre
étiquetage en la tokenisation et l'étiquetage GRACE. Les déductions ayant été faites sur notre
jeu d'étiquettes, ce transfert permet de faire les évaluations sur le jeu d'étiquettes GRACE. Ce
dernier, à cause de sa trop grande fidélité aux parties du discours traditionnelles et de ses
propriétés distributionnelles insuffisantes, n'est pas très approprié aux déductions contextuelles.
Mais il a l'intérêt majeur dtre le résultat d'un large consensus de la communauté scientifique
du tagging du français, et de permettre de faire les calculs d'écart entre le corpus de référence et
les résultats de différents taggers dans des conditions expérimentales unifiées et rigoureuses.

          
 Conclusion

Face aux difficultés rencontrées par l'analyse syntaxique traditionnelle, principalement dues à
son aspect combinatoire, et à l'obligation de disposer d'un inventaire exhaustif des structures
syntaxiques d'une langue, le tagging constitue une échappatoire prometteuse, mais il s'est
surtout centré sur ses aspects opératoires, et s'est peu interrogé sur ses bases théoriques.
 Dans les deux cas, les règles sont de la forme : conditions (cid:222)
actions.

Dans cet article, nous avons tenté de pallier cette lacune, en montrant que le tagging prend le
contre-pied de l'analyse syntaxique traditionnelle en mettant l'accent sur l'explicitation des
processus, et que, par là même, il ouvre la voie au renouveau de l'analyse syntaxique en la
fondant sur l'explicitation des processus : processus de déduction contextuelle dans les
syntagmes non récursifs, et processus de mise en relation des syntagmes non récursifs. On
étend ainsi à toute l'analyse syntaxique les propriétés calculatoires du tagging, et on obtient ainsi
des algorithmes de complexité linéaire en temps.
L'explicitation des processus en analyse syntaxique (au détriment de l'explicitation des
structures) pourrait peut-être aussi conduire des linguistes à déplacer leur intérêt, des structures
vers les deux processus de transformation entre les deux "ordres" ainsi définis par Tesnière
dans (Tesnière 1959), page 16, § 1 :
1.- L'ordre structural des mots est celui selon lequel s'établissent les connexions.
et page 18, § 8 :
8.- Nous appellerons ordre linéaire celui d'après lequel les mots viennent se ranger sur la
chaîne parlée. L'ordre linéaire est, comme la chaîne parlée, à une dimension.
Tesnière définit alors ainsi les deux processus de transformation 6 entre ces deux "ordres",
page 19, § 4 :
[...] nous pouvons dire que [...] parler une langue, c'est en transformer l'ordre structural
en ordre linéaire, et inversement que comprendre une langue, c'est en transformer l'ordre
linéaire en ordre structural.
 Processus appelés "génération" et "analyse" dans la communauté du TAL.




        
          
 Introduction et état de l'art

L'interrogation de bases de données textuelles est l'un des domaines où la technologie visant
la compréhension du langage naturel n'a pas vraiment réussi à fournir des systèmes très
convaincants. Les systèmes qui lisent des textes, en assimilent le contenu, et répondent à des
questions formulées librement sur ces textes seraient d'une très grande utilité pour une large
variété d'applications, notamment si aucune restriction de rédaction n'est imposée aux
questions et aux textes. Ils seraient la solution parfaite au problème de la surabondance de
l'information qui s'accentue de plus en plus de nos jours avec le réseau internet.
 Ce travail est financé par le fond national suisse de la recherche scientifique, n° de contrat : 121445448-95.


Même si actuellement (et dans les prochaines années à venir) il est possible d'implémenter pareils
systèmes, ceux-ci restent limités à des domaines particuliers, à des corpus restreints, et avec des
coûts de développement assez élevés. Ces systèmes (comme LILOG (Herzog et Rollinger
1991 restent des cas prototypiques de systèmes de laboratoire.
Actuellement, il n'y a que deux sérieux prétendants pour traiter des textes en grandes
quantités : La recherche d'informations et l'extraction d'informations. Malheureusement les
deux techniques ont de sérieux inconvénients.
Les techniques standards de recherche d'informations ("Information Retrieval" donnent la
possibilité à l'utilisateur de poser des questions de tous types et permettent la recherche dans
des collections de textes très importantes (plusieurs giga-octets). Des exemples de ces systèmes
sont : SMART (Salt-on, 1989 SPIDER (Schäuble,1993 GURU (Maarek, 1992). Le système
GURU par exemple, permet de générer des systèmes d'aide pour divers systèmes documentés.
GURU se base sur la notion d'"Affinité Lexicale" (sorte de collocation) entre deux unités du
langage. Pour tout texte de la documentation et toute question, GURU calcule les affinités
lexicales (AL) et les trie selon la quantité d'information véhiculée par chaque mot composant
les AL. Les textes qui sont présentés à l'utilisateur sont ceux qui contiennent en priorité les AL
de la question. Les systèmes de ce type sont très avantageux puisqu'il ne nécessitent aucune
connaissance préalable du domaine traité. Cependant, leur inconvénient est qu'ils sont conçus
pour rechercher des textes en entier ce qui les rend peu utiles pour des questions très spécifiques
et pour des documents dont la taille dépasse deux ou trois paragraphes. Par ailleurs, les
techniques classiques de recherche d'informations possèdent des limitations inhérentes, elles
traitent les mots comme des items syntaxiquement isolés (d'où la confusion entre "livre de
l'année" et "année du livre").
Les techniques d'extraction d'informations quant à elles sont rapides, elles sont appropriées
lorsqu'il s'agit de textes de tous types et de taille variable, mais elles sont limitées à la recherche
d'informations dans des schémas d'informations prédéfinis à l'avance. Les systèmes
développés dans le cadre de la série de conférences MUC ("Message Understanding
Conference") (ARPA, 1996) en sont des exemples, ils permettent d'extraire des informations
spécifiques depuis des textes de presse. Cependant certains de ces systèmes, comme celui
développé par R. Grishman (Grishman, 1996 et le système FASTUS (Appelt et al., 1993) qui
utilisaient des modules d'analyse syntaxique, ont été simplifiés, pour des raisons de rapidité, en
supprimant du traitement ces modules d'analyse syntaxique. Les résultats ont été bien sûr
affectés. Dernièrement, dans le cadre de MUC-6 (Grishman et Sundheim, 1996 des tentatives
ont été initiées pour remettre l'accent sur des analyses plus profondes du langage.

          
Extraction automatique de réponses : vers une approche linguistique

Il y a actuellement un besoin croissant de systèmes, ayant une très grande précision et un fort
taux de rappel, capables de localiser des informations dans des collections de textes dont la taille
ne dépasse pas le giga-octet. Ces systèmes permettraient de traiter des questions formulées
librement pour l'interrogation de collections de textes regroupant des textes bruts. On peut citer
pour exemple les systèmes d'accès aux manuels techniques en ligne, les systèmes d'aide à
l'utilisation de logiciels complexes, et les systèmes publics d'interrogation accessibles via l'internet.

Pour ces tâches, une très grande précision est requise (les questions peuvent être très
spécifiques la plupart du temps il est vital d'avoir un taux de rappel presque parfait (certaines
explications recherchées dans les manuels techniques sont uniques et dans certains cas le
temps de réponse est un facteur important (par exemple pour des activités où il faut retrouver
l'information exacte pour remédier à un imminent dysfonctionnement d'un système). Ce dont
on a besoin, c'est un système qui trouve, avec une très grande précision, les passages et les
phrases dans un texte (ou une collection de textes) dont le sens exprime la réponse exacte à une
question spécifique. C'est l'idée principale de l'extraction automatique de réponses. Le fait
d'opérer au niveau du sens des phrases (des questions et des textes) implique l'utilisation
d'informations linguistiques (syntaxiques et sémantiques) qui peuvent s'avérer très coûteuses à
implémenter. Cependant l'application qu'on envisage est destinée à la recherche dans des
collections moyennes de textes (quelques centaines de kilo-octets, parfois quelques mégaoctets et à des domaines spécifiques. Ceci fait de l'extraction automatique de réponses un
compromis raisonnable entre d'une part les systèmes questions-réponses visant à proposer des
réponses à tous types de questions, et d'autre part la recherche d'informations et l'extraction
d'informations qui peuvent traiter des corpus très importants.
Nous allons décrire dans ce qui suit ExtrAns, un système d'extraction automatique de
réponses utilisant (un sous ensemble de) la documentation en ligne du système Unix (les "man
pages"). Un premier prototype du système est opérationnel, il permet de traiter des questions
formulées librement et les textes originaux de la documentation du système Unix, même si
dans certains cas les données (les documents et les questions) ne peuvent pas être analysées
complètement. Le système est extensible d'une façon incrémentale. En effet, les
perfectionnements de la grammaire et/ou de la composante sémantique améliorent
automatiquement la précision de la recherche sans que cela n'affecte les autres composants du
système.

          
Les composants et les exigences du système

Si l'on considère qu'un système d'extraction automatique de réponses doit être capable de
traiter des textes très techniques, il a besoin d'un module très fiable qui réalise toutes les
segmentations en mots et en phrases d'un texte (qu'on appellera dans ce qui suit segmenteur,
"tokeniser" en anglais une grammaire ayant une large couverture, un analyseur syntaxique
assez robuste, un traitement des ambiguïtés, un module qui permet de construire une
représentation sémantique à partir des structures syntaxiques des phrases, et un moteur de
recherche capable d'utiliser ces représentations sémantiques.
Dans ce qui suit, nous allons décrire en détail les trois composants les plus importants du
système disposés de manière séquentielle. Premièrement, nous présenterons certaines des
difficultés engendrées par le pré-traitement de manuels techniques qui va au-delà de ce qu'en
général un segmenteur devrait faire. Nous ne décrirons pas dans cet article l'analyseur
syntaxique utilisé qui est un système existant, qu'on a adapté, appelé "Link Grammar" de
Sleator et Temperley (Sleator et Temperley, 1991). "Link Grammar" est, à notre connaissance,
le meilleur système disponible pour une utilisation en recherche pour l'anglais. Il est basé sur
une grammaire de dépendances et utilise un dictionnaire des formes.


est fournie sous forme d'un ensemble de connections étiquetées entre les mots formant la
phrase. Même si ce système est assez robuste, on l'a enrichi avec quelques stratégies pour
transformer les constituants des phrases non traités en mots-clés. Deuxièmement, nous
décrirons les principes permettant de construire des représentations sémantiques à partir des
structures syntaxiques produites par "Link Grammar". Nous ne présenterons pas en détails le
module qui permet de résoudre les ambiguïtés pour lequel on a adopté l'approche de Brill et
Resnik (Brill et Resnik, 1994). Troisièmement, nous montrerons comment nous traitons les
ambiguïtés syntaxiques qui persistent après le précédent processus de désambiguïsation.

          
Prétraitement du langage technique

L'analyse du langage technique est en général plus simple que l'analyse de textes portant sur
divers domaines (articles de presse, etc. cependant elle requiert un important pré-traitement.
Cela concerne les segmentations, la normalisation et l'analyse de la structure des documents.

            
Segmentations et normalisation

En général, la segmentation d'un texte revient essentiellement à identifier les mots et les
phrases. Cependant, dans les textes très techniques comme les pages du manuel d'Unix, cette
tâche requiert une attention particulière. Hormis les formes régulières des mots, le segmenteur
d'ExtrAns doit reconnaître une foule de symboles spécifiques au domaine traité, il doit ensuite
les représenter sous la forme d'expressions normalisées. Les cas suivants en sont des
exemples :
Les noms de commandes : eject, nice (problème : identifier ces mots lorsqu'ils sont utilisés
comme des noms de commandes dans des phrases comme "eject is used for ...", et leur
utilisation standard comme dans "It is not recommended to physically eject media").
Les noms de fichiers et les chemins : /usr/bin/X11, usr/5bin/ls, /etc/hostname.le
(problèmes : la barre oblique ("/") en tête, à l'intérieur et en queue, les nombres et les points).
Les options de commandes : -C, -ww, -dFinUv (problème : les séquences précédées d'un
tiret peuvent être utilisées comme options de commandes ou bien dans des exemples du type
"... whose name ends with .gz, -gz, .z, -z, _z or .Z and which begins ...").
Les noms de variables : filename1, device, nickname (problèmes : reconnaître les mots
utilisés comme noms de variables, la plupart du temps comme arguments de commandes, par
exemple "... the first mm is the hour number; dd is the day ...").
Caractères spéciaux faisant partie des mots : AF_UNIX, sun_path, ^S(CTRL-S K&R
C, ANSI C, C++, (exemple : "A single % is encoded by ." marques de
ponctuation (exemples : "... corresponding to cat? or fmt?", "/usr/man/man?",
"name@domain", [host!...hosthost!username", et "<signal.h>".
La normalisation de ce type de mots spéciaux consiste, entre autres, à les codifier d'une façon
appropriée, comme c'est le cas, par exemple, des noms de commandes (autrement l'analyseur
syntaxique échoue). Heureusement, les pages du manuel d'Unix contiennent des informations,
matérialisées par les commandes de formatage, qui vont au delà du simple niveau textuel.


Ainsi, les noms de commandes sont, en règle générale, en gras, et les expressions utilisées
comme variables, en italique, par exemple :
compress [ -cvf ] [ -b bits ] [ filename ...]
Ce type d'informations est extrait des commandes de formatage et codé avec les mots
correspondants pour pouvoir être reconnus par les modules suivants du système. Par exemple,
le mot "eject" est codé "eject.com" quand il est utilisé comme nom de commande, et le mot
"filename" est codé "filename.arg" quand celui-ci est utilisé comme argument.

            
L'analyse de la structure du document

Les commandes de formatage sont utilisées quelquefois dans les pages du manuel d'Unix
de façon non systématique (ces pages ont été écrites par différentes personnes). Pour extraire
les informations liées aux commandes de formatage (voir le paragraphe précédent le
segmenteur doit palier à ces inconsistances dans les textes sources. Il doit effectuer un
important travail d'analyse de la structure des documents en collectant, par exemple, les noms
de commandes et les arguments (de chaque page du manuel) des sections "SYNOPSIS" et
"NAME". Une fois ce travail fait, il est alors possible d'analyser la section "DESCRIPTION"
où on retrouve la description de chaque commande. Le traitement des pages du manuel d'Unix
devient ainsi un traitement de chacune des sections d'une façon particulière.
Finalement, dans le but de sélectionner et présenter les passages pertinents dans les textes (et
les mettre en évidence; voir section 6 le segmenteur associe à chaque mot un certain nombre
d'informations en construisant une représentation. Ainsi, il calcule la position de chaque mot
dans le texte et associe à chaque mot la phrase et la section où il se trouve.
          
Construction des expressions logiques

La principale caractéristique du système ExtrAns consiste dans le fait que les phrases du
texte sont converties en formules logiques existentiellement fermées qui encodent les
relations sémantiques essentielles entre les mots des phrases. Toutes les formules sont
quantifiées existentiellement car, pour les besoins de la recherche d'informations, celles-ci
existent dans l'univers du discours. Les verbes (représentés par "evt", pour 'eventualité'2 les
noms ("object", pour 'objet les adjectifs et les adverbes ("prop", pour 'propriété introduisent
des concepts dans le discours auxquels on peut se référer par la suite. En particulier, un prédicat
introduit par le verbe "copy" (copier) dans la phrase "cp copies files" (cp copie des fichiers)
serait : evt(copy,e1c1,f1 où e1 représente le concept, c1 représente la commande cp
et f1 représente les fichiers. Un prédicat introduit par le nom "cp" serait : object(cp,o1,c1
où o1 représente le concept "c1 est cp".
Les clauses de Horn, qui sont le sous-ensemble du calcul des prédicats utilisé par Prolog,
sont utilisées pour implémenter les expressions logiques.
 Dans cet article, le mot "éventualité" désigne "eventuality" au même sens que T. Parsons (Parsons,
1990).

Elles sont construites sur la base
d'une ontologie utilisant différents niveaux de représentation, (pour plus de détails voir (Hess,1997). Ainsi, pour chaque verbe le système crée un prédicat d'arité fixe, d'autres prédicats sont
créés pour les compléments (obligatoires) et les modifieurs (non obligatoires). Les liens entre,
d'une part les expressions construites, et d'autre part les phrases et les mots d'où les
expressions ont été déduites sont ajoutées à la représentation.
Les deux exemples suivants illustrent une autre distinction effectuée par le système :

Si l'opération de désinstallation échoue, eject imprime un message d'avertissement)
Dans le premier exemple l'événement copier a réellement lieu (dans le monde générique des
pages du manuel d'Unix). Cela n'est pas le cas pour les deux actions dans le second exemple
(échouer et imprimer) où toutes les deux sont introduites sous la portée de la condition. On
voudrait que ces passages soient trouvés pour certaines questions (voir l'exemple de la figure
Fig. 1 mais on n'aimerait surtout pas qu'ils le soient lorsqu'il est question des différentes
façons dont un utilisateur dispose pour imprimer intentionnellement un message ("How do I
print ... "; "Comment j'imprime ..." par opposition à l'impression des messages d'erreur, qui
est simplement un effet de bord. Pour cette raison, nous avons dû modifier les bases de notre
ontologie concernant les éventualités de manière à représenter différemment les éventualités qui
ont lieu réellement. Toutes les autres sont présumées exister seulement dans l'univers du
discours 3. Par conséquent, pour les deux exemples précédents, on a les deux représentations
suivantes 4 :
qui peut être paraphrasée de la manière suivante : "x1 se réfère à un objet qui est une
commande et qui est cp, x3 et x4 sont des objets qui sont des fichiers et qui sont respectivement
filename1 et filename2, x2 est un objet qui est le contenu de x3, e1 consiste en l'événement x1
qui copie x2 dans x4, et e1 a réellement eu lieu (holds(e1. Pour la deuxième phrase on a :
où ni l'événement "failing" (échouer) ni l'événement "printing" (imprimer) (e5 et e8) ne sont
représentés comme ayant eu lieu réellement.
 Notons toutefois, que cette distinction n'est pas encore faite pour les objets et les propriétés.
 Les informations relatives aux positions des mots ne sont pas présentées ici.

L'existence réelle des éventualités peut ainsi être
inférée, bloquée, ou bien non spécifiée selon le contexte (Hobbs, 1985).
On peut constater que les expressions logiques générées sont simplifiées. En effet, les
pluriels, les modalités et les quantifications sont ignorées; les conditionnels et les négations sont
repésentés comme des prédicats normaux et non comme opérateurs logiques. Cela est suffisant
et nous paraît mieux adapté à l'extraction automatique de réponses. Par exemple, si l'utilisateur
pose la question suivante : "Can cp copy a file onto itself?" (Est-ce que cp peut copier un fichier
dans lui-même? le système peut facilement trouver "cp refuses to copy a file onto itself" (cp
refuse de copier un fichier dans lui-même qui est une information utile pour l'utilisateur. Si on
utilisait une représentation sémantique plus complexe, le système devrait recourir à des
inférences beaucoup plus coûteuses pour trouver le même résultat.
Notons aussi que certaines informations typographiques (et indirectement provenant de la
structure du document) extraites du texte par le segmenteur ont été utilisées dans cette
représentation. Le fait que "eject" est le nom d'une commande (au lieu du verbe "eject" (éjecter)
en anglais) résulte en la création d'un prédicat supplémentaire object(command,x7). De
manière similaire, "cp" est reconnu comme référant à une commande. Sans le recours à ce type
d'informations, la première phrase ne serait pas accessible par des questions du type "What
commands copy files? " (Quelles sont les commandes qui copient des fichiers? la deuxième
serait considérée non grammaticale.
Dans le but de répondre aux questions, les procédures standards de réfutation sont utilisées
pour trouver toutes les preuves possibles de la question par rapport à la base de connaissance.
Ainsi, la question
(3) "Which command prints something?"
(Quelle commande imprime quelque chose?)
devient la requête Prolog suivante, après construction de l'expression logique et recherche
des synonymes 5
et le moteur d'inférences de Prolog retourne, dans R[S1, S2,... les références aux
phrases pertinentes, dont la phrase (2).
 Les synonymes sont ajoutés en ayant recours à un thésaurus du style Word-Net (Miller et al. 1990) que
nous sommes en train de développer pour notre application. Notons que "printing" (imprimer) pour le
système Unix veut aussi dire "printing to the screen" ou bien "displaying", c'est dire (afficher à l'écran).

          
Présentation des résultats (éventuellement ambigus)

La résolution des ambiguïtés est l'un des problèmes auquel notre système a été confronté.
L'analyseur syntaxique de Sleator et Temperley (Sleator et Temperley, 1991) ne traite pas les
ambiguïtés syntaxiques et sémantiques. Une phrase longue pourrait avoir des centaines, voire
des milliers d'analyses syntaxiques différentes. ExtrAns essaie de résoudre les ambiguïtés
(syntaxiques) en deux étapes. Dans la première étape quelques règles heuristiques éliminent les
cas les plus invraisemblables. Un exemple d'une telle règle est : "Une phrase prépositionnelle
introduite par ("of") ne peut être attachée qu'au nom ou à la coordination nominale qui la
précède immédiatement". D'autres règles, ajoutées au système, éliminent certaines analyses
incorrectes non détectées par l'analyseur syntaxique, par exemple l'analyse suivante "(The files
are deleted) and (directories are copied" est incorrecte. Dans la deuxième étape, nous utilisons
l'approche de Brill et Resnik (Brill et Resnik, 1994) pour désambiguïser les phrases
prépositionnelles. Le module proposé par les deux auteurs a été entraîné sur des données
extraites de notre corpus. Avec l'aide de ce module nous pouvons utiliser des données
statistiques pour résoudre les attachements des phrases prépositionnelles, la plus importante
source d'ambiguïté.
Après ces deux étapes, le nombre des ambiguïtés est réduit même si, pour quelques phrases,
certaines persistent car elles ne sont pas traitées par l'algorithme de Brill et Resnik. Si une
phrase a plusieurs interprétations irréductibles, ExtrAns les enregistre toutes dans sa base de
connaissances. Lorsque l'utilisateur pose une question, une phrase peut être trouvée plusieurs
fois (puisque celle-ci peut avoir plusieurs expressions logiques qui sont compatibles avec la
question). Ces différentes expressions logiques peuvent mettre en évidence divers mots,
comme conséquence aux différentes interprétations de la phrase. ExtrAns traite cela en
superposant toutes les mises en évidence des phrases de manière à assigner une couleur plus
significative, selon un schéma gradué de couleurs, aux parties ayant été trouvées le plus fréquemment par le moteur d'inférences.

            
Le résultat de la question "Which command prints something?" (Quelle commande imprime quelque chose?). (La figure originale est en couleur).

Par exemple, si l'on considère la chaîne "If filename2
has a mode which forbids writing, mv prints the mode" (Si filename2 a un mode qui interdit
l'écriture, mv imprime le mode dans la figure (Fig. 1). Celle-ci fait partie de toutes les
interprétations de la phrase désignée par mv.1/DESCRIPTION/6 qui ont été trouvées en
réponse a la question de l'utilisateur. Elle est par conséquent mise en évidence avec une couleur
plus intense. En effet, c'est cette partie de la phrase qui donne une réponse plus directe à la
question.
En outre, il est possible d'accéder à la page complète du manuel qui contient la phrase
(simplement en cliquant sur le nom du fichier correspondant se trouvant à gauche de chaque
phrase). La page du manuel montre les mêmes mises en évidence de passages permettant ainsi
à l'utilisateur de noter rapidement la phrase la plus pertinente et de déterminer, par inspection du
contexte, si la phrase contient en effet une réponse à la question. Avec cette façon de présenter
les résultats, les possibles ambiguïtés passent inaperçues.

          
Conclusion et recherches futures

Conclusions. Le système tel qu'il est actuellement montre que le concept d'extraction
automatique de réponses est très utile et qu'il requiert un effort relativement limité de traitement
du langage. Il montre aussi qu'il est très facile pour les utilisateurs de faire avec les ambiguïtés
lorsque celles-ci sont fusionnées, classées, et présentées dans leur contexte.
Caractéristiques à améliorer. Pour que le système soit plus utile, on doit clairement
agrandir notre corpus de textes du manuel technique du système Unix (le système utilise
actuellement 30 descriptions de commandes). On doit par conséquent améliorer le traitement
des phrases non grammaticales par le système. A l'état actuel, le système convertit les mots
qu'il ne peut pas analyser en mots-clés. Cette méthode peut être améliorée de manière à traiter
les parties de phrases lorsque la phrase ne peut pas être analysée entièrement. En plus, pour
certaines phrases, le nombre d'analyses syntaxiques reste très important, il est donc
indispensable d'améliorer les méthodes utilisées pour désambiguïser et éliminer les analyses les
moins plausibles en utilisant éventuellement des informations sémantiques. Nous sommes
aussi en train d'étendre de façon plus systématique notre thésaurus des synonymes et
hyponymes en utilisant des outils d'acquisition automatique de collocations. On envisage aussi
d'ajouter au système la possibilité de résoudre les pronoms et les anaphores nominales.
Finalement, nous sommes en train d'améliorer les techniques d'inférence pour augmenter le
taux de rappel en intégrant au système des modules qui permettent de prendre en compte, entre
autres, le problème de distributivité de la coordination.




        
          
 Introduction

La plupart des analyseurs syntaxiques conçus pour le traitement du langage naturel sont
construits pour des textes écrits qui par définition sont conformes à une grammaire bien
définie (la grammaire standard). Les entrées prévues pour ces analyseurs doivent être
dépourvues de toutes extra-grammaticalités. Cependant, une telle conception rend ces
analyseurs fragiles et non robustes face aux différentes distorsions que l'on trouve dans le
langage oral. Cette constatation a susciter un grand nombre de questions qui se résument à
savoir comment situer l'oral par rapport à l'écrit. Au-delà des modes de communication
différents, y a-t-il d'autres dissimilitudes entre l'oral et l'écrit ? Faut-il considérer que le
traitement de l'oral est une discipline indépendante nécessitant ses propres solutions ?

Ou au contraire, faut-il traiter l'oral dans une perspective d'adaptation des solutions conçues pour
l'écrit ?
Deux points de vue se sont développés concernant l'élaboration d'un analyseur syntaxique
robuste pour le traitement des dialogues oraux. Le premier point de vue porte sur la
conception d'un analyseur syntaxique robuste des dialogues oraux à partir d'une grammaire
de l'oral. Ainsi, en augmentant les règles de grammaire standard par des règles générant
différentes constructions de l'oral, il serait possible d'envisager l'automatisation de l'analyse
syntaxique des dialogues oraux. Cependant comme l'explique Kong et al. (1995 une telle
démarche peut engendrer une augmentation du nombre de
règles de manière
disproportionnée, ce qui rendrait leur gestion impossible.
Le deuxième point de vue prévoit la conception d'un analyseur syntaxique robuste à partir
de règles de grammaire standard et d'heuristiques gérant les distorsions de l'oral.
Concrètement, ceci s'effectue en concevant un analyseur syntaxique de l'écrit (non robuste)
et en ajoutant des procédures permettant le traitement des extra-grammaticalités de l'oral.
Ces procédures sont généralement basées sur les opérations d'ajouts, d'effacements, de
transpositions ou de substitutions de constituants. Cette perspective est actuellement celle qui
est la plus utilisée. Toutefois, elle présente aussi un inconvénient lié au fait que ces méthodes
sont moins performantes pour les applications indépendantes d'un domaine (Rosé et Lavie
1997).
D'autres alternatives sont en voie de développement. Ces alternatives s'inspirent de la
deuxième approche et préconisent l'ajout d'une deuxième phase de traitement. Elle consiste à
combiner les analyses partielles obtenues pour un énoncé afin d'en extraire la combinaison la
plus plausible. Cette stratégie permettrait de résoudre le problème de la deuxième approche
lié à l'utilisation de connaissances dépendantes du domaine pour la définition des traitements
des distorsions de l'oral (Rosé et Lavie 1997).
Dans cet article, nous nous intéressons plus à la deuxième approche. Nous visons à
montrer qu'il est possible de traiter des dialogues oraux orientés tâches en exploitant une
théorie de la grammaire et une technique d'analyse essentiellement utilisée pour la langue
écrite. Le corpus sur lequel nous travaillons est une retranscription orthographique
d'enregistrements authentiques constitué par Ozkan (Ozkan 1994) et analysé par Colineau
(Colineau 1997). Ce corpus met en scène un instructeur et un manipulateur, qui n'étaient pas
dans la même pièce, mais communiquaient entre eux grâce à des microphones et partageaient
les mêmes informations sur leurs écrans (les actions exécutées par l'un étaient visibles par
l'autre). L'expérience s'est déroulée dans le contexte de l'utilisation d'un logiciel de dessin.
L'instructeur avait sous les yeux la scène à représenter et donnait des directives au
manipulateur qui ignorait ce qu'il devait dessiner. Les dialogues ont été enregistrés et filmés
et leurs transcriptions orthographiques ont été faites par Ozkan.

Le projet MAREDI (MArqueurs et REprésentation des DIscours dans lequel s'inscrivent
nos travaux, vise à développer une approche et un outil d'analyse de discours qui s'appuie
essentiellement sur la détection et l'interprétation de marques de surface afin d'élaborer un
modèle conceptuel du discours sous la forme d'un ensemble d'états mentaux structurés
(Moulin et Rousseau 1997).

Nous développons actuellement un système constitué de quatre modules principaux : un réseau neuronal qui sert
à déterminer les types d'actes de dialogue accomplis à partir d'indices syntaxiques relevés à
la surface des énoncés (Colineau et Moulin 1996 ; Colineau 1997) ; un analyseur syntaxique
robuste qui doit pouvoir bien réagir aux problèmes d'extragrammaticalité qui sont fréquents
dans l'oral ; et un analyseur sémantique mettant en oeuvre une approche à base de cas
sémantiques et ayant recours à l'utilisation de templates (Boufaden et al. 1997) ; et,
finalement, un intégrateur qui, à partir des structures produites par les trois autres composantes, procède graduellement à la construction du modèle conceptuel du discours analysé.
Dans ce qui suit, l'analyseur syntaxique robuste constitue le thème principal de nos propos :
nous faisons abstraction des liens éventuels que pourrait avoir un tel analyseur avec un
module de reconnaissance de la parole.

          
Analyseur syntaxique robuste

Dans le contexte de la deuxième approche, nous nous basons sur l'hypothèse que l'oral et
l'écrit ont en commun une même grammaire qui est indépendante des phénomènes de
périphérie de l'oral—sur la notion de périphérie, voir (Chanod 1993). Ces derniers
phénomènes sont considérés comme des déformations du langage qui sont dues au caractère
spontané de l'oral. De ce fait, l'utilisation d'outils conçus pour le traitement de l'écrit est
envisageable. Selon ce point de vue, nous percevons les transcriptions de dialogues comme
étant des textes où se mêlent les constituants bien formés et moins bien formés.
En partant de ces faits, nous proposons un système basé sur une stratégie hybride. L'idée
de base consiste à élaborer un système à deux composants autonomes. Le premier
composant, le noyau, effectue une analyse des constituants bien formés selon un procédé
classique d'analyse de textes écrits. Tandis que le deuxième composant, le module de
recouvrement, se charge de traiter les constituants moins bien formés (appelés distorsions
dans la suite de l'article). Ce composant garantit la robustesse de l'analyseur syntaxique. Les
deux composants interagissent au niveau de l'analyse grâce à un module de supervision,
appelé aussi superviseur, qui gère les interventions de chacun des deux composants. La
gestion des interventions s'effectue grâce à un système qui détecte des marques de surface
identifiant une construction particulière ou une distorsion.

          
Noyau de l'analyseur syntaxique robuste

Le noyau de l'analyseur joue le rôle d'un analyseur syntaxique de l'écrit. Il a été conçu de
manière à satisfaire deux conditions : traiter les constituants bien formés mais, surtout,
faciliter l'intervention du module de recouvrement des distorsions. Dans cette perspective,
nous avons utilisé des règles de la grammaire française (Figure 1) que nous avons extraites
de Dubois (1969). La description des règles s'est effectuée selon le schéma X-barre
(Chomsky 1981) : voir la Figure 1 à la page suivante. La stratégie d'analyse que nous avons
implantée est une stratégie déterministe ascendante s'inspirant de la méthode LR(k) et de
certains éléments de l'analyse par îlots (Satta et Stock 1991 ; Woods 1982). Le lexique utilisé
est un lexique général (BDLEX1) du français standard écrit. L'utilisation de la Théorie du
Gouvernement et du Liage et le choix d'une analyse s'inspirant de la méthode LR(k) visent
principalement à faciliter l'intervention du module de recouvrement des distorsions dans le
processus d'analyse, comme nous l'expliquons dans les sous-sections 3.1 à 3.3.
 Le dictionnaire que nous utilisons pour notre analyseur syntaxique du français est BDLEX, obtenu grâce à
l'aimable collaboration des gens du CLIPS de l'IMAG à Grenoble.

            
Extrait des règles de grammaire utilisées par le noyau

            
Théorie du Gouvernement et du Liage

La théorie du Gouvernement et du Liage (Chomsky 1981) a soulevé de nombreuses
critiques concernant le caractère inné de la grammaire universelle—par exemple, voir
(Abeillé 1993). Néanmoins, nous avons retenu deux aspects avantageux par rapport à notre
démarche : le cadre de sous-catégorisation des têtes de syntagmes et la concept de
transformation. Dans notre description du noyau, nous utilisons une grammaire qui reconnaît
les structures profondes (projections des cadres de sous-catégorisation des catégories
majeures). Nous avons choisi de modéliser l'appareil transformationnel en tant que module
autonome intervenant de la même façon que le module de recouvrement, c'est-à-dire grâce à
la détection de marques de surface identifiant les différentes constructions. Cette approche
vient renforcer notre vision de noyau indépendant de tous les phénomènes de périphérie. Le
deuxième avantage concerne l'encodage de la structure hiérarchique de la phrase. La
régularité du schéma X-barre permet le contrôle de la progression de l'analyse et facilite la
détection des regroupements (de constituants) non conformes à ce schéma.

            
Méthode d'analyse

L'approche déterministe revêt un intérêt particulier dans notre stratégie d'analyse. En
effet, dans le contexte du traitement de l'oral, le problème de l'ambiguïté de l'analyse prend
encore plus d'ampleur que pour l'écrit. À l'ambiguïté syntaxique qui caractérise les langues
naturelles, vient s'ajouter un deuxième problème, soit : le traitement des distorsions de l'oral.
Lorsque l'analyse échoue à une étape donnée, il est difficile d'identifier la cause de cet
échec. Celui-ci peut être causé par un "mauvais" choix de règle de grammaire ou encore par
une distorsion. Ainsi, afin d'isoler le traitement des distorsions, nous cherchons à minimiser
l'ambiguïté syntaxique.

Plusieurs analyseurs syntaxiques ont utilisé une approche déterministe de type LR(k) pour
le traitement de l'écrit comme, par exemple, GLR (Tomita 1987) et PARSIFAL (Marcus
1980). Nous avons opté pour une analyse de droite à gauche, donc RL(k car elle tend à
diminuer l'ambiguïté syntaxique, comme nous l'indique notre expérimentation sur nos
corpora, et elle facilite le recouvrement de certaines distorsions tels que les répétitions.

            
Marques associées aux transformations et aux procédures de recouvrement

La démarche utilisée pour l'identification des transformations et des procédures de
recouvrement s'inspire de la méthode proposée pour l'identification des actes de dialogue, à
savoir, déterminer des indices syntaxiques rendant compte des actes de dialogues (Colineau
1997). Ces indices ou marques sont des unités lexicales ou certaines formations syntaxiques.
C'est la combinaison de ces marques qui permet l'identification du type d'acte de dialogue.
Parallèlement à cette démarche, nous avons identifié un ensemble de marques syntaxiques
(Figure 2) capables de rendre compte des différentes constructions de l'écrit ainsi que de
certaines distorsions de l'oral. Les marques que nous avons relevées pour les constructions de
l'écrit permettent une identification exacte de chaque construction (et donc de chaque
transformation à appliquer). En ce qui concerne les heuristiques et procédures de
recouvrement, nous avons pu aussi identifier des marques caractéristiques. Actuellement, les
marques que nous avons relevées nous ont permis de reconnaître de manière univoque les
heuristiques. Néanmoins, d'autres expérimentations sont nécessaires pour affirmer la
généralité de notre démarche. Voici un exemple de marques identifiant des transformations et
procédures de recouvrement :

              
Exemples de marques de surfaces associées aux transformations et aux procédures de recouvrements.

          
Couche périphérique de l'analyseur syntaxique robuste

La couche périphérique se charge du recouvrement des distorsions. Elle est composée d'un
ensemble d'heuristiques et de mécanismes de recouvrement qui interviennent à chaque fois
qu'un ensemble de marques identifiant une distorsion est détecté par le superviseur. Afin de
définir les procédures de recouvrement, nous avons relevé les différentes distorsions
présentes dans nos corpora et elles représentent en moyenne 65% des énoncés.

Quatres types de distorsions ont été identifiés : les bruits, les effacements, les interruptions, et les
répétitions. Dans ce qui suit nous présentons les procédures de recouvrement du bruit et des
répétitions.

            
Heuristique pour le recouvrement du bruit

En général le bruit est associé aux éléments qui ne présentent aucun intérêt pour l'analyse
syntaxique et qui ne peuvent pas être insérés dans l'arbre d'analyse, comme dans l'énoncé tu
mets à gauche du du rond, où le déterminant du ne peut être inséré dans l'arbre d'analyse. La
condition d'insertion d'une unité lexicale permet de détecter la présence de bruit. Elle est
définie comme suit : un constituant respecte la condition d'insertion s'il répond à l'une des
trois conditions suivantes : 1) il peut être inséré dans le syntagme en cours de construction ;
2) il peut représenter le début d'un nouveau syntagme ; 3) il représente à lui seul un
syntagme. Ainsi, l'heuristique du recouvrement du bruit consiste à ignorer les unités
lexicales ne respectant pas la condition d'insertion.

            
Heuristique du recouvrement des répétitions

Les répétitions de syntagmes sont fréquentes dans nos corpora, comme par exemple
l'énoncé maint'nant tu prends un gros rond un gros cercle. L'heuristique que nous
proposons pour le traitement des répétitions part de l'hypothèse que la dernière information
répétée est celle retenue par l'allocutaire et celle sur laquelle se bâtit le reste du dialogue.
Ainsi, l'heuristique que nous retenons est la suivante : les constituants de la répétition
retenus pour l'analyse sont ceux répétés en dernier lieu. Cependant, notons que certains cas
de répétitions nécessitent un traitement particulier, comme par exemple dans l'énoncé
main'tnant tu prends le carré rouge euh le petit où il y a répétition de syntagmes nominaux.
L'application de l'heuristique du recouvrement des répétitions engendre une perte
d'information puisque le syntagme retenu dans l'analyse est le petit, qui est une construction
elliptique. En effet, en éliminant le premier syntagme le carré rouge, qui représente le
référent de le petit, nous risquons de ne pas pouvoir calculer l'ellipse. Dans la section
suivante, nous montrons qu'il est possible de traiter ce problème grâce à la gestion de
l'ordonnancement de marques.

          
Gestion de l'ordonnancement entre transformations et mécanismes de recouvrement

Le rôle du superviseur est particulièrement important, dans la mesure où nos corpora
représentent des textes où se mêlent les constituants bien formés et les distorsions. De ce fait,
il nous apparaît impossible d'établir un ordre figé quant à l'intervention du noyau et de la
périphérie dans le processus d'analyse. Afin de mieux expliquer ce point, reprenons
l'exemple je prends le carré rouge euh le petit, nous pouvons remarquer la présence d'une
ellipse du nom et une répétition du syntagme nominal (rappelons que l'analyse se fait de la
droite vers la gauche). Deux possibilités sont à envisager. La première consiste à effectuer
d'abord le recouvrement de la répétition ensuite le calcul de l'ellipse, ce qui revient à
analyser l'énoncé je prends le petit. Nous remarquons dans ce cas que l'objet de l'ellipse a
été retiré de l'énoncé après l'application du recouvrement de la répétition, ce qui implique
une perte d'information. La deuxième possibilité consiste à calculer d'abord l'ellipse puis à
appliquer la procédure de recouvrement de la répétition de syntagme. Le résultat de cet
enchaînement revient à analyser l'énoncé je prends le petit carré rouge, ce qui permet de
corriger l'énoncé sans pour autant perdre de l'information. Nous pouvons constater que
l'enchaînement des procédures joue un rôle important dans l'analyse syntaxique.

En respectant l'ordre d'occurrence des marques associées aux traitements de l'écrit et ceux de
l'oral il est possible d'effectuer une analyse sans perte d'informations.
Ainsi, le superviseur a pour rôle de gérer l'ordre d'intervention du noyau et de la
périphérie dans le processus d'analyse. Il mémorise toutes les marques rencontrées afin de
déclencher la transformation ou la procédure de recouvrement appropriée. Lorsqu'une
transformation et une procédure de recouvrement peuvent s'exécuter simultanément, c'est
l'ordre d'occurrence de leurs marques qui fixe l'ordre de leur exécution. Dans la Figure 3,
nous présentons l'algorithme détaillé du superviseur.

          
 Premiers résultats expérimentaux

L'implémentation actuelle de notre analyseur syntaxique robuste ne couvre pas tous les
phénomènes linguistiques, ni de l'écrit, ni de l'oral. La première phase de ces travaux visait
d'ailleurs à démontrer la valeur potentielle de l'approche ; nous estimons avoir atteint cet
objectif. Les transformations qui ont été implantées et testées sont celles des pronoms
clitiques, des interrogatives, des négatives et des relatives. Une partie seulement du
recouvrement de
:
implémentation complète pour les bruits et les répétitions ; implémentation partielle pour les
effacements ; et pas encore d'implémentation pour les interruptions.
les distorsions possibles est actuellement en fonction
toutes
Nous avons récemment évalué cette implémentation sur deux corpora, d'une centaine
d'énoncés chacun. Les résultats présentés dans le Tableau 1 (ci-dessous) indiquent les
pourcentages de succès pour chaque type de distorsion et cela de manière intrinsèque. Les
résultats pour "zéro distorsions" (énoncés corrects) sont plus faibles qu'attendus. Après examen plus détaillé de leurs causes, nous avons constaté qu'ils étaient dus principalement des
lacunes au niveau des phénomènes linguistiques couverts par le noyau de l'analyseur.
Cependant, nous considérons satisfaisants les résultats obtenus pour le recouvrement des
distorsions. En effet, les points importants qui ont été vérifiés sont le recouvrement des
différents phénomènes de l'oral avec succès et sans perte d'information, et le respect de
l'ordre d'intervention du noyau et de la périphérie dans le processus d'analyse.
En particulier, l'heuristique pour le recouvrement du bruit a prouvé son efficacité quant à
l'élimination du bruit dans les énoncés puisque nous enregistrons une moyenne de 87,5% de
corrections réussies. Pour le recouvrement des répétitions nous avons enregistré 86% de
corrections réussies et cela sans perte d'information. Jusqu'à maintenant, il est clair que nous
avons d'avantage concentré nos efforts sur le traitement des distorsions. C'est pourquoi nous
reconsidérons la couverture grammaticale du noyau dans la suite immédiate de nos travaux.

            
Taux de succès du traitement des énoncés comportant une occurrence de chacun des phénomènes considérés par l'analyseur syntaxique robuste. ‘n/a' signifie qu'il n'y avait pas d'énoncé de ce type.

              
            
Algorithme de gestion de l'ordonnancement des transformations et des procédures de recouvrements.

          
 Brève comparaison avec d'autres travaux

Nous signalons au lecteur deux recherches en rapport direct avec nos propres travaux.
Premièrement, Langer (1990) qui s'intéresse spécifiquement à l'analyse syntaxique
d'énoncés oraux dits "mal-formés" et utilise un concept de normalisation qu'il définit comme
étant la relation entre un énoncé mal-formé et sa version bien formée.
Pour ce faire, il augmente un analyseur de l'écrit d'un certain nombre d'heuristiques qui permettent de traiter
quelques phénomènes comme les déviations morpho-syntaxiques, certaines réparations et
certaines répétitions. Et deuxièmement, Nakano et al. (1994) qui augmentent un analyseur
d'abord conçu pour l'écrit d'un ensemble de règles qui permettent d'élargir la couverture des
phénomènes syntaxiques en ajoutant ceux qui sont spécifiques à la langue orale Japonaise.
Par rapport à ces travaux, notre approche présente le net avantage d'établir une distinction
claire, à la fois du point de vue théorique et du point de vue de la réalisation informatique,
entre ce que nous désignons comme étant le noyau et la périphérie de la grammaire. De plus,
le contrôle de la coordination de ces deux couches est effectué par un module dédié à cette
tâche, le superviseur : il s'agit donc d'une approche qui semble plus modulaire et flexible,
que celles citées ci-dessus.

          
Conclusion et travaux futurs

Nous avons présenté les principales caractéristiques de notre approche d'analyse syntaxique
robuste d'énoncés transcrits de l'oral. Les caractéristiques originales en sont l'exploitation
judicieuse des marques de surface ; l'adoption d'une perspective selon laquelle la langue
orale est une variation de la langue écrite; et la réalisation d'un analyseur syntaxique robuste
à partir des concepts de noyau et de périphérie, et d'un superviseur qui contrôle ces deux
couches. Bien que nous n'ayons pas encore terminé notre évaluation, les résultats obtenus
actuellement nous
la pertinence d'une conception modulaire et
complémentaire pour le traitement de l'oral.
laissent croire à
Nos travaux actuels s'attachent principalement à augmenter la couverture de la grammaire
noyau et à augmenter le nombre de distorsions traitées.

          
Remerciements

Nous remercions le FCAR (Fonds pour la formation de Chercheurs et l'Aide à la Recherche)
ainsi que le CRSNG (Conseil de Recherche en Sciences Naturelles et Génie du Canada) pour
l'aide financière apportée à nos travaux de recherche.




        
          
 Introduction

La technique text-to-speech pour la synthèse de la parole consiste à générer un énoncé
oral à partir d'un texte. Les systèmes utilisant cette technique obtiennent souvent, une
prosodie de type lecture. L'approche, qui consisterait simplement à placer un tel système text-to-speech en sortie d'un module de dialogue, serait doublement préjudiciable dans une
situation de dialogue homme-machine car (a) les interventions des interlocuteurs ne sont pas
des textes et (b) leurs répliques ne sont pas des lectures. En effet on ne dialogue pas comme
on lit, on ne dialogue pas non plus comme on écrit : les formes linguistiques sont différentes
(au niveau syntaxique et au niveau prosodique, mais aussi par un emploi plus fréquent
d'ellipses, de formes emphatiques, etc. et surtout ces formes véhiculent une composante
perlocutoire évidente. Plus que la recherche du naturel qui n'est peut être pas une fin en soi, il
faut viser l'intelligibilité et l'adéquation ou la justesse d'une réponse qui procède de sa
pertinence ; on sait également qu'une réponse peu pertinente n'est pas compréhensible. Au
niveau du dialogue homme-machine il y a donc nécessité de contrôler cette pertinence pour
deux raisons : (a) pour diminuer le coût cognitif de l'utilisateur (avec des retombées évidentes
sur la performance d'exécution de la tâche, la diminution de la fatigue, etc.) et, (b) pour
respecter le principe ergonomique d'observabilité qui stipule que l'utilisateur doit avoir une
conscience claire des véritables capacités de la machine.

La solution que nous proposons passe par deux étapes : la première, qui précède la
synthèse proprement dite est d'adapter un système de génération effectivement au dialogue et
non plus simplement à la réalisation linguistique d'un contenu, et la seconde est de faire
intervenir les niveaux pragmatiques dans la génération de la prosodie. Cette approche qui fait
donc appel à la pragmatique met l'accent sur l'aspect primordial de l'intersubjectivité que
négligent la plupart des systèmes pour l'instant. - les éléments pragmatiques que sont
seulement capables de gérer les interfaces homme-machine se situant ou bien à un niveau de
planification, Pollack M. (1990 par exemple, ou bien au niveau des problèmes de résolution
des références.
Notre travail s'inscrit dans une perspective de dialogue oral pour la génération d'énoncés
dans laquelle nous tentons de concilier la génération de texte classique proprement dite
(sémantique, syntaxe) et la pragmatique illocutoire, basé sur les théories d'Austin (Austin,
1962). Nos travaux reposent en partie sur le noyau de génération développé par l'équipe
Cristal-Gresec de l'université Stendhal de Grenoble, (Balicco L. 1993 et Pont-on C. 1996 sur
lequel nous greffons un module de traitement pragmatique illocutoire (désormais appelé
module pragmatique dans la suite du texte).

          
La pragmatique illocutoire dans le contexte de la génération

Austin (Cf. Austin, 1962) fut l'un des premiers à remettre en cause la conviction selon
laquelle le langage à pour fonction de décrire un état de faits et doit donc conséquemment
produire des affirmations susceptibles d'être vraies ou fausses (Cf. Searle, 1969). Austin
distingue dans l'action qui consiste à produire un énoncé, 3 types d'actes de langage : l'acte
locutoire, l'acte illocutoire et l'acte perlocutoire, chacun de ces actes représentant
respectivement le contenu de l'acte, ses visées et moyens, et ses effets. Nous retenons
d'Austin et de Vanderveken (Vanderveken, 1988) qui a formalisé la logique illocutoire, que
chaque énoncé est, à la base, formé de trois constituants principaux : un contenu
propositionnel, un but (que le locuteur poursuit) et le degré de force qu'il utilise pour
réaliser ce but. Le contenu propositionnel est du niveau du locutoire, le but et la force du
niveau illocutoire. Malgré ses qualités, cette approche est essentiellement monologique, c'est-à-dire qu'elle ne prend pas en compte le locuteur dans la dynamique du dialogue. Nous
devons donc l'étendre dans un cadre plus général prenant en compte les autres éléments du
dialogue qui permettent d'aborder le problème de l'intersubjectivité.
La figure 1 présente le cadre que nous avons choisi de donner au dialogue et la relation qui
existe entre chacun de ses éléments avec les théories d'Austin.

            
Cadre pour le dialogue et liaison avec les théories d' Austin

Ce cadre représente les éléments qui entrent en jeu à chaque tour de parole.
C'est à dire aussi bien les relations qui interviennent entre le locuteur et l'allocutaire - et c'est le point sur
lequel nous mettons l'accent – que leur connaissance respective du langage et que les rapports
qu'ils peuvent avoir au monde. Deux pôles se dégagent à partir de ce cadre et dans la
perspective du dialogue :
- d'un côté, les aspects purement langagiers et référentiels - représenté par le langage et le
monde - que Searle (Cf. Searle, 1969) a mis en évidence par la notion de direction
d'ajustement entre les mots et le monde et qui sont déjà pris en compte au niveau des
générateurs (au moins pour l'aspect référentiel
- de l'autre côté, les aspects intersubjectifs - représentés par le locuteur et l'allocutaire -
qu'il est indispensable de faire intervenir dans une situation de dialogue au cours de laquelle,
ils coordonnent leurs actions (Vernant, 1992) et règlent leur jeu inter-social.
Selon l'hypothèse de Searle « tout ce que l'on veut dire peut être dit » (principe
d'exprimabilité) nous postulons que chacun des éléments du cadre ainsi que les relations
qu'ils entretiennent peuvent se projeter dans la langue au niveau de l'énoncé. Inversement, les
énoncés véhiculent ces éléments que l'allocutaire saura reconnaître et interpréter. Cette
approche résolument linguistique s'inscrit dans la perspective du marquage illocutoire et
interactif des énoncés des travaux de Roulet E. (1980 Rubattel C. (1980) et De Spengler N.
(1980).

          
Réalisation

La visée globale de l'utilisation de la pragmatique illocutoire en dialogue homme-machine
est d'améliorer la pertinence des répliques de la machine, en agissant sur la forme et sur le
contenu des énoncés générés. Plus prosaïquement, notre réalisation s'attache à mettre en
valeur les structures linguistiques nécessaires à la prise en compte d'une formalisation des
actes de langage, dans un cadre restreint au dialogue. L'architecture générale de notre système
est donnée sur la figure 2.

            
architecture générale du système

Formellement nous appelons F la forme et P le contenu propositionnel d'un énoncé. Il
s'agit pour la machine (jouant le rôle du locuteur L de produire l'énoncé F(P) dans le
contexte du dialogue, c'est-à-dire sous la contrainte de réalisation du but et à l'intention de
l'allocutaire A. La forme (ou force illocutoire chez Vanderveken) est définie par le but
illocutoire b et le degré de force f. On pose F <b, f>. Nous supposons que F et P sont
indépendants ce qui permet de réaliser le système de génération à l'aide de modules
indépendants. Ainsi on peut envisager quatre étapes dans le processus :

Une fois ces informations obtenues, on procède à la mise en forme du message de sortie,
c'est-à-dire à la production proprement dite de l'énoncé en ajustant sa forme finale à la force
illocutoire.
Nous appelons cadre illocutoire la structure F(P, A, L, Rel) où F <b, f>.

Nous avons défini une syntaxe sur ce cadre de façon à ce que le module pragmatique
puisse en traiter l'ensemble des éléments.
Il prend en entrée les éléments du cadre et fournit
en sortie tous les éléments nécessaires au noyau de génération pour générer l'énoncé
proprement dit en langage naturel.

          
Représentation et prise en compte des éléments du cadre dans le système

            
Représentation

Nous nous fondons sur les représentations utilisées par Pont-on C. (1996) dans son noyau
de génération. Il s'agit des graphes conceptuels (Cf. Sowa J. 1984) dont le modèle trouve ses
fondements en linguistique et en psychologie. C'est pourquoi il est particulièrement bien
adapté au traitement du langage naturel. Nous ne discuterons pas ce point dans cet article.
Il existe par ailleurs au sein du noyau de génération un module de traduction de formalisme
qui permet de traiter les CMR - Common Meaning Representation - et les descriptions
fonctionnelles. Au niveau du noyau de génération, les graphes conceptuels sont représentés en
Prolog sous la forme d'un prédicat à deux arguments : une liste de variables de description et
la définition même du graphe. Nous précisons ce point qui a son importance dans le
fonctionnement du module pragmatique. En effet, les descriptions associées à chaque graphe
conceptuel regroupent l'ensemble des caractéristiques de la proposition associée au graphe
conceptuel. Elles permettent par exemple de préciser la forme (assertive, interrogative ou
impérative) de la proposition et le temps verbal à employer. C'est aussi cette approche des
graphes conceptuels qui a influé sur le choix du noyau de génération comme base de
développement. En effet ces descriptions sont une première approche des caractéristiques
linguistiques globales et essentielles de l'énoncé qui permettent une maîtrise des effets. Les
descriptions vont donc être un des points de liaison entre le générateur proprement dit et le
module pragmatique.

            
Le locuteur et l'allocutaire

La notion d'intersubjectivité dont Benveniste E. a montré que la condition était le langage,
est prise en compte dans les rôles respectifs que jouent L et A, ainsi que dans leur relation
Rel. Ceci implique et justifie qu'il soit possible de marquer linguistiquement ces rôles et cette
relation. C'est en parlant, qu'un individu se pose comme un « je » devant un « tu ». « Nous
parlons à d'autres qui parlent, telle est la réalité humaine » commente Benveniste E. (in
Benveniste, 1966). Qui plus est, du point de vue de l'action, Vernant D. considère que cette
dernière n'existe à travers l'acte de langage, que dans l'interprétation qui en est faite par celui
qui fait l'action.
Dans cette ligne de pensée, nous avons donc recensé l'ensemble des rôles que peuvent
tenir L et A pour les prendre en compte au niveau de la génération. Selon les rôles respectifs
de L et A (apparaissant dans la relation Re différentes formes linguistiques peuvent
marquer la subjectivité. Au niveau interne le module pragmatique est capable de traiter aussi
bien les formes personnelles directes, représentatives des rôles (c'est à dire « je » et « tu »)
que les formes conceptuelles plus traditionnelles, même si la subjectivité n'est réellement
marquée qu'à travers les formes personnelles - nonobstant les remarques de Moeschler J. et
Reboul A. (1994) quant à la subjectivité de la troisième personne.

            
La forme

Au niveau de la force, nous utilisons une échelle numérique allant de 0 à 5, ainsi que des indicateurs de dynamicité.
Ces indicateurs permettent de préciser si la force est croissante,
décroissante ou stable, d'une intervention à la suivante. La prise en compte de ces
phénomènes nécessite la gestion de l'historique du dialogue. Etant donné que les réalisations
linguistiques de la force sont fortement dépendantes des relations qui unissent le locuteur et
l'allocutaire, cela impose aussi de disposer d'une représentation de l'utilisateur dans la
machine (mais pour l'instant, nous n'avons pas encore de modèle de l'utilisateur sophistiqué
dans notre système).
b) le but illocutoire
Un but se définit visvis de la tâche sur laquelle porte le dialogue. On appelle but un état
du monde que désire atteindre le locuteur, aussi bien qu'un état mental auquel il souhaite
arriver (par exemple connaître une information, acquérir un savoir-faire). Visvis d'un acte
particulier, accompli dans le but de réaliser une tâche, on appelle but illocutoire la visée
actionnelle ou communicationnelle du locuteur à l'adresse de l'allocutaire.
Le passage du but au but illocutoire permet de s'affranchir des contraintes qui existent au
niveau de chaque application puisque des buts particuliers nous passons aux classes
génériques bien définies des buts illocutoires. A charge pour l'instant au moteur de dialogue,
d'opérer la transition entre les buts - par exemple, de réservation, de renseignement, etc. dans
une application de réservation hôtelière - aux buts illocutoires qui permettent l'aboutissement
à de tels buts.
Dans ces conditions, en réduisant notre propos au dialogue homme-machine opératif, les
différents buts illocutoires que nous avons retenus se répartissent en trois grandes classes :
- le faire-faire, qui correspond à une demande de réalisation d'action par l'allocutaire,
- le faire-savoir et le faire-croire, qui correspondent à une réponse à l'allocutaire ou à une
assertion de la part du locuteur – c'est un apport d'informations certaines ou incertaines (ou
conjecturales
- le faire-faire-savoir et le faire-faire-croire, qui correspond à une demande d'information
(mais aussi de confirmation).
Ainsi, par exemple, à partir de P fermer ! objet ! porte on obtient les énoncés
suivants :

          
Module de traitement pragmatique illocutoire

Les traitements qu'effectue ce module permettent d'obtenir, à partir des éléments du cadre
du dialogue, une représentation conceptuelle interprétable par le noyau de génération. Il s'agit
donc de traduire tous les éléments du cadre en termes de représentations conceptuelles puis de
produire une sortie pour le noyau de génération. Pour cela le module pragmatique travaille à
l'aide de quatre opérateurs de base. La fonction de ces opérateurs de base est de transformer
la représentation conceptuelle de départ qui fait partie des éléments du cadre en une représentation plus complexe dans laquelle interviennent les autres éléments du cadre, c'est à
dire à la fois la prise en compte du but, de la force et la représentation du locuteur et de
l'allocutaire.
 On pourra trouver dans la littérature maintes autres échelles de force, pour ne citer par exemple que
(Boissier O. 1993) qui utilise les valeurs {null, informing, bargaining, persuading, commanding}. Le manque de
consensus dans la prise en compte de la force prouve la difficulté à définir une échelle stable.

Ces opérateurs de base sont :
- l'adjonction : l'information pragmatique se rajoute au contenu propositionnel proprement
dit sous la forme d'une proposition complémentaire : « je te dis que "la station est sur la
table" », « je veux que tu "fermes la porte" », etc. C'est essentiellement la prise en compte du
but qui fait intervenir ce type de phénomènes ;
- la substitution : l'information pragmatique passe par le remplacement d'un composant
par un autre ; par exemple la pronominalisation (ou son inverse) : de « je suis bien
malheureux...» à « t-on pauvre père est bien malheureux...» ;
- la suppression : l'information pragmatique utilise les possibilités de raccourci du langage
(notion d'ellipse notamment) pour atteindre les effets voulus : « Le sel ! » devant être compris
comme « Passe-moi le sel ! », puis « le sel, sur la table ! » signifiant « Remets le sel sur la
table ! », etc. ;
- la modification : l'information pragmatique peut obliger à des transformations plus
radicales de l'énoncé avec par exemple les phénomènes d'indirection tel que « Avez-vous du
sel ? ».
Le fonctionnement général du module pragmatique est le suivant :
a) Le module commence par interpréter le but illocutoire b pour compléter le graphe
conceptuel qui représente P à exprimer en lui adjoignant un certain nombre de caractéristiques
qui sont du niveau des descriptions. Ainsi en règle générale la différence entre un faire-savoir
et un faire-faire-savoir pour le même P va se faire sur la forme linguistique de la phrase :
interrogative ou non.
b) L'étape suivante consiste à prendre en compte le degré de force pour calculer les
contraintes, utiliser les opérateurs de base possible et exprimer le degré de force au niveau des
concepts.
c) En fonction de L, A et Re des branches supplémentaires peuvent être greffées au
graphe conceptuel initial P pour pouvoir exprimer linguistiquement la subjectivité dans
l'énoncé. Ensuite, et selon leur relation, L et A sont pris en considération.
Le module s'appuie sur les capacités linguistiques du noyau de génération pour pouvoir
exprimer l'ensemble des éléments en présence, en terme de forme de surface. Ceci est
possible notamment par le biais des outils que partagent le module et le noyau, outils qui sont
soit génériques soit spécifiques à une application, notamment au niveau du lexique.
L'avantage du retour-arrière (backtracking de Prolog) est de permettre la résolution des
incompatibilités qui peuvent apparaître entre des opérations de niveaux différents (par
exemple, un choix effectué au niveau pragmatique peut entraîner l'échec de la génération
mais uniquement au niveau de la réalisation linguistique) par une «remontée» dans l'arbre des
solutions.
Pour clarifier l'exposé, nous donnons l'exemple d'un cadre de dialogue et d'une réalisation
linguistique obtenue après traitement.

            
Exemple d'un cadre de dialogue

          
 Conclusion

Notre système, grâce à son module de traitement pragmatique, a la capacité de générer des
énoncés plus naturels dans le dialogue. La génération est basée non plus sur le contenu
informatif seul, mais aussi sur une situation pragmatique, exprimée dans un cadre bien défini.
Notre approche montre l'intérêt de prendre en compte les paramètres illocutoires dans le
cadre du dialogue. Outre les phénomènes que nous avons répertoriés et que notre système est
capable de traiter, il existe une grande quantité de processus rhétoriques au sein du langage,
qui restent encore à introduire. Nous sommes parfaitement conscients des limites de notre
système face à la richesse et à la complexité du langage naturel (cependant tous les
phénomènes n'ont certainement pas besoin d'être implémentés en dialogue homme-machine).
D'autres phénomènes sont quant à eux, irrémédiablement trop complexes pour être traités
dans le cadre limité que nous nous sommes fixés.
L'indépendance du module pragmatique permet de mettre en relief les phénomènes
linguistiques dont la pragmatique fait usage, mais aussi d'avoir un outil facile à utiliser et à
faire évoluer. Le fait d'obtenir un module de traitement pragmatique indépendant qui
préexiste à la génération démontre que la pragmatique (au moins dans sa partie illocutoire)
peut être un élément quasi indépendant qui améliore la qualité des réponses de la machine
face aux demandes de l'utilisateur mais aussi au regard des exigences du concepteur de
l'application. Le contenu, dans cette perspective n'est plus désormais le point central mais
devient un élément d'un schéma plus global et plus riche.




        
          
Introduction

Cet article présente la mise en oeuvre d'un dispositif expérimental de Traitement Automatique du
Langage Naturel qui porte le nom de GASPAR (Fleury 1997). Ce dispositif vise à établir une
représentation et un classement évolutifs d'unités lexicales représentées sous la forme d'objets
informatiques appelés des prototypes. GASPAR a pour but de construire des représentations
évolutives pour les mots à partir d'informations extraites sur corpus. Il doit conduire également à la
construction de classes de mots de manière inductive. Les classes de mots produites peuvent ensuite
être utilisées dans des applications liées à la construction de bases de connaissances sur un domaine
de spécialité. Notre travail s'inscrit dans une reprise de l'approche harrisienne (Harris 1970, 88) et
vise à automatiser les traitements de représentation des mots et de leur classement et à souligner les
limites de cette induction de savoirs. Nous exposons d'abord les problèmes posés par la
représentation d'unités de langue dans un dispositif de TALN. Nous examinons ensuite une chaîne
de traitements qui permet l'acquisition d'informations à partir de corpus. Puis nous présentons le
cadre de représentation choisi pour construire des représentations informatiques des mots et des
comportements associés. Nous décrivons également les processus mis en place pour une
représentation et un classement automatiques des mots et enfin nous présentons les premiers
résultats construits avec GASPAR.

          
Représentation et classement automatiques de mots

            
Un problème : représenter et classer les mots

Notre travail vise à prendre appui sur des connaissances très générales qui sont affinées voire
remodelées et changées au gré des observations rencontrées. La première tâche de GASPAR
consiste à construire des représentations informatiques des mots et de leurs comportements. Il s'agit
de repérer des descriptions des mots et de leurs comportements et de les représenter. Dans la mesure
où les descriptions linguistiques peuvent évoluer, notre approche vise à ne pas préjuger des
informations que l'on peut associer aux mots. Il ne s'agit donc pas d'encoder à la main des
informations prédéterminées. La seconde tâche consiste à reclasser les informations représentées en
prenant appui sur le fait que la représentation de savoirs évolutifs implique un classement évolutif
de ces savoirs. Les processus de classement visent aussi à organiser le matériel lexical dans un
domaine de spécialité afin de déterminer les classes sémantiques sous-jacentes aux classes de mots
construites (Habert & al. 1997).

            
Un corpus

Notre approche de représentation vise à repérer des informations à partir de réalisations rencontrées
sur corpus (Habert & al. 1997). Le corpus utilisé est celui qui est constitué dans le cadre du projet
MENELAS (Zweigenbaum 1994) pour la compréhension de textes médicaux. Ce corpus est utilisé
par le Groupe de Travail Terminologique et Intelligence Artificielle (PRC-GDR Intelligence
Artificielle, CNRS). L'unité thématique de ce corpus a trait aux maladies coronariennes. La phase
d'acquisition de savoirs à partir de corpus prend appui sur la systématicité structurelle et sémantique
propre aux sous-langages (Harris 1970, 88) afin de mettre au jour les proximités de co-occurrences
entre mots pour dégager les relations sémantiques sous-jacentes. Les informations utilisées par les
processus de représentation informatique des mots et des arbres associés sont issues d'une chaîne de
traitements composée des logiciels LEXTER, AlethIP et ZELLIG. Le but de ces outils est d'extraire
des informations à partir de corpus (LEXTER, AlethIP) et de simplifier ces informations puis de
caractériser leurs fonctionnements (ZELLIG). Les informations extraites sont des arbres d'analyse,
ces arbres étant ensuite simplifiés dans le but de déterminer les arbres élémentaires de dépendance
qu'il est possible d'associer aux mots : sont considérés comme élémentaires les arbres mettant en
évidence une relation binaire entre deux mots pleins, nom ou adjectif, dans des schémas comme, par
exemple, N Prep N ou N Adj. Sur la séquence "alteration severe de la fonction ventriculaire
gauche", ZELLIG met en évidence les dépendances élémentaires : (a) alteration severe,
(b) alteration de fonction, (c) fonction gauche, (d) fonction
ventriculaire.

            
Un outil

L'outil de représentation choisi est la programmation à prototypes (Blascheck 1994) (désormais
notée PàP) dont les principaux domaines d'application se situent dans le développement d'interfaces
utilisateur (Smith 1995). Notre travail utilise le prototype comme un outil de représentation de faits
linguistiques dans la mesure où nous pensons qu'il peut répondre à certains problèmes que posent
ces faits de langue. Cet outil conduit à construire des structures de représentation simples et
ajustables pour rendre compte justement des problèmes d'ajustements qui sont à l'oeuvre dans la
construction du sens dans le langage naturel. Avec la PàP, il ne s'agit pas de partir d'une somme
d'informations figées et connues par avance mais de construire progressivement les entités
informatiques suivant les informations dont on dispose. Si les informations à représenter ne sont pas
connues de manière définitive, il est possible de commencer le processus de représentation en
utilisant les informations déjà recensées puis d'affiner dynamiquement les objets construits dès que
de nouvelles informations sont disponibles. Cette mise à jour des objets peut être réalisée
manuellement ou automatiquement (en définissant les opérations idoines). On peut donc envisager
des processus de représentation qui se déroulent de manière continue suivant les flux d'informations
disponibles. Une fois que l'on a construit un objet particulier, on utilise deux opérations
fondamentales pour représenter d'autres éléments sous la forme de prototypes : le clonage et
l'ajustement. L'opération de clonage produit une copie conforme de l'objet cloné. On peut ensuite
d'ajuster le prototype issu de l'opération de clonage pour représenter adéquatement le nouvel
élément. L'ajustement de l'objet cloné n'altère en rien le prototype qui a servi de support pour
l'opération de clonage. On peut modifier les propriétés du nouvel objet sans modifier les propriétés
du prototype initial. On peut ensuite réitérer les opérations de clonage et d'ajustement pour
représenter les objets souhaités. Il est important de souligner que la notion de prototype mise en
avant par la PàP ne correspond en rien à la notion de prototype développée par la psychologie
cognitive. Self est le langage utilisé pour l'implémentation du système GASPAR. Self est un
langage à prototypes qui permet l'héritage multiple et dynamique. Self a été conçu par David Ungar
et Randall Smith à l'université de Stanford (Ungar, Smith 1987). Sa 1ère implémentation date elle
aussi de 1987. La dernière version, Self-4.0, est disponible depuis juillet 95 (Self Group 1995). Self
est désormais développé par Sun Microsystems. Un prototype est un objet composé d'attributs. Self
offre des primitives qui permettent d'ajouter ou de supprimer dynamiquement des attributs aux
objets. Les objets dialoguent entre eux via un mécanisme d'envoi de messages. L'héritage en Self se
réalise au travers de la notion de délégation qui permet de factoriser localement des comportements
partagés. Dans le langage Self, un parent commun à plusieurs prototypes est appelé un objet
traits.

          
Construction inductive des savoirs avec GASPAR

Un premier objectif est de construire des représentations informatiques évolutives de mots à partir
d'informations extraites sur corpus. Le travail de représentation mis en place ne construit pas une
représentation prédéterminée du sens attaché aux mots ou aux structures syntaxiques représentées, il
propose des amorces d'interprétation qui doivent être affinées par un travail d'interprétation plus fin.

            
Représentation dynamique des unités lexicales

GASPAR dispose, au départ, d'informations extraites à partir d'un corpus (sous la forme d'un fichier
texte). Pour chaque entrée lexicale, GASPAR dispose d'informations morphologiques et
sémantiques décrivant ces mots, d'une liste d'arbres élémentaires et d'une liste d'arbres d'analyse
associés aux arbres élémentaires. GASPAR s'appuie uniquement sur ces informations pour
construire des prototypes afin de représenter les mots et leurs comportements (les arbres associés).
Le processus de génération des mots se déroule de la manière suivante. Pour chaque unité lexicale,
GASPAR vérifie s'il existe une représentation prototypique adéquate pour la représenter. Si elle
existe, GASPAR conserve l'objet trouvé. Si elle n'existe pas, et s'il n'existe aucune représentation
prototypique de la catégorie dont elle relève, GASPAR commence par créer automatiquement une
représentation prototypique de cette nouvelle famille catégorielle, puis il construit une
représentation prototypique de ce nouveau représentant de cette famille (en tenant compte des
informations fournies pour décrire ce nouvel élément) (Figure 1a). Si le mot à représenter ne
possède pas de représentation prototypique et s'il existe déjà une représentation prototypique d'un
élément de la même famille catégorielle, GASPAR utilise les opérations de clonage et d'ajustement
pour représenter ce nouvel élément (Figure 1b).

              
Génération automatique des prototypes de mot " pontage "(a) et " effort "(b)

            
Représentation dynamique des contraintes syntaxiques

GASPAR procède de la même manière pour la représentation des arbres. Pour chaque entrée
lexicale lue, on dispose d'une liste d'arbres élémentaires à représenter. Avant de représenter ces
arbres élémentaires, GASPAR vérifie si ces arbres disposent déjà d'une représentation prototypique.
Si elle n'existe pas, il la créé automatiquement en tenant compte des informations fournies :
constituants et contraintes. Pour chaque arbre élémentaire construit, on peut avoir une liste d'arbres
d'analyse à représenter. Avant de représenter ces arbres, GASPAR vérifie là encore si ces arbres
disposent déjà d'une représentation prototypique. Si elle n'existe pas, il la créé automatiquement en
tenant compte des informations fournies : constituants et contraintes. GASPAR affecte ensuite les
prototypes d'arbres aux prototypes de mots auxquels ils sont associés. De même, il associe les
prototypes d'arbres d'analyse construits aux prototypes d'arbres élémentaires associés. Dans la
figure qui suit, le dispositif GASPAR construit les structures pour représenter les arbres associés à
pontage en tenant compte des informations données pour la description de ces arbres.
Dans la figure suivante, on présente, via l'interface graphique de Self, les prototypes construits pour
représenter le mot pontage et les deux prototypes d'arbre élémentaire associés : il s'agit des
prototypes représentant l'arbre NAdj et l'arbre N1PrepNPivot. Cette figure contient les masques
graphiques construits pour représenter les mots et les arbres manipulés par GASPAR.
Ces masques graphiques constituent des points d ́entrée pour la présentation des informations associées aux
prototypes construits.

              
Génération automatique des prototypes d'arbres associés au mot " pontage ".

              
Objets GASPAR pour le nom pontage.

          
Classement des prototypes

Le second objectif est de classer les mots représentés et de tendre vers la détermination de classes
sémantiques, de manière inductive. Tout d'abord, il faut souligner que le classement automatique
présenté ici s'appuie principalement sur des contraintes syntaxiques associés aux mots. Dans notre
travail, la syntaxe est utilisée pour dégrossir la représentation et le classement des mots mais ne
permet pas à elle seule de classer les mots représentés. A l'inverse des approches harrissiennes et
statistiques, notre approche ne conduit pas à la détermination de classes sémantiques satisfaisantes
mais elle constitue une méthode d'amorçage pour l'élaboration de l'ontologie du domaine, nous
suivons sur ce point la démarche suivie par (Habert, Nazarenko 1996) : la construction de
l'ontologie du domaine étudié nécessite un part d'interprétation. Notre approche de classement des
mots est conçu en fait pour aider à accéder aux sens (Habert & al. 1997).
Les processus de classement utilisent la notion d'héritage définie dans l'implémentation de Self pour
réaliser la mise en place d'un réseau de comportements partagés sur l'ensemble des prototypes
construits. Ces processus ajoutent de manière dynamique des liens de délégation entre les
prototypes de mots ou d'arbres et des pôles de comportements partagés construits automatiquement.
Le classement des mots ne s'intéresse qu'aux comportements linguistiques associés à ces mots : on
cherche à évaluer les partages possibles de tels comportements. Il s'agit en particulier de chercher
les prototypes d'arbres élémentaires communs à un ensemble de prototypes de mots. Si on considère
les noms stenose et lesion, ils partagent des comportements (les arbres NPivotPrepN2 et
NAdj). Si on considère maintenant le nom angioplastie, celui-ci entre dans des constructions
du type "indication de angioplastie" (l'arbre N1PrepNPivot). GASPAR construit donc un pôle de
comportements partagés qui va porter les arbres élémentaires communs. Il établit un lien de
délégation entre ce pôle et les prototypes concernés. Sur notre famille de mots comprenant les noms
stenose, lesion et angioplastie, on obtient le mini-réseau suivant :

            
Un mini-réseau de comportements partagés

Les processus construits permettent en fait d'évaluer plusieurs types de recherches de
comportements partagés sur les prototypes construits. (1) GASPAR peut tout d'abord rechercher sur
tous les mots d'une même catégorie s'il existe des arbres élémentaires en commun. Si tous les
prototypes de mots d'une même catégorie partagent exactement les mêmes comportements (les
mêmes prototypes d'arbres élémentaires l'objet traits qui porte les comportements partagés de
cette catégorie est mis à jour : il portera ces comportements communs. Dans tous les cas, les
prototypes de mots portent, quant à eux, leurs comportements propres. (2) GASPAR peut ensuite
rechercher sur les prototypes pris deux à deux s'ils partagent des arbres élémentaires. Si deux
prototypes de mots d'une même catégorie partagent un ou plusieurs comportements (un ou plusieurs
prototypes d'arbres élémentaires un objet traits est automatiquement construit pour porter ces
comportements partagés. Dans ce cas, GASPAR ajoute automatiquement aux prototypes concernés
un attribut parent qui pointe sur ce nouvel objet porteur de comportements partagés. (3)
GASPAR peut aussi évaluer les comportements partagés sur des sous-familles de prototypes de
mots de même catégorie. Si plusieurs prototypes de mots d'une même catégorie partagent
exactement les mêmes comportements (les mêmes prototypes d'arbres élémentaires un objet
traits est automatiquement construit pour porter ces comportements partagés. Dans ce cas,
GASPAR ajoute automatiquement aux prototypes concernés un attribut parent qui pointe sur ce
nouvel objet porteur de comportements partagés. Les comportements propres à chacun des
prototypes leur restent attachés. (4) GASPAR permet aussi d'évaluer automatiquement les
différences comportementales des arbres élémentaires. Il est en effet possible d'établir une recherche
sur les arbres élémentaires de même catégorie des comportements partagés (arbres d'analyse) par
ces arbres élémentaires. Ce classement utilise une démarche similaire à celle qui est utilisée pour
classer les mots. Si plusieurs prototypes d'arbres élémentaires d'une même catégorie partagent
exactement les mêmes comportements (les mêmes prototypes d'arbres d'analyse un objet traits
est automatiquement construit pour porter ces comportements partagés. Là encore, GASPAR ajoute
automatiquement aux prototypes concernés un attribut parent qui pointe sur ce nouvel objet
porteur de comportements partagés. La figure qui suit donne une trace graphique de pôles de
comportements partagés construits sur notre corpus de test. Elle présente en particulier la classe de
mots regroupant les adjectifs marginale, circonflexe, coronaire e t carotide.
Dans cet exemple, ces adjectifs partagent un arbre élémentaire porté par l'objet traits construit.
Il est à noter que la classe produite ici est sémantiquement homogène.

            
Classes de mots sur corpus de test.

GASPAR permet d'activer des processus de classement qui proposent des regards multiples et
croisés sur les savoirs représentés. Ces processus construisent des réseaux de hiérarchies locales
entre prototypes de mots et prototypes d'arbres ou entre prototypes d'arbres, ces liens multiples
constituent autant de pistes de sens à interpréter. Pour le moment GASPAR construit des pôles de
comportements partagés par des ensembles de prototypes. Il reste ensuite à interpréter ces pôles de
comportements partagés par une intervention manuelle. En effet le classement opéré s'appuie
essentiellement sur des contraintes syntaxiques, et la syntaxe ne permet pas à elle seule de délimiter
des classes de noms reflétant une notion. Si GASPAR peut automatiser le classement des prototypes
lexicaux sur la base des comportements qui leurs sont associés, les résultats restent à qualifier, à
nommer : dans notre dispositif, c'est l'observateur conscient qui donne le sens. C'est en examinant à
la main les rapprochements constatés et les classes de mot construites que l'on pourra leur donner
un nom c'est à dire nommer les choses. Le réseau mis en place ne construit donc pas une
représentation du sens attaché aux unités lexicales ou aux structures syntaxiques représentées, il doit
proposer des amorces d'interprétation qui doivent être affinées par un travail d'interprétation plus
fin. Ce réseau est conçu en fait pour aider à accéder aux sens (Habert & al. 1997). Les pôles obtenus
et les classes de mots sous-jacentes sont des ébauches imparfaites qui permettent une organisation
du matériel lexical. Ces classes doivent ensuite aider à affiner le travail de description des unités
lexicales représentées sous la forme de prototypes.

          
 Ajustements dynamiques et interprétations manuelles

Notre approche vise à établir une démarche interprétative contrôlée et progressive. La démarche
suivie s'inscrit dans une perspective expérimentale à différents niveaux : (1) construire des
représentations des mots à partir de savoirs extraits d'un corpus ; (2) construire des représentations
des comportements des mots : les arbres associés ; (3) établir un premier classement. Si les
informations attachées aux mots ne sont pas disponibles dès la première phase de génération des
prototypes, il sera toujours possible d'ajuster les représentations construites en utilisant un nouveau
flux d'informations disponibles ultérieurement. On peut par exemple projeter les résultats
intermédiaires construits sur des bases de savoirs établies par ailleurs pour ensuite affiner le travail
de description amorçé à la manière de la démarche suivie par A. Mikheev et S. Finch (1994). Il
s'agit en fait de tendre vers une cohérence des classes sémantiques issues des processus de
classement afin de dégager par affinements successifs des descriptions sémantiques pour les mots
représentés. Cette construction progressive de la description des mots passe par un apprentissage
manuel de nouveaux savoirs. C'est à l'utilisateur du dispositif d'interpréter et d'évaluer les objets
construits et les résultats produits. En l'occurrence, cette intervention peut être réalisée par
l'utilisateur linguiste du dispositif ou par un spécialiste du domaine. Ce travail d'interprétation est
d'ailleurs un passage obligé de toutes les approches en classification automatique (en analyse de
données par exemple, mais aussi dans des traitements syntaxiques à la Grefenstette (1993, 94).

          
 GASPAR : un dispositif expérimental

Les informations utilisées sont nettement insuffisantes pour produire des résultats significatifs. Le
classement opéré prend appui sur des caractéristiques grossières à la fois en raison de contraintes
matérielles et de la difficulté à récupérer et organiser les informations à représenter. Les résultats
produits par GASPAR sont en fait limités actuellement par les contraintes matérielles imposées par
ce langage expérimental sur les machines que nous utilisons. Il faut en effet beaucoup de mémoire
et d'espace disque sur les machines qui portent le système Self pour mettre en oeuvre cette
représentation de la mouvance. La mise en oeuvre de ce dispositif peut être considérée comme une
expérience pilote qui offre une image partielle, pour le moment, des traitements réalisés et des
résultats à venir. Pour le travail réalisé sur les gros corpus nous avons restreint le nombre de
contraintes syntaxiques associées aux mots. Dans un premier temps nous avons travaillé sur des
séquences NAdj extraites via LEXTER. À partir de 8 754 séquences comportant des groupes
nominaux, nous avons extraits 586 mots (des noms) auxquels sont attachés 1 413 arbres
élémentaires de type : Sn1 -> Nom Adj, Sn2 -> Adj Nom, Sn3 -> Adj XX, Sn4 ->
XX Adj. Cette première sélection a donc consisté à ne garder que les arbres binaires portant les
feuilles Nom/XX e t Adj. Le processus de génération conduit à la création des prototypes pour
représenter les catégories syntaxiques Nom, Adj, XX, Sn1, Sn2, Sn3, Sn4, des objets
traits associés et de plus de 2 000 prototypes par copie et ajustement. Avec les prototypes créés,
GASPAR a ensuite cherché à repérer ceux qui partageaient exactement les mêmes comportements.
Ce processus de classement conduit à la création automatique de 54 pôles de comportements
partagés. On présente ci-dessous quelques classes de mots obtenues.

Les classes produites sont, dans l'ensemble, cohérentes mais ne produisent pas encore des résultats
pertinents sur le domaine étudié : certaines classes évidentes ou prévisibles sont mises au jour. La
classe de mot associée au pôle n°3 est homogène dans sa relation avec l'adjectif dernier, de
même pour la classe n°2 dans sa relation avec l'adjectif ponderal. La classe n°1, où la relation de
localisation qualifie un membre ou une région du corps, est elle aussi cohérente ; pour cette classe,
on note que les noms qualifiés ne le sont que pour l'adjectif localisant gauche ; à la différence de
la classe n°10, celle-ci étant moins homogène. Les classes n°5, 6, 9 regroupent quant à elles des
noms sémantiquement plus éloignés. Pour enrichir ce travail de description du comportement des
mots, on doit évidemment pouvoir examiner d'autres types de relation binaire. On doit aussi
d'examiner en détail tous les types possibles de regroupements de mots : certains mots partagent
individuellement plus de comportements avec d'autres mots. L'absence de critères numériques
manque aussi pour comparer les fréquences de réalisation des proximités de co-occurrences
rencontrées.

          
 Perspectives

Le choix des prototypes semble cohérent avec la volonté de représenter des savoirs évolutifs. Les
prototypes sont malléables : ils se construisent contextuellement et leur spécialisation se définit
suivant les évolutions contextuelles. Ils peuvent commencer par fixer un savoir minimal - qu'il est
possible d'attacher à une entrée lexicale - puis finir par étendre ce noyau de sens dans les directions
permises par les configurations interprétatives rencontrées. Les limites de l'automatisation des
processus de représentation et de classement marquent le champ de travail qu'il reste à effectuer
manuellement pour mener à bien la représentation et le classement des unités lexicales. La nécessité
de sous-représenter la description des mots dans un dispositif de TALN conduit de fait à une perte
en richesse expressive. La syntaxe est utilisée dans notre travail comme un " marche-pied pour
l'acquisition de connaissances " (Habert, Nazarenko 1996). La PàP permet ensuite d'établir un
compromis intéressant entre formalisation et implémentation : cette démarche de représentation
permet en effet de mener un travail d'expérimentation qui doit conduire à une représentation par
ajustements successifs. Notre travail a porté, dans l'immédiat, sur la mise en oeuvre de processus
automatiques pour la représentation et le classement de mots sous la forme de prototypes. Nous
n'avons pas pu testé les processus définis dans GASPAR sur des gros corpus. Les résultats actuels
restent donc limités. De nombreux prolongements restent à faire. Sur le plan technique, la
couverture des gros corpus doit être réalisée ; on pourrait ainsi, sur le plan linguistique, évaluer et
analyser les résultats construits. Le développement d'outils pour la représentation et pour le
classement dynamiques des savoirs linguistiques doit être étendu. La réalisation d'un classement
automatique général des unités lexicales semble hors d'atteinte tant qu'on ne pourra pas construire
de nouvelles connaissances capables d'enrichir les savoirs déjà établis. Il reste malgré tout possible
d'affiner les processus de classement mis en place et de fait d'affiner le classement visé tout en
continuant à travailler sur les métaconnaissances à l'oeuvre dans les faits de langue utilisés.

          
 Remerciements

Je remercie Benoît Habert (ELI - ENS de Fontenay-St Cloud) qui a suivi pas à pas les différentes
étapes de ce travail. Je remercie également Didier Bourigault (DER-EDF Benoît Habert et Adeline
Nazarenko pour m'avoir donné accès aux résultats de LEXTER et de ZELLIG. Je n'oublie bien
évidemment pas les membres du groupe Self à Stanford et à Sun Microsystems qui ont développé le
langage Self et qui ont toujours répondu avec bienveillance à mes sollicitations.




        
          
Introduction

La description d'itinéraires est d'un intérêt à la fois théorique, notamment pour les Sciences
Cognitives, et pratique, pour la construction d'outils d'aide à la navigation (Denis, 1994). Plusieurs auteurs travaillant sur la génération de descriptions d'itinéraires se sont intéressé aux aspects cognitifs (Carstensen, 1992; Maaß, 1993; Herzog et al., 1993; Gryl & Ligozat, 1995). Nous
nous limiterons ici à la génération automatique de descriptions d'itinéraires en métro. En particulier, nous nous intéresserons aux variations du contenu et de la forme linguistique. C'est surtout
par rapport à ces aspects que notre travail se distingue des autres travaux portant sur la description d'itinéraires.
Le contenu et la forme d'une description d'itinéraire dépendent avant tout de la nature des
données : le type d'environnement et l'itinéraire particulier à décrire. Cependant, comme révèle
l'analyse de notre matériel empirique, de grandes variations sont possibles, tant au niveau du
contenu qu'au niveau de la forme, ce qui fait que de nombreuses descriptions peuvent être produites pour un itinéraire donné. Afin de pouvoir simuler cet éventail de variations, nous avons
essayé d'isoler les facteurs pertinents. Deux d'entre eux semblent particulièrement importants :
l'aspect pragmatique qui consiste à tenir compte de l'importance relative d'informations et l'aspect stylistique. On notera que ce premier aspect ne se résume pas à un couple de termes binaires
(important vs non important car une information peut être "plus ou moins" importante.
Ainsi, l'importance relative d'une information ne détermine pas seulement son inclusion / exclusion au
sein du texte, mais également sa forme linguistique en cas d'inclusion. Quant aux facteurs stylistiques, ils impliquent à la fois le degré d'explicitation des informations (explicite vs implicite)
et leur réalisation linguistique.

          
 Modèle cognitif général

La production de descriptions d'itinéraires peut être décomposée en deux étapes : la détermination de l'itinéraire et la formulation de sa description (Carstensen, 1992; Maaß, 1993; Denis,
1994). La première tâche n'appartient pas au domaine du discours puisqu'elle peut être mise en
oeuvre pour des fins non-verbales (ex. dessiner un plan ou même non-communicatives (effectuer soi-même un déplacement). La deuxième tâche, en revanche, est de nature discursive, car
elle vise la production d'un texte.
La détermination de l'itinéraire consiste à identifier un parcours dans un espace, en utilisant
d'une part des connaissances sur l'environnement (connaissances référentielles et d'autre part
des connaissances liées aux besoins de l'interlocuteur (connaissances pragmatiques). Le résultat
de ce processus est une représentation référentielle. Celle-ci constitue le point de départ du processus de la description de l'itinéraire. Il est fondé sur des connaissances discursives relatives
au domaine (aspects "conceptuels", "textuels" et "linguistiques") et sur d'autres connaissances
pragmatiques concernant l'interlocuteur.
C'est notamment la partie "discursive" du traitement qui nous intéresse ici, c'est-à-dire la
description de l'itinéraire. Elle consiste à "transformer" la représentation référentielle de l'itinéraire en sa représentation discursive. Ce processus se divise essentiellement en trois parties :
conceptuelle, textuelle et linguistique. La structuration conceptuelle consiste à "découper" la représentation référentielle en unités verbalisables (informations). La structuration textuelle implique le choix des parties du contenu conceptuel à expliciter dans le texte. Enfin, la réalisation
linguistique détermine la forme de surface. Notre modélisation des principes conceptuelles, textuelles et linguistiques – principes faisant partie des connaissances discursives – est fondée sur
l'analyse de descriptions d'itinéraires en ville destinées à des piétons. Cette modélisation nous a
servi ensuite de trame pour analyser des exemples de descriptions d'itinéraires en métro en vue
de la génération automatique.

          
Analyse et modélisation de descriptions d'itinéraires en métro

Notre corpus est constitué d'une trentaine de descriptions d'itinéraires en métro parisien. Il a
été recueilli par courrier électronique auprès de dix sujets qui décrivaient chacun trois itinéraires.
Le corpus contient donc dix descriptions de trois itinéraires variant en termes de longueur et de
complexité. Le premier itinéraire (Saint-Lazare – Jussieu) est le plus long (9 stations) et contient
un changement. Le deuxième (Denfert-Rochereau – Place d'Italie) est de 4 stations et c'est le
plus simple car il n'implique pas de changements. Enfin, le troisième itinéraire (Jussieu – Gare
de Lyon) est de même longueur que le précédent (4 stations mais c'est le plus complexe car il
contient deux changements.
Notre analyse du corpus a porté sur trois aspects : le contenu conceptuel, le contenu textuel
et la forme linguistique. Nous en présentons les résultats dans les trois sections suivantes.
La distinction entre le contenu conceptuel et le contenu textuel n'est habituellement pas faite dans
les modèles de génération de textes dont nous avons connaissance, dans lesquels la détermination du contenu ("quoi dire") est opposée à la détermination de la forme linguistique ("comment dire").
D'après nous, le contenu conceptuel concerne l'ensemble des messages "à transmettre",
tandis que le contenu textuel concerne les informations "à exprimer" réellement dans le texte
(les informations explicites par opposition aux informations implicites).

            
Contenu conceptuel

Un itinéraire est structuré en segments et en relais. Un segment est un fragment d'itinéraire
qui se caractérise par un même "chemin" et par une suite de "lieux" différents, tandis qu'un relais
correspond à un lieu qui coïncide avec un changement de chemin. La description de l'exemple 1
ci-dessous reflète la structuration de l'itinéraire en deux segments et un relais, comme illustré
dans la figure 1.
Ex. 1 À Saint-Lazare, prendre la direction Gallieni. Descendre à Opéra (deux stations plus loin).
Prendre alors la direction Mairie d'Ivry/Villejuif jusqu'à Jussieu (7eme station).

              
Structuration conceptuelle "globale" de l'itinéraire décrit dans l'exemple 1.

L'analyse de descriptions d'itinéraires en ville nous a permis de constater qu'un même itinéraire peut être structuré de différentes façons car la détermination d'un "chemin" peut se faire
selon un ou plusieurs critères, comme la direction, le type ou le nom de la voie, des caractéristiques de l'environnement immédiat, etc. Cependant, dans le cas du métro, les principes concernant la structuration conceptuelle d'itinéraires sont plus stricts. Cela est dˆu au fait qu'en métro
les "chemins" sont toujours définis par la direction de la ligne à emprunter. Ainsi, un segment
correspond à un fragment d'itinéraire avec la même direction et un relais à un lieu de changement de direction, c'est-à-dire à une station de correspondance.
Au sein des segments et relais, nous distinguons quatre types d'étapes. Dans le tableau 1, nous
énumérons ces étapes, et nous en donnons des exemples tirés du corpus. Comme nous le montrerons plus loin, toutes ces étapes n'ont pas nécessairement à être explicitées, du fait que les
principes de la structuration conceptuelle d'itinéraires font partie intégrante des connaissances
partagées par les interlocuteurs. La représentation conceptuelle de l'ensemble des étapes consécutives est néanmoins nécessaire, aussi bien pour le locuteur afin qu'il puisse produire une description, que pour l'interlocuteur afin qu'il puisse la comprendre.

              
Étapes d'itinéraire et descriptions correspondantes.

                
La structuration conceptuelle concerne également la détermination de repères. Nous en distinguons deux principaux types : les repères-simples et les repères-chemins. Les premiers sont associés aux relais, tandis que les seconds sont associés aux segments.

Dans le cas des descriptions d'itinéraires en métro, le premier type de repères correspond aux stations, et le deuxième type
correspond aux sections de lignes de métro allant dans une même direction. Nous définissons les
repères en termes de couples attribut-valeur. Nous avons spécifié les attributs pour chaque type
de repères, comme montré dans le tableau 2.

              
Représentation de repères.

                
L'analyse du corpus nous a également permis de déterminer les attributs obligatoires et optionnels (voir tableau 2). Nous considérons comme "obligatoires" les attributs apparaissant dans
toutes les descriptions du corpus : direction de repère-chemin et nom de repère-simple (nom
d'une station de correspondance). Bien qu'il n'apparaisse pas dans toutes les descriptions, nous
considérons également l'attribut entité pour les deux types de repères comme obligatoire car ses
valeurs sont stables dans le contexte du métro ("ligne" pour un repère-chemin et "station" pour
un repère-simple). Par ailleurs, ces informations sont connues de l'interlocuteur, si bien qu'il
n'est pas nécessaire de les expliciter. Les attributs optionnels sont ceux qui n'apparaissent pas
dans toutes les descriptions du corpus : nom et dimension de repère-chemin et ordre de repère-simple.
Les attributs obligatoires doivent être représentés au niveau conceptuel. Ils font donc partie intégrante du contenu conceptuel des descriptions, même quand ils ne sont pas explicités
dans le texte (c'est le cas de l'attribut entité). Quant aux attributs optionnels, leur représentation conceptuelle et leur forme linguistique dépendent de l'importance relative que leur donne le
locuteur. La valeur informationnelle peut être définie en fonction d'indices linguistiques. Selon
Combettes (Combettes, 1992 les propositions principales relèvent du "premier plan" du texte
(informations primaires tandis que les structures subordonnées relèvent du "second plan" (informations secondaires). En ce qui concerne l'importance relative des informations exprimées,
nous pensons que les structures subordonnées peuvent avoir un statut différent. En effet, l'analyse du corpus nous amène à croire que, par exemple, une proposition relative indique une importance plus grande de l'information exprimée qu'une apposition entre parenthèses.
Pour les attributs optionnels dimension et ordre, qui véhiculent une même valeur informationnelle (ex. 3 pour "trois stations" et pour "troisième station" nous avons pu dégager des facteurs "contextuels" qui influencent leur importance relative. Il s'agit notamment du nombre de
stations dans un segment (distance partielle) et de la partie de l'itinéraire. Nous avons remarqué qu'une plus grande importance est accordée à l'information concernant la distance s'il n'y a
qu'une seule station et/ou s'il s'agit du dernier segment de l'itinéraire. L'importance relative du
troisième attribut optionnel : nom de repère-chemin (nom d'une ligne) n'est pas affectée par ce
type de différences ; l'importance relative sera la même pour tous les noms des lignes pertinentes
pour la description d'un itinéraire.
À la suite de l'analyse du corpus, et en vue de la génération automatique, nous avons défini
quatre stratégies d'attribution d'importance pour les attributs dimension et ordre et trois stratégies pour l'attribut nom de la ligne, ainsi que les formes linguistiques correspondantes. Ces
résultats sont présentés dans les tableaux 5 et 6 plus loin.

            
Contenu textuel

La détermination du contenu textuel consiste à sélectionner, à partir du contenu conceptuel,
les étapes d'itinéraire et les attributs de repères à expliciter dans le texte. Comme nous l'avons
déjà signalé, toutes les étapes ne sont pas nécessairement exprimées dans la description, surtout lorsqu'il s'agit d'un itinéraire composé de plusieurs segments. L'étape la moins souvent
mentionnée dans les descriptions d'itinéraires en métro est le transfert-segment. En effet, à la
différence des descriptions d'itinéraires en ville, on ne rencontre pas ici d'expressions comme :
"continuer" ou "aller tout droit". Cela est dˆu à la situation du voyageur de transport en commun qui, une fois embarqué, n'est plus tout à fait maître de son déplacement. Cependant, il faut
noter que même si les verbes d'action n'apparaissent pas dans notre corpus pour exprimer un
transfert-segment, ce dernier est parfois mentionné en termes de distance : "il y a n stations"
(expression de l'attribut dimension de repère-chemin). En ce qui concerne les autres étapes, il
s'avère que le début-segment est toujours explicité. Ceci peut être expliqué par le fait que le
type d'expression lui correspondant exprime également l'attribut obligatoire de rep ère-chemin :
direction (ex. "prendre direction Gallieni"). En revanche, l'expression des autres étapes est variable. Afin d'illustrer ce point, nous analysons dans le tableau 3 les exemples 2 et 3 ci-dessous
référant à un même itinéraire.
Ex. 2 Prendre direction Gallieni (ligne 3). Sortir à Opéra (2 stations). Prendre direction Mairie
d'Ivry/Villejuif (ligne 7). Descendre à Jussieu (7eme station).
Ex. 3 À partir de Saint-Lazare, prendre la ligne 3 en direction de Gallieni. S'arrêter à la station Opéra
et changer pour prendre la ligne 7 en direction de Mairie d'Ivry.

              
Analyse des exemples 2 et 3 par rapport à l'expression des étapes.

                
Dans l'exemple 2, les mêmes étapes sont exprimées pour les deux segments de l'itinéraire :
début-segment et fin-segment, tandis que l'étape transfert-relais n'est pas exprimée. L'exemple 3
est différent : pour le premier segment on mentionne les étapes début-segment et fin-segment, on
mentionne aussi l'étape transfert-relais, et pour le deuxième segment seulement l'étape début-segment est exprimée. La dernière phrase de ce deuxième exemple n'est pas représentée dans le
tableau car elle n'évoque pas d'étape. L'analyse du contenu textuel des descriptions en termes
d'étapes nous a permis de spécifier des schémas textuels, appelés blocs, utilisés par notre générateur.
Comme déjà mentionné, l'analyse du contenu textuel du corpus traite également du problème
de l'explicite/implicite de certains attributs de repères. Il s'agit des attributs "obligatoires"
aux valeurs stables, c'est-à-dire de l'attribut entité de repère-chemin (valeur : "ligne") et de
l'attribut entité de repère-simple (valeur : "station"). Concernant l'entité "ligne", nous avons
constaté qu'elle est mentionnée explicitement uniquement pour introduire l'attribut nom de
repère-chemin (nom d'une ligne). Ainsi, nous trouvons des énoncés comme : "prendre la ligne 3
en direction de Gallieni", mais non : "prendre la ligne en direction de Gallieni". Quant à l'entité
"station", son expression n'est pas nécessaire pour introduire l'attribut nom de repère-simple
(nom d'une station) ; nous trouvons aussi bien "descendre à Opéra" que "descendre à la station
Opéra". Il apparaît que la décision d'expliciter ou pas l'information concernant l'entité "station"
s'applique généralement à toute la description, c'est-à-dire à toutes les stations mentionnées.

            
Forme linguistique

En ce qui concerne la forme linguistique, nous distinguons deux unités de base : séquences et
connexions. Les premières ont pour fonction d'exprimer des informations concernant les étapes
et les repères, tandis que les secondes servent à "connecter" les séquences et les blocs de séquences avec la structure conceptuelle de l'itinéraire. Les séquences sont divisées en noyaux et
ajouts. Dans le tableau 4, nous donnons un exemple d'une description d'itinéraire analysée en
termes d'unités linguistiques.

              
Analyse d'une description d'itinéraire en termes d'unités linguistiques.

                
Les noyaux sont réalisés par des propositions indépendantes. Elles expriment les informations
obligatoires et des informations optionnelles auxquelles on attribue une plus grande importance.
Les ajouts correspondent à des structures "subordonnées" (relatives, appositions, propositions
"anaphoriques"). Elles expriment des informations additionnelles concernant les repères : attributs de moindre importance (généralement des attributs optionnels). Dans les tableaux 5 et 6
plus bas, nous présentons les règles induites à partir du corpus qui sont utilisées par notre générateur de descriptions d'itinéraires. On y trouve une spécification des stratégies concernant
l'importance relative des attributs optionnels en relation avec les formes linguistiques.

          
Génération de descriptions d'itinéraires en métro

            
Stratégies concernant l'importance relative des attributs dimension et ordre et leurs formes linguistiques correspondantes.

              
            
Stratégies concernant l'importance relative de l'attribut nom de ligne et ses formes

              
Le générateur est programmé en GNU Emacs Lisp. Son architecture est fondée sur notre modèle cognitif de la production de descriptions d'itinéraires et sur les résultats de l'analyse du
corpus que nous venons de décrire. Le générateur est composé de deux modules : (1) un module référentiel qui détermine les itinéraires, et (2) un module discursif qui engendre les descriptions correspondantes. Pour valider notre modèle, nous avons utilisé les données du métro
de Montréal. Le fonctionnement du générateur décrit ici est illustré dans la figure 2 pour l'itinéraire "Charlevoix – Acadie" ayant deux changements.
La sortie du module référentiel et l'entrée du module discursif est constituée par la représentation référentielle de l'itinéraire (voir figure 2). Cette représentation est une liste de noeuds qui
correspondent à des quais, encodés par des couples dont le premier élément indique la station et
le deuxième la direction.
Le module discursif est divisé en deux parties. La première est la partie conceptuelle dont
le rôle est de préparer les informations à transmettre. Elle est réalisée par deux fonctions principales. La première prend en entrée la représentation référentielle, pour donner en sortie une
première représentation conceptuelle symbolisant la structuration de l'itinéraire en unités "globales" (segments et relais) et "locales" (étapes). La deuxième fonction prend en entrée la représentation précédente, les données référentielles pertinentes, ainsi que deux paramètres spécifiant les "stratégies d'importance relative" des attributs optionnels : dimension de repère-chemin,
ordre de repère-simple (premier paramètre, valeurs possibles : 0-3) et nom de repère-chemin
(deuxième paramètre, valeurs possibles : 0-2). Ces paramètres sont responsables des variations
du contenu conceptuel et de la forme linguistique. Dans notre exemple (figure 2 ces valeurs sont
fixées respectivement à 1 et à 2. La fonction produit en sortie, sous forme de couples attribut-valeur, la deuxième représentation conceptuelle, concernant les repères. Sont représentés ici tous
les attributs obligatoires des repères ainsi que leurs attributs optionnels (dimension, ordre et/ou
nom de ligne) selon les valeurs de leurs paramètres d'importance. Si la valeur du paramètre d'importance est 0, l'attribut concerné n'est pas représenté. Si cette valeur est supérieure à 0, l'attribut et son importance relative sont représentés.
Ce dernier type d'information est encodé par les attributs : degré-dim, degré-ordre et degré-nom.

            
Illustration du traitement pour l'itinéraire "Charlevoix – Acadie".

Dans la représentation des repères donnée en
exemple (figure 2 les attributs dimension et ordre sont représentés uniquement pour la distance
d'une station (donc seulement pour le repère-chemin et le repère-simple du premier segment)
car l'importance spécifiée dans le paramètre en entrée est faible (valeur 1). Le paramètre d'importance pour l'attribut nom de repère-chemin a la valeur 2 (importance forte) donc cet attribut
est représenté en sortie.
La deuxième partie du module discursif est la partie linguistique. Elle utilise un dictionnaire
de formes verbales et nominales employées dans les descriptions d'itinéraires en métro. La réalisation linguistique est basée sur des fonctions qui encodent les schémas de séquences variant en
fonction des types d'étapes à décrire. Ces fonctions prennent en entrée les informations concernant les attributs de repères pertinents, ainsi que la forme verbale à utiliser (ex. inf : "descendre",
prés-2 : "tu descends"). La version de schéma d'une séquence est choisie en fonction des attributs "spéciaux" : degré-dim, degré-ordre et degré-nom qui encodent le degré d'importance des
attributs optionnels, en accord avec les règles données dans les tableaux 5 et 6. Ainsi, dans le
texte de notre exemple (figure 2 la première séquence contient un ajout sous forme d'apposition entre parenthèses exprimant la valeur de l'attribut ordre ("station suivante"). S'agissant
de la même valeur informationnelle que pour l'attribut dimension, ce dernier n'est pas exprimé
(bien qu'il aurait pu l'être à la place de l'attribut ordre : "une station", ce choix étant aléatoire).
Puisque les attributs dimension et ordre des autres repères ne sont pas contenus dans la représentation conceptuelle, ils ne sont pas exprimés dans les séquences décrivant ces repères. Les attributs nom des repères-chemins (ex. "ligne verte") sont exprimés dans les noyaux des séquences
(propositions principales) du fait de leur importance élevée (valeur 2).
Le module discursif contient aussi une partie textuelle. Une des fonctions de cette partie encode les schémas de blocs textuels qui sont des combinaisons des schémas des séquences. Deux
autres fonctions encodent respectivement des schémas de connexions et des schémas de combinaisons possibles de blocs au sein d'une description, ces combinaisons variant en fonction du
nombre de segments de l'itinéraire. La description dans la figure 2 contient trois blocs textuels.
Le premier et le troisième blocs utilisent le même schéma (avec des variations stylistiques "locales") qui consiste à exprimer les étapes début-segment et fin-segment (pour le premier bloc :
"prendre la ligne verte... jusqu'à Lionel-Groulx" ; pour le troisième "prendre la ligne bleue...
et sortir à Acadie"). Le deuxième bloc est engendré à partir d'un autre schéma qui consiste à
exprimer les étapes transfert-relais, début-segment et fin-segment ("changer... prendre la ligne
orange... descendre à Snowdon"). Le schéma de connexions utilisé pour "connecter" les blocs
est le suivant : "à partir de (station ... puis, ... enfin, ...".
Afin d'illustrer la façon dont le contenu conceptuel, le contenu textuel et la forme linguistique
peuvent varier pour le même itinéraire, nous donnons ci-dessous deux autres exemples des descriptions d'itinéraires engendrés automatiquement par notre générateur.
La première description ne contient pas d'informations concernant les attributs dimension et
ordre ni ceux de nom de lignes car les deux paramètres indiquant l'importance relative de ces in-
formations ont été fixés à 0. Le schéma des blocs textuels utilisé prévoit la mention explicite des
étapes suivantes : début-segment et transfert-relais (premier bloc) ; début-segment et fin-segment
(deuxième bloc) ; transfert-relais, début-segment et fin-segment (troisième bloc). Le schéma de
connexions entre les blocs est le suivant : "de (station), ... à (station), ..., à (station), ...".
L'importance relative des attributs dimension et ordre est maximale dans le deuxième texte
(valeur 3) ; en conséquence, le nombre de stations est donné pour tous les cas. L'expression
linguistique de cette information varie cependant pour chaque segment selon les règles définies
dans les tableaux 5 et 6, c'est-à-dire selon qu'il s'agit d'une seule station, du dernier segment
ou autre. Le paramètre spécifiant l'importance relative de l'attribut nom de ligne a été fixé
à 1 (importance faible). Par conséquent, cette information est exprimée par des ajouts du
type appositions entre parenthèses. Le schéma de blocs textuels utilisé ici exprime les étapes
suivantes : début-segment et fin-segment (premier bloc) ; transfert-relais, début-segment et
fin-segment (deuxième bloc) ; début-segment, transfert-segment et fin-segment (troisième bloc).
Le schéma de connexions entre les blocs utilisé est : "d'abord, ... ensuite,... finalement, ...".

          
Conclusion et perspectives

Nous avons présenté la mise en oeuvre d'un générateur de descriptions d'itinéraires en métro,
fondé sur un modèle cognitif de la production de descriptions d'itinéraires et sur une analyse de
corpus. Des facteurs d'importance relative d'informations et des facteurs stylistiques, observés
dans le corpus, nous permettent de mieux contrôler le contenu conceptuel, le contenu textuel et la
forme linguistique pour un itinéraire donné. Les descriptions engendrées automatiquement sont
comparables à celles produites par les sujets.
Notre générateur tourne actuellement sur deux ensembles de données, celui du métro de
Montréal et celui du métro parisien. Nous envisageons d'élargir la portée du générateur à
d'autres types de transport en commun et à la navigation routière.




        



        
          
Introduction

Cet article présente ILIAD, une chaîne de traitement automatique pour l'analyse de l'information contenue dans des corpus de textes de grande taille. Cette chaîne de traitement associe
des méthodes informatiques linguistiques et des méthodes statistiques, assurant, pour chacune
des étapes, une grande robustesse. De plus, son architecture ouverte facilite la prise en compte
de nouvelles fonctionnalités par l'insertion de nouveaux modules de traitement.
L'analyse de l'information peut être définie comme un ensemble d'outils et de méthodes
permettant à un opérateur humain de collecter l'information contenue dans un corpus sans le
lire de façon séquentielle. Cet opérateur peut être ainsi capable d'analyser l'avancement d'un
domaine scientifique ou technique en considérant simplement un ensemble de classes de termes,
construites à partir du corpus initial.
ILIAD a été expérimenté sur un corpus de 2,5 Mb de textes en français sur le domaine de
l'agriculture. Ce sont des résumés de notices bibliographiques et constituent de ce fait, un corpus
d'une grande homogenéïté. L'analyse par ILIAD produit 50 classes actuellement utilisées par
les experts pour la veille technologique ou la recherche d'information.
Après une présentation de l'architecture globale du système, nous décrirons les différentes
étapes du traitement, les méthodologies adoptées (basées sur les avancées récentes de la terminologie, tant du point de vue linguistique que du point de vue de la représentation des connaissances) puis leur évaluation.

            
Du texte à la classification

            
Architecture globale

La Figure 1 donne un exemple de classes construites à partir d'un ensemble de résumés. Chaque
classe regroupe, sur la base d'une mesure de similarité, les termes fréquemment co-occurrents dans
les textes. Ce processus est divisé en quatre étapes
(voir Figure 2 (1.a) et (1.b) pour le pré-traitement
des textes et du thésaurus (étiquetage morpho-syntaxique, segmentation et lemmatisation puis (2) pour
l'acquisition des termes, (3) pour l'indexation, et
(4) pour la classification.
Chaque étape repose sur des outils ou méthodes
existants (ACABIT, FASTER, SDOC) qui sont décrits dans les sections suivantes.
ILIAD a éte appliqu e à un corpus de 7272 résumés de textes en
français (2,5 Mb) sur le domaine de l'agriculture,
extraits de la base PASCAL 1 (corpus désigné par
[AGR]), et utilise le thésaurus A GROVOC 2 qui est
une taxinomie d'environ 15 000 termes, associés
à leurs synonymes et enregistrés dans un format
proche de la norme SGML.
. PASCAL est la base documentaire scientifique développée et maintenue par l'INIST-CNRS, France
. Thésaurus multilingue développé par AGRIS - FAO (Unité de traitement AGRIS ; Organisation des Nations
Unies pour l'alimentation et l'Agriculture).

          
Des textes à la classification

Nous illustrerons la chaîne complète d'analyse par la phrase exemple ( extraite de notre
corpus (comme le montre la Figure 1
Phrase :
La part respective des deux portions du syst ème racinaire de surface et de profondeur dans l'alimentation hydrique d'un arbre varie en fonction de la demande atmosph érique et de la disponibilité de l'eau
du sol.

            
Préparation des Corpus

Les textes et le thésaurus sont préparés par deux processus identiques, c'est pourquoi nous
ne détaillons que celui concernant les textes.
Ceux-ci sont tout d'abord segmentés en phrases, et mis dans un format proche de SGML, puis
l'étiqueteur de Brill3(1993) associe à chaque mot sa catégorie grammaticale la plus probable.
L'application de ces deux tâches sur la phrase ( ) du 3 produit:
La/DTN:sg part/SBC:sg respective/ADJ:sg des/DTC:pl deux/CAR portions/SBC:pl du/DTC:sg système/SBC:sg racinaire/ADJ:sg . . . varie/VCJ:sg . . .
où DTN est la catégorie des déterminants, SBC des noms, DTC des prépositions contractées, ADJ
des adjectifs, CAR des cardinaux, et VCJ des verbes conjugués.
La dernière étape dans la préparation du texte est la lemmatisation des mots fléchis, au moyen
d'un programme basé sur des règles, qui associe à chaque forme fléchie un lemme et un ensemble de traits flexionnels.
La morphologie flexionnelle du français est complexe (en comparaison, par exemple, avec
celle de l'anglais) ; on compte environ 80 modèles de conjugaison et autant de terminaisons
pour les verbes (cf. (Bescherelle, 1990)), et plus de 40 familles flexionnelles pour les noms et
adjectifs. De nombreuses terminaisons sont ambiguës (par exemple "e" ou "s") 4 ce qui fait que
leur interprétation dépend de la catégorie du mot auquel elles sont attachées.
De nombreux lemmatiseurs du français, utilisés en recherche d'information sont, pour cette
raison, basés sur la consultation d'un dictionnaire (cf. (Savoy, 1993)). D'autres systèmes, basés,
au moins partiellement, sur règles (cf. (Karttunen, 1994), (Guilbaud et Boitet, 1997)), ne sont
pas directement comparables avec celui-ci, car ils exploitent un texte non-étiqueté. Par conséquent, le programme d'ILIAD, qui prend en entrée le texte étiqueté, ne doit pas se préoccuper de
désambiguı̈sation. Ne dépendant pas d'un dictionnaire (sauf pour quelques listes d'exceptions
aux règles utilisées), il est capable de prendre en compte les mots inconnus et les néologismes
qui sont supposés avoir un comportement flexionnel régulier. Enfin, de par sa conception, le
lemmatiseur est capable de détecter et corriger certaines erreurs d'étiquetage, quand l'étiquette
est incompatible avec la terminaison du mot, et de rediriger les processus de lemmatisation vers
le traitement de la catégorie la plus adéquate.
Les mots traités par le lemmatiseur sont ceux dont la catégorie fait partie des catégories
majeures fléchies : VCJ, SBC, ADJ, DTN . Le programme est composé de trois modules :
Conversion des cat ́
egories : Un filtre se charge de traduire les catégories affectées par
l'étiqueteur de Brill en un ensemble d'étiquettes internes au programme. Ce système, basé sur
une table de conversion, rend le programme relativement indépendant de l'étiqueteur et du
format d'entrée.
. Cet étiqueteur a été entraîné pour le franc ̧ ais à l'INaLF (cf. (Lecomte et Paroubek, 1996)).
4. "e" est une terminaison ambigu ̈e au sens o`u elle indique notamment le féminin singulier chez les adjectifs,
le singulier chez les noms, l'impératif singulier ou la première/troisième personne du singulier du présent de
l'indicatif/subjonctif. . .

Module de lemmatisation : Pour chaque mot à lemmatiser, la compatibilité terminaison/catégorie est vérifiée, après quoi la fonction correspondante est activée. Deux processus sont
exécutés en parallèle : la génération de la base non fléchie du mot à partir de la base, éventuel-
lement altérée, du mot fléchi, et le calcul des informations flexionnelles. Ces deux proces-
sus mettent en jeu les bases et les terminaisons des mots. Ainsi, si la séquence d'entrée est
(1) tiennent/VCJ, la fonction découpe le mot selon le suffixe de ème personne du pluriel ent.
La forme neutre de la base est calculée (tenir) ; le calcul des informations flexionnelles résulte
de l'intersection des traits de la base fléchie (qui marque soit l'indicatif présent, soit le subjonc-
tif présent, soit l'impératif d'un verbe du ème groupe) et du suffixe (qui est valide à tous les
temps conjugués, sauf au futur et à l'impératif).
Sortie du lemmatiseur : Les résultats sont tout d'abord codés dans un format interne,
puis convertis dans un format de sortie. Ainsi, la lemmatisation de (1) génère tout d'abord la
séquence :
Les performances du lemmatiseur ont été évaluées en comparant ses résultats avec ceux des
deux corpus lemmatisés manuellement : on trouve 1,75% de différence par rapport au lexique
TLFnome 5 (412 081 entrées) et 0,12% de différence par rapport à un corpus d'articles de "LeMonde" étiqueté par l'Equipe
TALANA (432 636 occurrences). La comparaison n'a porté que
sur les phénomènes couverts actuellement par le programme. Entres autres, les taux calculés ne
tiennent pas compte des noms composés par hyphénation, dont les règles de lemmatisation n'ont
pas encore été intégrées au programme. Essentiellement, les autres phénomènes non encore
couverts sont : le nombre de certains noms ou adjectifs en fonction du suffixe et la détermination
du genre de la majorité des noms par l'examen de leur terminaison quand cela est pertinent.

            
Pré-traitement du thésaurus

Le thésaurus est converti en une liste de termes. Il est ensuite étiqueté par l'étiqueteur de
Brill qui a été au préalable entrainé spécifiquement pour l'étiquetage de séquences nominales.
Le taux de réussite est de 98%. Après la lemmatisation, la structure d'un terme est la suivante :

            
L'indexation des textes par les termes

L'indexation des textes par les termes consiste à :
a) acquérir de nouveaux termes à partir des textes lemmatisés Etape
2 de la Figure 2) pour
enrichir le thésaurus (voir 3.3.1),
b) utiliser le thésaurus ainsi enrichi pour extraire les variations des termes et indexer les
textes ( Etape
3 de la Figure 2, voir 3.3.2).
. Lexique de référence dérivé de le Trésor de la Langue Française

            
Acquisition de termes à partir du corpus

Les termes recyclés obtenus à partir du thésaurus AGROVOC sont de bons descripteurs mais
malheureusement ils sont en nombre insuffisant pour permettre une réelle indexation du document. Un grand nombre de termes spécifiques au domaine reflétés par le corpus manquent ainsi
que des termes à caractère générique, c'est-à-dire non spécifique au domaine mais porteur dans
notre corpus d'informations essentielles. Ces nouveaux termes sont extraits automatiquement 6
de notre corpus à l'aide du programme ACABIT (Daille, 1996) basé sur une approche mixte. La
méthodologie est la suivante : une série de filtres linguistiques chargés de repérer les séquences
de mots partageant une structure morphosyntaxique caractéristique des termes est appliquée sur
le corpus étiqueté et lemmatisé. Ces structures sont ensuite soumises à un test statistique qui
permettent de les classer de la plus à la moins pertinente. Ces filtres linguistiques sont exprimés
sont forme de grammaires locales chargées de détecter les noms composés binaires, c'est-à-dire
comportant deux mots n'appartenant pas  ́a l'ensemble des mots fonctionnels 7 . Cette décision
de se concentrer sur des termes nominaux binaires est fondé sur des critères linguistiques et
statistiques, et sur un critère pratique : ils sont en adéquation avec le format d'entrée de FASTR
(cf. section 3.3.2).
Les règles (2) et (3) ci-dessous exprimées à l'aide de la syntaxe des expressions régulières
isolent des patrons prédéfinis de termes binaires : la règle (1) détecte des groupes adjectivaux ; la
règle (2) des termes de structure morphosyntaxique SBC ADJ comme par exemple alimentation
hydrique ; la règle (3) des termes de structure Nom1-Préposition-Nom2 comme par exemple eau
du sol .
Ce programme d'extraction de termes prend aussi en compte certaines de leurs variations.
Pour illustrer la manière dont sont prises en comptes les variations des termes, examinons la
règle (4) permettant de reconnaı̂tre une coordination de tête entre deux termes de structure
Nom1-Préposition-Nom2 :
La règle (4) isole deux termes binaires, par exemple système de surface et système de profondeur au sein d'une séquence de mots comportant une coordination comme par exemple système
racinaire de surface et de profondeur .
Tous les candidats termes binaires ainsi extraits du corpus sont ensuite, pour chaque phrase,
triés selon la valeur calculée du coefficient de vraisemblance (loglike) (Dunning, 1993). Cette
mesure statistique donne de bons résultats pour obtenir un classement des candidats termes, extraits à l'aide des filtres linguistiques, du plus au moins représentatif du domaine (Daille et al.,
1995). Pour chaque phrase ne sont retenus que deux candidats termes ; ceux pour qui la valeur
du coefficient de vraisemblance est la plus élevée. Cette étape est illustrée par la table 1(a).
Ce filtrage ad-hoc nous permet de couvrir correctement le corpus puisque 60 % des candidats-termes sont ainsi retenus. Néanmoins, si le coefficient de vraisemblance donne de bons résultats pour obtenir un tri des candidats en fonction de leur représentativité par rapport à un domaine,
nous n'avons pour l'instant aucune preuve qu'il soit aussi pertinent pour juger leur caractère
informatif. L'évaluation de la pertinence du coefficient de vraisemblance pour l'accès à l'information est actuellement en cours et fait intervenir des documentalistes, experts dans le domaine
de l'agro-alimentaire. Il est aussi évalué s'il est plus intéressant d'effectuer le filtrage au niveau
de la phrase ou directement au niveau du paragraphe (ce dernier représentant une référence
bibliographique).
Dans l'attente des résultats de l'évalation sur le caratère informatif des candidats termes,
nous avons ici verifié leur correction linguistique La précision du programme d'extraction a été
évaluée sur un ensemble de 300 phrases extraites au hasard du corpus. La table 1(b) montre que
95% des candidats termes retenus sont bien formés.
. L'extraction des termes à partir du corpus est automatique ; néanmoins la liste de candidats termes proposés
nécessite habituellement d'ˆetre post-éditée de manière à éliminer les mauvais candidats.
. Sont considérés comme mots fonctionnels, les prépositions, les articles, les conjonctions de coordinations
pouvant apparraître à l'intérieur d'un terme.

              
L'extraction des termes par ACABIT

                
            
Extraction de variantes pour l'indexation

L'étape suivante de la chaı̂ne de traitement consiste à indexer les documents en recherchant
dans les documents les occurrences de termes et de leurs variantes (voir le récapitulatif de
l'organisation du projet à la figure 2). L'indexation est faite au moyen de l'analyseur transformationnel partiel FASTER (Jacquemin et al., 1997).
L'analyse des documents par FASTER repose sur les trois types de données suivants :
1. un corpus morphologiquement analysé et désambiguı̈sé où les mots fléchis sont associés
à un lemme unique et une structure de traits morphologiques unique. Cette structure attributs/valeurs définit les caractéristiques morphologiques du mot fléchi telles que le temps,
le genre, le nombre, etc. (voir 3.1);
2. une liste de termes lemmatisés où les mots ont un lemme unique et une catégorie syntaxique unique. Soit ces termes appartiennent à un thésaurus — et on parle alors d'indexation contrôlée (voir 3.2) —, soit ces termes sont issus d'ACABIT, l'outil d'acquisition
automatique de termes présenté au  3.3.1 — et il s'agit alors d'indexation libre —;
3. une métagrammaire du langage considéré décrivant les patrons de variation terminologique (environ 100 métarègles).
Dans ce paragraphe, nous décrivons le rôle de la métagrammaire qui implémente des transformations morpho-syntaxiques locales représentant des variations terminologiques. Les méta-règles sont des fonctions de l'ensemble des termes dans l'ensemble des variantes ; elles prennent
en entrée un terme de la base et produisent en sortie un patron de variation de ce terme. Ce patron est partiellement instancié puisqu'il comporte des éléments non lexicaux. Il est utilisé pour
retrouver dans les documents des occurrences de variantes de termes. La table 2 illustre les index en sortie de FASTER lors de l'analyse de la phrase ( à partir des termes acquis
par ACABIT sur le corpus [AGR]. On retrouve, bien sûr, en sortie, les deux termes acquis par
ACABIT sur cette phrase (les deux premières lignes de la table 1(a) et les occurrences (2.a)
et (2.h) de la table 2). Les autres occurrences correspondent à des termes acquis sur d'autres
phrases du corpus [AGR].
La plupart des travaux en TALN pour la recherche d'information sont appliqués en indexation libre (Schwarz, 1990; Sheridan et Smeaton, 1992; Strzalkowski, 1996). Ces analyseurs
à large couverture décomposent des structures syntaxiques en dépendances élémentaires qui
constituent les index du texte. Au contraire, la finalité de FASTER est l'indexation contrôlée :
il s'agit de retrouver, au moyen d'une base de termes et d'une métagrammaire de variations locales, les occurrences de ces termes et de leurs variantes. La sortie de l'analyseur sont des liens
linguistiquement documentés aux termes de la base. Une étiquette 0 sur le lien signifie qu'il
s'agit d'une occurrence — éventuellement fléchie — d'un terme de la base. Toutes les autres
étiquettes dénotent des variantes.
Les exemples de la table 2 illustrent les deux familles de variations actuellement prises en
compte par FASTER :
– Les variantes syntaxiques mettent en jeu une transformation de la syntaxe du terme sans
modifications morphologiques des mots autres que les flexions. Ainsi, la variante (2.c)
de la table 2, est obtenue au moyen d'une variation, appelée modification, qui est l'in-
sertion d'un modifieur adjectival après le nom tête dans un terme de structure initiale
Nom-Préposition-Nom. Cette variation permet d'extraire système racinaire de surface
comme une variante de système de surface.
– Les variantes morpho-syntaxiques contiennent au moins un mot du terme initial qui a
subi une transformation de morphologie dérivationnelle. En outre, ces variations peuvent
également faire appel à des transformations syntaxiques. Par exemple, la variante (2.f)
de la table 2 est obtenue au moyen d'une transformation Nom à Verbe. Celle-ci permet
d'extraire alimentation hydrique d'un arbre varie comme une variante de variation de
alimentation.
L'extraction de la variante (2.f) repose sur une transformation morpho-syntaxique implémentée par la métarègle suivante :

              
La sortie de l'analyseur FASTER sur la phrase

                
              
Les dix premières variantes Nom à Verbe de [AGR]

                
              
Évaluation quantitative de l'extraction terminologique sur [AGR]

                
La métarègle (1) permet d'extraire 3 100 variantes du corpus [AGR] à partir de la liste de
termes extraits par ACABIT. La table 3 illustre les 10 premières variantes ainsi extraites.
La quantité et la qualité de l'extraction terminologique effectuées par FASTER sont évaluées,
du point de vue linguistique, dans les tables 4 et 5. La précision est mesurée sur un échantillon
de 1 000 variations choisies aléatoirement. La table 4 indique que les variantes de termes
représentent 40% des index. Elles sont ainsi réparties : 45% des variantes sont des variantes
syntaxiques et les 55% restant sont des variantes morpho-syntaxiques. Alors que les occurrences de termes (non variantes) sont extraites avec une très grande précision, les variantes de
termes sont obtenues avec une précision d'environ 75%.

            
Classification: SDOC et la méthode des mots associés

Le dernier élément de la chaı̂ne exploite les termes obtenus au 3.3 pour construire des
classes de termes. Ces classes visent à synthétiser le contenu informatif du corpus [AGR] et
sont construites suivant la méthode des mots associés, une approche robuste utilisée dans les
systèmes de recherche d'information (voir (Callon et al., 1986; Callon et al., 1991; Salton et al.,
1994)). Cette méthode, basée sur la distribution dans les textes des unités d'information que sont
les termes est mise en oeuvre dans le système SDOC (Grivel et Francois, 1995; Grivel et al.,
1995).
La classification des termes prend en compte deux critères : leur fréquence dans le corpus et
la cooccurrence des termes dans chacun des résumés. SDOC procède en deux étapes :
1. construction d'un réseau d'association basé sur la cooccurrence des termes en calculant
un indice de similarité appelé Indice d'
Equivalence
:
deux termes, leur fréquence, et la fréquence de leur cooccurrence.
2. partition du réseau en utilisant l'algorithme du simple lien (voir (Callon et al., 1986)).
Les experts du domaine ont montré que les classes qui constituent le réseau lexical sont des
structures sémiotiquement décodables, comme le montre la classe racine de la Figure 1 (voir
aussi (Muller et al., 1997)). Ces classes mettent en valeur des relations de type hypo/hyperonymie
(évapotranspiration/évaporation) ; partie-de (racine/pommier) ; complémentarité (racine/partie
aérienne); synonymie (racine/système racinaire) ainsi que des relations processus/entité (enracinement/système racinaire).

          Conclusion

Le système présenté ici est une association originale et unique de techniques de traitement
automatique de la langue et d'infométrie. Elle a permis la réalisation d'un outil d'extraction, de
représentation et d'accès aux concepts d'un domaine extraits d'un très grand volume de textes.
Ce projet est actuellement opérationnel sur un corpus français de résumés dans le domaine
de l'agriculture. La prise en compte d'une nouvelle langue (anglais) et de nouveaux domaines
techniques seront terminés dans quelques mois.
Le système est actuellement utilisé par des experts pour l'analyse de l'information en agriculture et en médecine. De plus, une évaluation de la pertinence des candidats termes extraits
lors des étapes (3) et (4) pour l'indexation est en cours de réalisation par des indexeurs.
La réalisation de ce système nous a également permis de mettre au point un lemmatiseur
pour le français, langage considéré traditionnellement comme trop complexe pour permettre
une lemmatisation qui n'est pas basée sur l'utilisation d'un dictionnaire.
Les applications que nous envisageons pour un tel système sont la recherche d'information,
l'enrichissement automatique des thésaurus, et l'assistance pour l'indexation manuelle et la
catégorisation de documents.




        
          
 Introduction

En Traitement Automatique des Langues Naturelles (TALN le problème de la gestion des
ressources linguistiques est crucial. Le volume des données manipulées, leur grande variété et la
vitesse de traduction sont autant de paramètres qui font de la construction de dictionnaires un
élément clé de tout système de TALN. La dispersion des outils sur des plates-formes
hétérogènes dont les lexicographes ont besoin lors de l'indexation et le coût élevé qui en résulte
freinent les avancées dans ce domaine.
Beaucoup d'efforts ont été faits pour essayer de créer une plate-forme unique qui réduirait les
coûts de production des dictionnaires mais peu de résultats ont été obtenus. D'autre part, pour le
projet Universal Networking Language, nous devons faire face à des besoins très importants. À
court terme, des outils d'indexation pour construire les dictionnaires seront nécessaires au
projet. Nous pensons qu'il est possible de résoudre les problèmes de dispersion des outils en
proposant une application générique multi-outils. Nous pourrons l'expérimenter dans le cadre
du projet UNL.

          
 Problématique

UNL est un projet de communication multi-lingue interpersonnelle. Il se base sur une langue
pivot, l'UNL et 13 autres langues. Il y a au minimum un partenaire pour chaque langue du
projet. Il faudra à court terme indexer près de 200 000 entrées avec un coût maximal de 5 F par
entrée. Pour satisfaire cette demande, nous avons donc besoin de travailler avec plusieurs
indexeurs dispersés en même temps. Il faudra ensuite regrouper les données en construisant une
base lexicale pour différents outils et différents partenaires. Nous avons étudié plusieurs
solutions :
Le Lexicaliste est un système de gestion de bases de données monolingues. Il ne permet pas
d'implémenter une architecture avec pivot interlingue, nécessaire au projet UNL. De plus,
son interface multifenêtres devient rapidement compliquée.
Genelex [Genelex 93] permet de décrire une architecture sous forme de graphe. Par contre,
aucun outil n'est disponible pour indexer à grande échelle.
Utiliser une base de données sur un PC présente de nombreux avantages mais ne convient pas
pour modéliser des structures complexes . Il faudrait en plus pouvoir équiper chaque
utilisateur avec le même logiciel et recentraliser les données ensuite.
Construire une super base lexicale centrale sur une puissante station avec des indexeurs reliés
par réseau est une solution idéale. C'est malheureusement une solution trop gourmande en
équipement. De plus, les liaisons réseau actuelles sont trop coûteuses pour pouvoir travailler
à domicile. Avec les contraintes d'un tel projet, et après analyse des différentes solutions, il n'existe pas
de solution satisfaisante. Nous proposerons donc une nouvelle méthode pour indexer une
grande base lexicale.

          
Solution mise en oeuvre

            
solution mise en oeuvre

Les lexicographes indexent la base en
remplissant des fichiers Word. Cette
technique a permis d'indexer 20 000 entrées
équivalent à 50 000 acceptions en 9 mois
pour un dictionnaire Français-Anglais-Malais. Nous avons ensuite amélioré cette
technique en séparant le travail en deux
parties :
La base de données centrale est gérée par
un lexicologue. Il récupère d'abord
plusieurs dictionnaires qu'il fusionne.
Il crée une description des entrées de la base
sous forme de grammaire. Grâce à cette
description, il prépare le résultat de cette
fusion sous forme de fichiers Word™ qui
sont envoyés aux indexeurs et récupère les
fichiers une fois révisés et complétés pour
les intégrer dans la base après filtrage. Il
peut renvoyer plusieurs fois les fichiers aux
indexeurs, si le résultat n'est pas assez
satisfaisant.
Les indexeurs travaillent à domicile sur
leur ordinateur personnel. Ils n'ont besoin
que du logiciel Word™ sur Mac ou PC.
Pour faciliter le travail des lexicographes,
nous avons ajouté des outils d'aide à
l'indexation sous forme de macros Word™.

          
Le serveur BaLeM

La base centrale est implémentée en MCL (Macintosh Common LISP). La structure de la
base n'est pas figée, ce qui nous permet de l'adapter aux changements. Nous pouvons à tout
moment intégrer des dictionnaires existants ou générer automatiquement des dictionnaires pour
différents systèmes de traduction comme ARIANE [Boitet 97] ou le système Deco utilisé à
l'Université des Nations Unies par le centre UNL pour le japonais et l'anglais.

          
Les postes des lexicographes

Le lexicographe dispose d'une vue globale de l'extrait de
dictionnaire
avec lequel il travaille.
Il peut corriger très rapidement les erreurs qu'il détecte et peut s'inspirer des articles
précédents ou suivants, qu'il voit en totalité sans avoir à ouvrir de
Chaque unité d'information est donnée sous forme de paragraphe dans un style particulier
Gaschler et al. 94. À l'aide des macros, le lexicographe peut sélectionner la catégorie dans
une liste (ce qui évite les erreurs dans les abréviations), vérifier la validité d'une entrée ou
calculer l'ensemble des styles valides à la suite d'un élément d'information afin d'insérer un
nouvel élément d'information ([Mangeot 97]).

            
Exemple de fichier d'édition du dictionnaire 

          
 Conclusion

La technique utilisée pour remplir notre base nous a permis d'indexer 20000 acceptions
UNL en 7 mois. Elle nous semble satisfaisante et générique. Nous continuerons donc à l'utiliser
pour indexer le reste des termes UNL (UW). Elle pourra facilement être reprise pour la
fabrication de grandes BDLM.
Nous avons depuis implémenté une interface Web directement reliée à notre base grâce à
MCL pour la consultation de notre base par les utilisateurs. Nous mettons actuellement en
oeuvre une interface pour le lexicologue administrateur codée en MCL et intégrée à la base pour
permettre des modifications en temps réel.




        
          
Insuffisance des approches localistes

Les verbes de déplacement sont généralement, selon nous, traités selon des critères trop localistes (changement de LRV chez Laur, CoL chez Sablayrolles, changement de localisation chez Flageul, primitive GO chez Jackendoff). Si ces critères sont assez pertinents pour des verbes comme entrer ou sortir, ils perdent leur raison d'être pour beaucoup d'autres verbes comme marcher ou courir (verbes souvent appelés médians et plus encore leur pertinence pour des verbes comme slalomer, contourner, passer (en construction transitive doubler... Ces derniers verbes ne se laissent pas définir en termes de changement de lieu car ils possèdent une sémantique liée à la notion même de déplacement, de trajectoire. Par ailleurs, les entités mises en relation avec ces verbes par l'intermédiaire de compléments sont assimilées à des lieux, c'est dire à des portions d'espace. Pourtant, il nous semble que, même en se restreignant à une sémantique purement spatiale, on ne peut se contenter d'une telle assimilation. Une zone spatiale (ou plusieurs zones) peut certes être attribuée à nombre de noms, mais n'est certainement pas la seule entité convoquée par les expressions du déplacement. Comment signifier « longer une route » si un chemin ne peut être fourni par route, et « remonter/descendre une rivière » sinon avec un chemin orienté ? Nous pensons que la notion de lieu n'est pas suffisante et focalisons dans la suite sur chemins et trajectoires.

          
 Formalisation d'entités complémentaires

Nous proposons donc un formalisme de chemins (et, par augmentation, de chemins
orientés et de trajectoires) auquel on associe des contraintes propres : type de chemin
(pouvant intégrer les contraintes de linéarité introduites par Asher relations entre chemins
(parallélisme, intersection relation entre chemins et objets spatiaux (intersection, extériorité).
Nous nous plaçons dans l'espace affine (A,E) où A est un ensemble de points et ER3.

            
Chemin

Un chemin est un couple (I, C) où I[a,b] est un intervalle (une partie connexe de R) et C
une fonction de classe C1 (continue et dérivable) de I dans A, et telle que (cid:153) x (cid:68) [a,b
longueur (Ca,xx-a. Il est orienté lorsqu'il est muni d'une relation d'ordre.

            
trajectoire

Une trajectoire est un chemin orienté muni d'une fonction continue et croissante définie
sur un intervalle temporel (faisant le lien entre temps et position) :

            
esquisse des « contraintes de forme » possibles

Elle peuvent exprimer la forme d'un chemin. Par exemple un certain trait sera pertinent
pour le verbe slalomer et le groupe nominal « route en lacets ». Un autre pour « aller droit » et
pour « route rect-iligne ».

            
esquisse de relations entre trajectoires

Nous introduisons la notion de parallélisme entre chemins (et donc, par héritage, entre
trajectoires) qui n'est proposée, à notre connaissance, par aucune étude antérieure et qui
pourtant nous semble impliquée dans un certain nombre de relations spatiales. Cette relation
porte sur la globalité d'un chemin (donc, a fortiori, d'un déplacement en faisant abstraction
des petites dérives locales : longer un ruisseau ne signifie pas que l'on ne puisse pas à faire
un écart, par exemple pour éviter un buisson. Très grossièrement, deux chemins seront
considérés parallèles si l'intégrale des valeurs absolues de l'angle de leurs tangentes sur tout
le chemin ne dépasse pas une certaine valeur d'admissibilité. Bien sûr, plus les chemins sont
longs, plus on va accumuler de valeurs d'angles ; il faut donc diviser l'intégrale par la
longueur des chemins.
Nous introduisons aussi la notion d'orientation de chemin, qui est aussi une contrainte
globale acceptant les dérives locales et construite aussi par une intégrale.
Enfin, la notion de rapprochement (par rapport au point P) est formalisée par :

          
Les emplois des chemins et trajectoires

Ce formalisme s'appliquera aussi bien sur certains noms (rue, ruisseau se verront attribuer
un chemin ou un chemin orienté certaines formes (un objet plan aura un chemin construit
selon son périmètre certaines locutions prépositionnelles (le long de, autour de certains
verbes (tous les verbes de déplacement feront référence à une trajectoire).

            
 objets

On voit donc apparaître la notion d'objet hybride : une rivière, par exemple, possédera
entre autres un trait relatif à une étendue plane (trait 1), mais aussi un trait relatif à l'existence
d'un chemin (trait 2).

            
le verbe longer

Notre vision de ce verbe pourrait se résumer par : « on ne longe pas un lieu, on longe un
chemin ». L'information sémantique majeure porte en effet sur un espace à une dimension : Il
nous semble que dans l'exemple (2) l'appartenance de Paul à un lieu s'acquitte mal de cette
tâche : la façon de se déplacer est très contrainte. la rivière possède une facette chemin, qui
offre un chemin C1. Le référent du verbe est la trajectoire T de chemin C. On pose alors :
C C1.

            
 le verbe contourner : X contourne Y

il exprime une contrainte exercée par un objet sur une trajectoire. Cela peut s'exprimer par
une contrainte mathématique simple. Soit C le chemin associé à la trajectoire portée par le
verbe, et L le lieu issu du complément Y. Il faut et il suffit que :

          
Conclusion

Nous avons essayé de montrer que la sémantique du déplacement nécessitait la prise en
compte réelle des trajectoires et des chemins, aussi bien au niveau des verbes que des noms.
Nous avons proposé un formalisme qui nous semble homogène dans la mesure où il
s'applique aussi bien à des noms (rue) qu'à verbes (de déplacement) ou des prépositions. Il
sera intéressant de le compléter afin qu'il tienne compte de la taille des objets, ceux-ci étant
pour le moment assimilés à leur centre.



